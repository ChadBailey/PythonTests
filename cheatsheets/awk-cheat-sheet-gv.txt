AWK:CHEATS BY GV
http://awk.freeshell.org/ #also check freenod irc - channel awk
http://awk.freeshell.org/AwkTips
https://www.gnu.org/software/gawk/manual/gawk.html
https://www.tutorialspoint.com/awk/awk_arithmetic_functions.htm
http://www.pement.org/awk/awk1line.txt
http://www.theunixschool.com/2012/09/grep-vs-awk-examples-for-pattern-search.html
https://www.yumpu.com/en/document/view/25827537/sed-and-awk-101-hacks#
AWK reserved variables :http://www.thegeekstuff.com/2010/01/8-powerful-awk-built-in-variables-fs-ofs-rs-ors-nr-nf-filename-fnr/?ref=binfind.com/web
https://www.shortcutfoo.com/app/dojos/awk/cheatsheet
http://web.mit.edu/gnu/doc/html/gawk_5.html
awk xml parsing: http://awk.freeshell.org/XMLScraping

AWK:CHEAT SHEET TXT 2 MANPAGE FORMATTING
##Read this cheat file with man pages:
http://technicalprose.blogspot.gr/2011/06/how-to-write-unix-man-page.html
man formatting: man 7 man & man 7 man-pages
groff programming: http://web.cecs.pdx.edu/~trent/gnu/groff/groff.html#IDX123
man pages making: https://liw.fi/manpages/
https://linux.die.net/man/1/help2man

##Working Command:
man --nj <(h=".TH man 1 2017 1.0 cheats page";sed "1i $h" cheatsheets/utils*gv.txt |sed 's/^UTILS:/.SH UTILS:/g; s/^$/\.LP/g; s/^##/\.SS /g; s/\\/\\e/g;G' |sed 's/^$/\.br/g')
You can also combine with --nh 
PS: man options --nj = not auto justified , --nh = not auto break words with hyphen on line changes.

##man and groff/troff require special handling.
man ignores normal line feeds at end of lines ($); empty lines (^$) are recognized and displayed
Line feeds in man pages can be done by inserting .br between two lines.
More .br in series of lines are ignored by man and got intepreted as a single line feed - not multiple new lines.
Man pages should start with a .TH line
Man sections / header start with .SH. 
Subsection start with .SS. Alternativelly you can use .B to make this line bold. .B follows text identation - .SS has it's own idents.
The backslash \ works as escape in groff, so you need to escape the backslash with \e (or \\ can also work)
The example tr -d '\n' will become tr -d '\en' with \e escaping, or will become tr -d '\\n' with \\ escaping.

For sed explanation see the sed cheatsheet.

AWK:BASICS
Depending on the application you can call AWK once to load the file and then you can manipulate the fields within AWK.
The whole awk syntax is based on the patter awk 'condition1{action1}condition2{action2}...'
If action is missing then print $0 is performed (default actions) (i.e awk 'NR==3' :prints the third line
If condition is missing then condition 1=true is used (default) and the action is always performed. i.e awk '{print $2}'

Typical usage advantage is when you need to read multiple patterns / values /columns / data from the same file.
If you do that with loop & grep you most probably it will be necessary to grep many times the same file and this makes the script slow.
Instead you can just once AWK the file and do whatever nanipulation you need inside AWK.
For complicated data manipulation is usual to have a seperate file full with AWK code and then call AWK with -f flag (=from file) to apply the code in your file/input
Remember the 48H log example that you need to see events logged in any minute of the 48H time frame. The use of loop and grep per minute leads to 3000 greps of the file, while you can do it with one AWK access.

##AWK Reserved Words
awk '/pattern/ {action}' file 		# Execute action for matched 'pattern' condition 
$0 									# Reference current record line
FS 									# Field separator of input file (default whitespace).If unset (FS="" of -F"") then field separator is single char (like grep '.')
-F 									# Command line option to specify input field delimiter before starting awk script.
OFS 			# Output Field separator (default whitespace). Mind that this can be assigned in command line like awk '{your script}' OFS=: file or awk -F" " -v OFS="\t"
NF 				# Number of fields in current record/line (i.e {print $NF})
NR 				# Record Line number of the input stream. It keeps increasing in the second file if provided (awk '...' file1 file2)
FNR 			# Reference number of the current record relative to current input file . If more than one file is given in awk, it is reset
FILENAME 		# Reference current input file
ORS 			# Output Record separator (default newline). Can be set to another value, or can be used to avoid double print (print a;print$0 <==> print aORS$0)
RS 				# Record separator of input file (default newline)
RT				# Record Terminator . Is like the RS but in case of multi RS it is dynamically updated to keep the current used separator/terminator to display the last field. Thus it's setting varies acc to multi RS value.
FPAT			# FPAT = "([^,]+)|(\"[^\"]+\")"  -Defines what the field contains. FS defines what a field does NOT contain thus is field separator

^ , $ 							# Match beginning/end of field
== 								# Equal operator for if
~ 								# Match regex opterator for if (i.e if ($2 ~ "something") print $0)
!~ 								# Regex not equal operator
BEGIN 								# Denotes block executed once at start of the script (No input processed yet)
END 								# Denotes block executed once at end of the script
a=str1 OFS str2 					# Space Concatenates str1 , OFS and str2
{print $1} 							# Prints the first field of input (as read using FS separator)
{print $1,$2; print $3,$4} 		# Prints field 1 & 2 in one line, and field 3 & 4 on next line.(print sthing with ; = "\n") 
'BEGIN {FS=OFS="\t"}; 			# Sets Input Field Separator (FS) and Output Separator (OFS) to tab. This works ony for printing fields/texts separated with comma (i.e print $1,$2) and does NOT work for $0 since $0 is one big field.
next 						    # Goes to next record - blocks following code execution and restarts from the beginning
nextfile					    # Stops the processing of current file1 and goes to file2 , starting from the beginning
while (cond) {actions}				# While loop. Do is not required. Brackets not required for one command. 
do {actions} while (conditions)		# Alternative while loop syntax. Break, Continue and exit can be used in loops 

##AWK Builtin Functions
https://www.gnu.org/software/gawk/manual/html_node/String-Functions.html
http://awk.freeshell.org/Backreferences   #apply to gensub,etc

sub(regex, sub, string) # Substitute sub for first occurrence of regex in string  ($0 is used if ommited)
gsub(r,t,s)             #Substitute t for all occurrences of regex r in string s - Accepts \& in replacement = backref to replaced part. Does not accept \\1 like gensub
gensub(regexp, replacement, how [, target])  #Accepts a target other than current FNR (vs sub and gsub who work with FNR) and return the modified string instead of the number of substitutions. Seems that does not accept \& but accepts groups like \\1,\\2
substr(str, start, l)   # Returns substring of string str, starting at index start of length l. If length is omitted, the suffix of str starting at index start is returned.
system(cmd)             # Execute cmd and return exit status. Usefull if you need to insert a file between records (instead to load in memory, cat it)
tolower(s)              # String s to lowercase
toupper(s)              # String s to uppercase
getline                 # Set $0 to next input record from current input file. 
getline <file2          # Goes to next input record of file2.From this point onwards, awk fields $0,$1, etc refer to file2
getline var <file2      # Read file2 and assign to var the whole record/line of file2 ($0). Without <file current file is used
nextfile                # goes to the next file - changes the flow of the script (does not return back)
operators               # https://www.tutorialspoint.com/awk/awk_operators.htm

length(s)               #return the length of s. Works even with arrays,fields,etc
match(s ,r[,a])         #return index of where s matches r or 0 if there is no match; set RSTART and RLENGTH.If array a is given is filled with matches
split(s,a,r,seps)       #splits s into array a on regex r;returns number of fields; seps is an array holding each separator used (similar to RT for records)
sprintf (fmt, list)     #return list formatted according to fmt.can be assigned to variable (printf can not do that)
substr ( s, i, n)       #return the n-character substring of s starting at i;if n is omitted, return the suffix of s starting at i
index(s, t)             #Return  the index of the string t in the string s, returns 0 if t is not present.

simple conditions checks       # Can be expressed like awk '$5 == "abc123"' - prints lines that field5 match abc123. Print is ommited since it is the default awk action
advanced conditions checks     # {if (expression) {commands for true} else {commands for false}}
expr?true:false                # Conditional Expression - ternary operator. expression is evaluated and true command is run if expression=true
Directories on the command line are fatal for standard awk; gawk ignores them if not in POSIX mode.

AWK:BASIC FUNCTIONS EXAMPLES
awk '{print "NR:",NR,"FNR:",FNR,"fname:",FILENAME,"Field:",$1}' file1 file2                   #Both files will be printed in series

awk 'BEGIN{a[1]="g";a[1]="v" FS a[1];print a[1]}'	        # prints "v g" . Value of a[1] is replaced by a new value and the old value of a[1]. Mind the gap between new oldvalue that means concatenate.

awk 'BEGIN{a="g";a="v" FS a;print a}'			            #Same as above but without arrays. Needs space since space is concatenate operator in awk

awk '{getline <"file2";print $1}{print $1}' file1           #Reading is done by file2 , all fields correspond to file2

awk '{getline var2 <"file2";print var2}{print $1}' file1    #each line of file2 goes to var2 unsplit but now all Fields ($1,etc) refer to file1.

awk '{getline n;print $1,$2}' file                          #For a file with five lines, second and forth line is stored in var n , 1st,3rd and 5th lines are printed.

awk '{getline n;print n;print $1,$2}' file                  # Prints lines 2(n)-1($1,$2)-4(n)-3($1,$2)-4(n)-5($1,$2)

awk '{getline n;print $1,$2;print n}' file                  # Prints lines 1-2-3-4-5 ($1,$2 in series) and then line 4 (last n). Mind that print n in the end prints only after the file is finished

awk '{getline n1;getline n2;print $1,$2}' file              #Prints lines 1 and 4. getlne n1 gets line2 , getline n2 gets line3, line1 is printed, line4 is printed, line5 goes to n1 - no more lines - end.

awk '{a=$1;getline;print a,$2}' file                        # a gets $1 of current line1. getline goes to line2. prints a($1 of line1) and $2 of line2.Next iteration goes to line 3 - a=$1 of line3 , getline gets line4 , print a,$2 prints $1 of line3 and $2 of line4.

awk '{if ($1>="3") nextfile;print $0}' file2 file1          # This prints line 1 and 2 from file2 and then line 1 and 2 from file1, no matter if file1 has more than 3 lines. Can be usefull with regex match (~)

awk '{print ($0==p?"":$0); p=$0}' file                      # Checks consecutive lines;print with condition check. If $0 equals to var p then print "" else print $0. At the end assign p=$0.

awk 'BEGIN {while ((getline a[++c] < "file1") > 0) { } print c}' #Reads file1 and prints the total number of lines. Mind the array indexes.

PROCINFO["/dev/stdin", "READ_TIMEOUT"] = 5000
while ((getline < "/dev/stdin") > 0) print $0               # Read user input with timeout

awk 'BEGIN{system("echo 1 > f")}'

awk 'BEGIN{ORS="\n\n"}{print}' ./tmp/file4					# Double space a file. ORS=Output Record Separator


##AWK FIELDS SPLITTING - SPLIT FUNCTION 
http://awk.freeshell.org/Frequently_Asked_Questions#toc1
http://awk.freeshell.org/RangeOfFields   #more field splitting techniques in this link

You can set FS to multiple chars like -F"[:;]" and the $0 will be splitted in many pieces.
Though there is no way to refer to the delimiter used for each fiels. Even if you print FS it will print literally [:;].
As a result, in such situation the output will respect OFS (space by default) and will be a mess.

A good alternative similar to RS and RT (=last separator used to split records) is to use SPLIT function .
Split will split the $0 in pieces according to a customized regex, and will store each piece in an array and each separator used for splitting 
in a separate "seps" array.
Most important if you combine split with printf you can rebuild the record accuratelly = identical to Input Record $0

SPLIT Test:

$ echo "$b"
alert tcp any any -> any any (msg: "this is a "dummy" rule (to test) the rule"; flow:to server; sid:1233; rev:1; no case; content: "nothing";)

$ echo "$b" |awk '{fsep="[\";:]";split($0,f,"y|[\";:]",sep);for (k=1;k<=length(f);k++) printf("No=%s \t|Val=%s \t\t|Seperator=%s\n", k,f[k],sep[k])}'
No=1 	|Val=alert tcp an 		|Seperator=y
No=2 	|Val= an 		        |Seperator=y
No=3 	|Val= -> an 		    |Seperator=y
No=4 	|Val= an 		        |Seperator=y
No=5 	|Val= (msg 		        |Seperator=:
No=6 	|Val=  		            |Seperator="
No=7 	|Val=this is a  		|Seperator="
No=8 	|Val=dumm 		        |Seperator=y
No=9 	|Val= 		            |Seperator="
No=10 	|Val= rule (to test) the rule 		|Seperator="
No=11 	|Val= 		    |Seperator=;
No=12 	|Val= flow 		|Seperator=:
No=13 	|Val=to server 	|Seperator=;
No=14 	|Val= sid 		|Seperator=:
No=15 	|Val=1233 		|Seperator=;
No=16 	|Val= rev 		|Seperator=:
No=17 	|Val=1 		    |Seperator=;
No=18 	|Val= no case 	|Seperator=;
No=19 	|Val= content 	|Seperator=:
No=20 	|Val=  		    |Seperator="
No=21 	|Val=nothing 	|Seperator="
No=22 	|Val= 		    |Seperator=;
No=23 	|Val=) 		    |Seperator=



##MATCH FUNCTION
usually usage is match(string,substring).
awk '{print match($0,"-")}' <<<""abc-cde-fgh"       -> prints 4= the position of substr inside string. Even if the search term is "-c" will return 4. 

match can accept a third argument = array var. 
In this array all captured regex groups/expressions are stored.
awk '{print match($0,"-cd",a)}{for (i in a) print i,",",a[i]}' <<<"abc-cde-fgh"
4 #print of match
0Rstart , 4   #print of the array. 
0Rlength , 3  #print of the array
0 , -cd       #print of the array.Index 0,value "-cd"

##Advanced match usage as a regexp mapping tool
match can return different start,length and array indexes for more matches at once. 
Also can return in array elements each regex group with a separate index that can be used in whole awk program
awk 'match($0,/(.*)-...-(.*)/,a){for (i in a) print i,a[i]}' <<<"abc-cde-fgh"  #
0start 1		#Array printing of start position and length of each pattern
0length 11	#Printing is random, since for i in a prints in random order by default
1start 1
2start 9
2length 3
1length 3
0 abc-cde-fgh	#a[0] : Contains the whole matched substring. Equals to the original string in this case since the pattern matches the whole input string
1 abc			#a[1] : contains the first regex group. You can call a[1] from now on, and this will print abc
2 fgh			#a[2] : contains the second regex group


##Another Match Example with partial regex matching
awk '{print match($0,/-(...)-/,a)}{for (i in a) print i,a[i]}' <<<"abc-cde-fgh"     #a[0]="-cde-" and "a[1]=cde"


##REGEX Non greedy operator equivalent by Chazelas:
http://unix.stackexchange.com/questions/49601/how-to-reduce-the-greed-of-a-regular-expression-in-awk
Perl (.*?)(;) = match everything up to first ;  <===> (.[^;]*)(;)  or (.[^;]*;) if you want ; to be included in the match

##ADVANCED MATCH
echo "$a"
Text1 somethingAA0123456something,elseAA9876543foo text1
$ awk '{match($0,/(\w+\s)(\w+)(\w\w[0-9]{7})(\w+,\w+)(\w\w[0-9]{7})(\w+\s)(\w+)/,a);print a[1],a[3],",",a[5],a[7]}' <<<"$a"
Text1 AA0123456,AA9876543 text1


##Skip Input Columns Techniques
echo "Field1 Field2 Field3 Field4 Field5 Field6"  |awk '{print substr($0, index($0,$4))}' --> Field4 Field5 Field6
#Use of substr combined with index, since substr expects an integer as starting point and this is what index function returns.

echo "Field1 Field2 Field3 Field4 Field5 Field6"  |awk '{$1=$2=$3="";print}'
   Field4 Field5 Field6
#Mind the gaps in the beginning. Especially usefull if you need to "delete" middle columns.
#Actually you are not really delete those columns but you replace their contents with a blank value and this is assigned to $0 structure

echo "Field1 Field2 Field3 Field4 Field5 Field6"  |awk '{$1=$2=$3="\x08";print}' 
 Field4 Field5 Field6
#Just one space in front due to hex 08 = back space. Mind that awk handles hex codes nicely. It is strange that more bckspaces are ignored if entered (i.e ="\x08\x08\x08"). It is also strange that \x00 is handled as an empty value ""

echo "Field1 Field2 Field3 Field4 Field5 Field6"  |awk '{gsub($1FS$2FS$3FS$4,$4);print}'  --> Field4 Field5 Field6   
#Columns 1-2-3-4 have been replaced by column4. No gaps here.

echo "Field1 Field2 Field3 Field4 Field5 Field6"  |awk '{$1=$2=$3="\x1b";print $0}'  ->No blanks are printed in the beginning.
Although if you pipe the output to |od -An -t x1c you will see that \x1b (esc) chars are present but not printed = non-printable char.
 1b  20  1b  20  1b  20  46  69  65  6c  64  34  20  46  69  65  6c  64  35  20  46  69  65  6c  64  36  0a
 033     033     033      F   i   e   l   d   4       F   i   e   l   d   5       F   i   e   l   d   6  \n 

For a fixed known number of columns , you can always do 'print $4,$5,$6'

For a known/fixed number of columns you can also delete completely a middle column with awk '$0=$1FS$2FS$4'


Removing Columns from the end:
awk 'NF--' 		#Removes last column
awk 'NF-=2'		#Removes last two columns
awk 'NF=$3'		#Removes all columns after column 3

## Force output to respect OFS when printing of $0
Normally OFS is not affecting the print of $0. if you just {print} the input line will be printed as it is, ignoring OFS.
You can workaround this either by awk -F"," 'gensub(FS,OFS,"G",$0)' OFS=: <<<"one,two,three"
or more cleverly: 
awk -F"," -v OFS=":" '{ $1 = $1; print }' <<<"one,two,three" #Prints one:two:three
The statement $1=$1 will reconstruct the $0 according to OFS. If you just print $0, will be printed with comma.

PS: printing fields i.e {print $1,$2) will be done by respecting OFS.

##Conditional Expressions Clarification
Mind that the compact awk usage allows awk 'condition{action}' without if - then
Action can be ommited; in this case the default action (print $0) is performed. 
On the condition side we can have either boolean condition check (true /false) or arithmetical checks (i.e awk 'NR>10')

Moreover all vars have a value. You can either built an expression based on the actual value of the variable , or based on the fact if this variable has a value or not (boolean testing).
In Boolean testing, awk will evaluated as true whatever variable has a non-zero / non-null value, no matter if it is negative or positive.

See this difference:
.B (A) awk 'a[$0]++' file	  #Prints all non unique lines of a file.
Here the status of the a[$0]++ is checked (before increasement). If a[$0]++ has any valid value will be printed
First line a[$0] is null=no printing and then increasing value from null/zero to 1 (++)
All the other times that same $0 is found will have a value different than zero thus will be printed.

.B (B) awk '(a[$0]++ == 1)' file #Prints all lines that found two times . Emulates uniq -d
Here the real value of a[$0]++ is checked before increasement and printing is done only when a[$0] is 1 
On the first line a[$0] will return null=false=0 
On the second line a[$0] will have a value of 1 = printing
On the third line a[$0] will have a valuer of 2 = non printing

.B (C) Code Golf Challenge: How to emulate uniq -D with as less code as possible.
uniq -d prints the lines that found to be repeated two or more times  
So even if line1 is repeated 10 times, uniq -d will just print line1 (one time) and this is what (B) does.

uniq -D print ALL repeating lines . If line1 repeats 10times, uniq -D will print line1 10 times.
This is close to awk (A) but (A) is not printing the first occurence of a future repeated line. 
awk (A) prints only the next occurences after first. So for a 10 times found line1 awk (A) will print Line1 9 times.

We could force an extra separated print a[$0] for all duplicated lines (i.e in END) but this may mess output format. 
It is more challenging to print duplicates along with their NR in which they found in file (print them as you found them)

Another alternative would be in END to print array elements a[$0] with a value >1. But for big files this should be inefficient.

##Transform Header only line to header + field 1 +field2
http://stackoverflow.com/questions/42669125/how-to-convert-heading-of-file-content-into-column-using-awk/42672530#42672530

My way: awk 'BEGIN{FS=OFS="|"}NF==1{h=$0;next}$0=h OFS $0' file1 #Fallpit: No blank line is printed between headers change

Smart way: awk 'BEGIN{FS=OFS="|"} NF==1{h=$0} {print (NF==1?"": h OFS $0)}' file1 #Includes blank line on every header change

Another Smart Way : awk -F '|' '{if(NF==2)$0=F"|"$0;else{F=$1;$0=""}}NR>1' YourFile


##FIELDS SPLITTING WITH GENSUB
gensub is great because the manipulated string returns in gensub and not in the text/field under manipulation as gsub/sub does

lynx -dump "http://www.meteorologos.gr/" |awk '/Αθήνα/{a=1;next}a==1{print gensub(/(...)(..)(.*)/,"\\2 βαθμοί",1,$0);exit}' |espeak -vel+f3 -s10 -k3
splits the $0 in three parts: 3 chars - 2 chars - rest chars.                          ^^Regex      ^^Replace  ^identifier (1=first intance, 4=forth instance, "g"=global

## PRINTF
The great about printf("%s",var) is that does not print new lines along ALL the input lines, and thus you can concatenate fields of ALL lines with just one command.
If you apply "%s\n" will emulate the behavior of classic print in awk = will print the desired field on each separate line.
in a script like '{printf("%s",$0) all lines of input file will become one single line = new lines ignored.

Compare:
echo -e "a b c\nd e f\n" |awk '{print $0}' --> prints a b c in one line and d e f in the next line - AS EXPECTED
echo -e "a b c\nd e f\n" |awk '{printf("%s", $0)}' --> a b cd e f  - all in one line - new lines / ORS ignored
echo -e "a b c\nd e f\n" |awk '{printf("%s\n", $0)}' --> same as print $0
echo -e "a b c\nd e f\n" |awk '{printf("%s%s", $0,RT)}' --> same as print $0 since RT is new line = RS

echo -e "a,b,c\nd,e,f\n" |awk -v RS="," '{printf("%s%s", $1,$3)}' ---> Will print in ONE line abcef = The first field of all the lines.

## PRINTF - CUSTOM RS AND RT
Combining printf with multi RS and RT you can easily reconstructure the original input record no matter in how many pieces has been split by awk and multi RS.
Actually RT is the dynamic RS used each time to split the records.

Tests:

echo -e "a,b,c\nd,e,f\n" |awk -v RS="," '{print NR,$1,$2}'
1 a 
2 b 
3 c d   #Mind that d is $2 because between c and d there is not comma but newline
4 e     #This is why usually when you want to split completelly all fields to records, in RS you include also the new line i.e RS=",|\n"
5 f     

echo -e "a,b,c\nd,e,f\n" |awk -v RS=",|\n" '{print NR,$1,$2}'
1 a 
2 b 
3 c 
4 d 
5 e 
6 f 
7  

And now this is how Printf and RT can come in play to make the record back to normal. RT will change values dynamically.
For line1 (a) , RT will be comma. But for line 4 (d) RT will be new line.

echo -e "a,b,c\nd,e,f\n" |awk -v RS=",|\n" '{print $1,RT}'
a ,   # Line 1 - RT comma 
b ,   # Line 2 - RT comma
c     # Line 3 - RT new line
        # Just an empty new line that follows field c - before d
d ,   # Line 4 - RT comma
e ,   # Line 5 - RT comma
f     # Line 6 - RT new line

  

But above printing does not even close to the original record. What about this one:

# echo -e "a,b,c\nd,e,f\n" |awk -v RS=",|\n" '{printf("%s%s", $1,RT)}'
a,b,c
d,e,f

Yes this is identical to input record using printf (ignoring new lines) and printing RT dynamic value.
The new line is produced because RT will be \n when reading field "d".

With this method, you can break down all the fields to records, you can manipulate/change them , and then print them back to the original format
using the printf (no - new lines) instead of print and making use of RT dynamically changing value along the corresponding records.
You just need to include \n in the customized RS for this shit to work.
PS: If you try to set ORS=RS="multi Record separators here" it does not work. Print will just print the whole regex as ORS instead of comma or new line or whatever.


Tip:
Es[ecially in Fields Splitting, you can apply a kind of ternary if at printf : for (k=1;k<=NF,k++) printf("%s%s",$k,(k==NF)?ORS:OFS)

----------------------------------------------------------------------------------------------------------------
AWK:PRINT WITH IF CONDITION
##Print lines if specific columns have same value
awk -F ':' '$3==$4' file.txt -->  Prints only when $3 equals to $4. print can be ommited since it is the default action.
echo "Geo 123 bg ty 123" |awk -F" " '$2==$5' -> Geo 123 bg ty 123  # Print lines in which field 2 = field 5, otherwise returns nothing.
echo "Geo 123 bg ty 123 Geo" |awk -F" " '$1==$6' --> Geo 123 bg ty 123 Geo # Print if field1=filed6 , meaning Geo=Geo. Works even with strings!!!

----------------------------------------------------------------------------------------------------------------
AWK:EXTERNAL VARIABLES
##Import variables
You can import a shell variable by awk -v awkvar=$shellvar (i.e awk -v dt=$date).
Inside awk you need to refer to awkvraible without $.
Each external var requires it's own -v declaration.

##Export AWK variables
$ mkfifo fifo
$ echo MYSCRIPT_RESULT=1 | awk '{ print > "fifo" }' &
$ IFS== read var value < fifo
$ eval export $var=$value

##Export Alternatives
If you need to catch let's say one var you can also do it like shellvar=$(awk...). Whatever awk prints will be stored in $() pipe buffer.
For more vars, you could also use a temp file , printing inside va="value"\n and then you could source the file in shell scripts.

----------------------------------------------------------------------------------------------------------------
AWK:REPLACE Switch position of comma separated fields:
echo "textA,textB,textC,dateD" |awk -F, '{A=$3; $3=$2; $2=A; print}' OFS=,
textA,textC,textB,dateD
OFS affects only the output display separator. If omited space (default OFS) will be used.

print all the lines between word1 and word2 : awk '/Tatty Error/,/suck/' a.txt

Print up to EOF after a matched string: awk '/matched string/,0' a.txt

----------------------------------------------------------------------------------------------------------------
AWK:PRINT WITH MULTIPLE DELIMITERS

##Basic Test
Input: --- 22:16050075:A:G 16050075 A G
$ awk -F"[ :]" 'BEGIN{OFS="\t";print "$1","$2","$3","$4","$5","$6","$7","$8"}{print $1,$2,$3,$4,$5,$6,$7,$8}' file
$1      $2      $3              $4      $5      $6              $7       $8
---     22      16050075        A       G       16050075        A        G

#mind the diffrenece with -F"[:]"
$1        $2         $3  $4     
--- 22    16050075   A   G 16050075 A G 

##Examples
$ awk -F"name=|ear=|xml=|/>" '{print $2} {print $4}' a.txt >b.txt
Input: <app name="UAT/ECC/Global/MES/1206/MRP-S23"   ear="UAT/ECC/Global/MES/1206/MRP-S23.ear" xml="UAT/ECC/Glal/ME/120/MRP-  S23.xml"/>
Output: 
UAT/ECC/Global/MES/1206/MRP-S23   
UAT/ECC/Glal/ME/120/MRP-  S23.xml
Test: awk -F"name=|ear=|xml=|/>" '{print "Field1="$1} {print "Field2="$2} {print "Field3="$3} {print "Field4="$4}' a.txt
Mind that separate {} create a newline to out file.

##Another example of using as field seperator (F) anything (a char, a word, two delimiters, etc).
Compared to cut : with cut you allowed to use only one delimiter (-d), or to define a chars range using -c (i.e -c1-10 : seperate file in character 1-10 , whatever this char is).
echo "This is something new for us" |cut -c1-12 --> This is some # You can not combine -c with -f or with another -c, but you can print a range -c1-10, or particular chars using -c1,10,12

echo "value1,string1;string2;string3;string4" |awk -F"[;,]" '{print $2}' -->string1
echo "value1,string1;string2;string3;string4" |awk -F"[;,]" 'NR==1{for(i=2;i<=NF;i++)print $1","$i}'
-->value1,string1
-->value1,string2
-->value1,string3
-->value1,string4

In case of file , separated with new lines you need to apply this a bit different version: 
awk -F"[;,]" 'NR==1{print;next}{for(i=2;i<=NF;i++)print $1","$i}' file

----------------------------------------------------------------------------------------------------------------
AWK:SEARCH / GREP Search for a pattern with not known occurencies:
awk '{{for(i=1;i<=NF;i++)if($i == "name:") printf $(i+1)" "$(i+2)" "} print ""; }' yourfile

This is usefull if we dont know how many "name:" entries exist per line
If we know that each line has i.e 3 entries then this also works: awk -F"name:" '{print $2 $3 $4}'
If a line has less than 3 no problem. Var $3 and/or $4 will be empty. 
If line has more than 3 the -F solution will miss the rest entries.

Also check this out: awk '{for(i=3;i<=NF;++i)print $i}'
awk '{for(i=5;i<=NF;i++) {printf $i " "} ; printf "\n"}' awkTest2.txt

----------------------------------------------------------------------------------------------------------------
AWK:PRINT Produce a sed script to replace values to a file with entries from another file
http://unix.stackexchange.com/questions/340246/how-to-replace-a-string-in-file-a-by-searching-string-map-in-file-b#340247

Consider a user map containing multiple lines with "userid username" (seperated by space)
Consider a text file (letter.txt) contaiining paragraphs with reference to the users as userid.
We want to replace all userids in letter file with their realnames present in name mapping file.
Tricky solution: Transform map file (each line) to the format 's/userid/username/g' and then call sed -f <transformed mapfile> <text file that needs replacements>
The awk part: $ awk '{ printf("s/<@%s>/%s/g\n", $1, $2) }' user_map.txt >script.sed
The sed part: $ sed -f script.sed letter.txt 

*BASH Way: var="$(cat file.txt)";while read -r id name;do var="${var//@$id/$name}";done<mapfile.txt;echo "$var"

*SED Way : while read -r id name;do sed -i "s/\@$id/$name/g" textfile.txt;done<mapfile.txt

*SED Bug : File is opened and "seded" multiple times (but either the Kusulananda solution does sed multiple times, correct? - No. Does one sed with multiple replace patterns)
On the other hand, bash way opens the file once and , makes replacements in memory ($var) and when finished just echo the $var.
Bash solution doesnot require any external tools; it is just bash parameter expansion feature.

----------------------------------------------------------------------------------------------------------------
AWK:REPLACE Insert dash in string
String: #  1  2016-05-31-1003-57S._BKSR_003_CM6
awk '{print substr($3,0,13)"-"substr($3,14,2)}' file.txt

Output : 2016-05-31-10-03

Alternatives:
$ cut --output-delimiter='-' -c7-19,20-21 file.txt
$ while IFS= read -r line;do line="${line:6:13}-${line:14:2}";echo $line;done<file.txt
$ sed 's/..$/-\0/g' <(cut -d- -f1-4 <(cut -d" " -f5- file.txt)) #use >newfile at the end to send the results to a new file
Mind the \0 usage of sed (refers to data on hold , kind of history/previous value)

----------------------------------------------------------------------------------------------------------------
AWK:REPLACE  Replace values upon criteria matching:
Input:
  a b c
A 5 2 0
B 0 5 4
C 4 3 4
D 2 0 2

Output expected (replace all fields with a value to 1)
  a b c
A 1 1 0
B 0 1 1
C 1 1 1
D 1 0 1

AWK script : 
BEGIN { OFS = FS = "\t" }NR != 1 {for (i = 2; i <= NF; ++i) {if ($i != "0") {$i = "1";}}}{ print }

Runit : awk -f script.awk datafile

----------------------------------------------------------------------------------------------------------------
AWK:PRINT IF CONDITION - Column Checker
Input:
A B B A
A A A
B A B A
B B
A A
A B

Output : 
If all columns are the same then print just once the common characted (i.e if AAA then print A)
If columns are different then print "multi"

Code:
awk '{ for (i = 2; i <= NF; i++) { if ($i != $1) { print "multi"; next } }; print $1 }'

----------------------------------------------------------------------------------------------------------------
AWK:PRINT Print line with matched pattern (simulates grep):
awk '/word/' file.txt
GNU awk also accepts variables in the form of awk "/$variable/" file.txt
Using '!/pattern/' you invert the match (similar to grep -v)
----------------------------------------------------------------------------------------------------------------
AWK:PRINT Print a line if columns are equal to 5 (ps: columns separated by "|")
awk -F \| 'NF==5' data3

----------------------------------------------------------------------------------------------------------------
AWK:COUNT LINES
awk '{ $0 = NR "\t" $0 } 1' file.txt | tail #Here obviously we assign to $0 the value NR=Number of Record + tab + $0. The 1 at end prints
GV: awk '{ print NR "\t" $0 }' file1 #This also applies numbering 

Counting Alternatives:
grep -n ^ file.txt | sed 's/:/\t/' | tail
sed = file.txt | sed 'N;s/\n/\t/' | tail
pr -n -t -l 1 file.txt | tail
nl /boot/config-4.9.0-1-amd64 |tail

#------------#-------------##------------#-------------##------------#-------------##------------#-------------#
AWK:PRINT / GREP WITH MULTIPLE PATTERNS - Simulate agrep (grep with AND) for patterns read from a file:
http://unix.stackexchange.com/questions/341076/how-to-find-all-files-containing-various-strings-from-a-long-list-of-string-comb
Basic Command: awk '/pattern1/ && /pattern2/' file   #Performs the default action print $0

while IFS= read -r line;do awk "$line" *.txt;done<./tmp/d1.txt
Above will print in screen the matched lines of all txt files.
If we need to print only the filename and not the matched contents then awk must be used like this:
awk "$line""{print FILENAME}" *.txt

In order this to work , search terms in file d1.txt should be in format /term1/ && /term2/ && /term3/ etc.
PS: Single quotes found not necessary in GNU AWK

In the example given in link , pattern file contains lines like this:
"surveillance data" "surveillance technology" "cctv camera"

Which are converted to awk format like this:
$ sed 's/" "/\/ \&\& \//g; s/^"\|"$/\//g' ./tmp/d1.txt
> /surveillance data/ && /surveillance technology/ && /cctv camera/ #mind the absence of single quotes

Alternativelly we could use :

(a) agrep by formatting the data in the form of 
pattern1;pattern2;pattern3 
and then using: while IFS= read -r line;do agrep "$line" *.txt;done<./tmp/d1.txt

(b) classic grep with xargs -r : grep -l pattern1 * |xargs -r grep -l pattern2 |xargs -r grep -l pattern3
The -l instructs grep to print filename . Usefull info for the next grep.
If we just want to multi grep with and on the data then even this should work:
grep pattern1 file |grep pattern2 |grep pattern3
The final output would include all three patterns. 

#------------#-------------##------------#-------------##------------#-------------##------------#-------------#
AWK:GREP AND MULTIPLE PATTERNS CASE B  
awk '/Label 3/ { print $3 } /Label 5/ { print $3 }'  <==>  grep -E 'Label 3 \|Label 5' |cut -d' ' -f3
Mind the print function between patterns.
AWK can directly print to a file using { print $3 >>"F3.csv" }

Combine multi grep pattern and cut -d' ' -f3 for multiple lines
awk '/Label 3/ { print $3 >>"F3.csv" } /Label 5/ { print $3>>"F5.csv" }'
OR
awk '/Label 3|Label 5/ { print $3 >> "F"$2".csv"}' #Field 2 is used as name for the output file.

Consider a file with lines 
Label 3 70
Label 4 95
Label 5 100

#------------#-------------##------------#-------------##------------#-------------##------------#-------------#

AWK:GREP AND MULTIPLE PATTERNS CASE C  
Consider small patterns file with two patterns per line.
Consider big data file with lines in where the patterns can be in different order
awk '/pattern1/ && /pattern2/'  works fine, either directly or by applying awk -f <(sed ..... patterns) datafile

What about a full awk implementation to handle the two patterns in an AND way?
##1. awk 'BEGIN { getline < "patterns"; split($0, patterns) } { for (i in patterns) if ($0 !~ patterns[i]) next; print }' file1 
#Above works only for 1st line of patterns (by geirha)

##2. awk 'NR==FNR{patts[$1]=$2;next}{for (i in patts) if (($0 ~ i) && ($0 ~ patts[i])) print}' patterns file1  
Restricted to work for only two patterns per line but works ok (regex match - no word match)

##3.awk 'NR==FNR{data[$0]=$0;next}{for (i in data) if ((data[i] ~ $1) && (data[i] ~ $2)) print data[i]}' file1 patterns  
#works even ok with pitfall that big data file is loaded in memory instead of patterns file.

##4.awk 'NR==FNR{patts[$0]="\\<" $1 "\\>.*\\<" $2 "\\>|\\<" $2 "\\>.*\\<" $1 "\\>";next} \
    {for (i in patts) {if ($0 ~ patts[i]) print}}' patterns file1   
#Just one more alternative using regex AND and word matching. AND can be emulated with pattern1.*pattern2 | pattern2.*pattern1
Patterns are transformed from "833 7777" to "\<833\>.*\<7777\> | \<7777\>.*\<833\>"

##5. awk 'NR==FNR{patts[$0]="\\<" $1 "\\>.*\\<" $2 "\\>|\\<" $2 "\\>.*\\<" $1 "\\>";next} \
{for (i in patts) {if ($0 ~ patts[i]) !found[i]?found[i]=$0:found[i]=found[i] ORS $0}} \
END{for (k in found) {print found[k];print "-----"}}' patterns file1   
#The final solution : word matching of pattterns with logical AND operation and printing in the end with matching groups and a separator string. Test it online : http://www.tutorialspoint.com/execute_bash_online.php?PID=0Bw_CjBb95KQMd2JiTTJndl9TLU0

##6. Extending to work with a 2d array in gawk , by geirha
printf 'p1 p2\np3\n' > patterns
awk 'NR==FNR{for(i=1;i<=NF;++i)patts[FNR][$i];n=FNR;next}END{for(j=1;j<=n;++j){for(p in patts[j]) print j, p}}' patterns
array checks can be made like : <geirha> for (i = 1; i <= NF; ++i) if ($i in patts[j]) ...

Complete code using 2D arrays with two or more patterns check on the same record in any order:
http://www.tutorialspoint.com/execute_bash_online.php?PID=0Bw_CjBb95KQMaTBHTlY3LXRaTFE

awk 'NR==FNR{for (i=1;i<=NF;i++) patts[FNR][$i]=1;f1=FNR;next}    #all patterns are held in a 2D array in format patts[1][patt1] patts[1][patt2], etc
#Reading of the data file
 {for (k=1;k<=f1;k++) #iterating through patts array [FNR] indices
  {  
    {gotit=0;for (r=1;r<=NF;r++) if ($r in patts[k]) gotit++}    #Iterating through all fields of curret data file record
    {if (gotit==2) found[k]?found[k]=found[k] ORS $0:found[k]=$0}    #If two indices of patts[k] match fields of $0. Can be changed to >2, >3, etc  
  }
}
END{for (l in found) {print found[l];print "-----"}}' patterns file1   


#------------#-------------##------------#-------------##------------#-------------##------------#-------------#
AWK:PRINT print a line before the pattern match and also the line containing the pattern 'Solaris': 
$ awk '/Solaris/{print x;print;next}{x=$0;}' file

Every line read is stored in the variable x. Hence, when the pattern matches, x contains the previous line. 
And hence, by printing the $0 and x, the current line $0 and the previous line x is printed. 

#------------#-------------##------------#-------------##------------#-------------##------------#-------------#
AWK:PRINT previous line, the pattern matching line and next line
$ awk '/Solaris/{print x;print;getline;print;next}{x=$0;}' file
x: is a var, containing the previous line , becasure of the assignment in the end x=$0
print: just prints the current line (stored at $0)
getline : stores next line to $0.
The last print prints again $0 which now is the next line due to previous getline  

#------------#-------------##------------#-------------##------------#-------------##------------#-------------#
AWK:REPLACE - DELETE STRING
awk '{ gsub(",","",$3); print $3 }' /tmp/data.txt
Replaces , with null in field $3 and then prints $3
If $3 is not provided , then gsub operation is performed in $0=Whole Record = Whole line
Alternative :$3=""

#------------#-------------##------------#-------------##------------#-------------##------------#-------------#
AWK:PRINT IF-THEN
https://www.yumpu.com/en/document/view/25827537/sed-and-awk-101-hacks#
awk -F'"' '{if ($0!="") print $1"\""$2" || "$2"\""$3;}' e.txt
awk '{if ($3 =="" || $4 == "" || $5 == "")print "Some score for the student",$1,"is missing";'}' student-marks
awk '$4<900 || $5<=5' file.txt #Prints the records in which $4 is less than 900 and $5 is -le 5
#Can combined with {print $2} to print a particular field instead of the whole line.
awk '$3==$4' file #prints records/lines only if $3 field = $5 field
awk '$2=="Tennis" #prints lines where #2 is tennis
awk '$2 ~ "Tennis" #prints lines where #2 contains tennis
#------------#-------------##------------#-------------##------------#-------------##------------#-------------#
AWK:IF-THEN-ELSE
http://www.thegeekstuff.com/2010/02/awk-conditional-statements
1. awk '{if ($3 >=35 && $4 >= 35 && $5 >= 35) print $0,"=>","Pass";else print $0,"=>","Fail";}'

#------------#-------------##------------#-------------##------------#-------------##------------#-------------#
AWK:IF-THEN-ELSE AND REPLACE

awk -F'"' '{if ($0=="") print $0;else {a=$2;gsub("lbk_addcolumn","ColumnAdd",$2);gsub("lbk_dropcolumn","ColumnDrop",$2);print $1"\""$2" || "a"\""$3;}}' e.txt

# Alternative - more easy to read:
# $ a=$(awk -F'"' '{if (($0=="")) print $0;else print $1"\""$2" || "$2"\""$3}' file.txt)
# $ sed 's/lbk_addcolumn/ColumnAdd/;s/lbk_dropcolumn/ColumnDrop/' <<<"$a
# Or Even with process substitution: 
# $sed 's/lbk_addcolumn/ColumnAdd/;s/lbk_dropcolumn/ColumnDrop/' <(awk -F'"' '{if (($0=="")) print $0;else print $1"\""$2" || "$2"\""$3}' file.txt)

This prints blank lines straight away (unprocessed) and performs all operation if lines are not blank.
You need to apply {} in if-then-else if you need more than one commands to be executed as a group. 
If you dont enclose commands in{} only the first command (up to first ;) is executed and then if condition check stops. 
Rest commands will be executed as a continuation of the code after if-then-else

For Input like this : 	
x;x;x;x;x;x;cmd="lbk_addcolumn TABLE_NAME_1 COLUMN_X";x;x;x;x

x;x;x;x;x;x;cmd="lbk_dropcolumn TABLE_NAME_2 COLUMN_Y";x;x;x;x

Gives Output like this:	
x;x;x;x;x;x;cmd="ColumnAdd TABLE_NAME_1 COLUMN_X || lbk_addcolumn TABLE_NAME_1 COLUMN_X";x;x;x;x

x;x;x;x;x;x;cmd="ColumnDrop TABLE_NAME_2 COLUMN_Y || lbk_dropcolumn TABLE_NAME_2 COLUMN_Y";x;x;x;x
(Ps: mind that blanc line is preserved - not processed. )

#-----------------------------------------------------------------------------------------------------------------------
AWK:PRINT last and first line
awk 'NR==1 {first = $0} END {print; print first}' file
Just save the first line to a variable (first in this example). print in the END, by default will print the last $0=last line

#-----------------------------------------------------------------------------------------------------------------------
AWK:SEARCH LIKE GREP WITH GAWK AND VARIABLE
$ i="Mytestserver02"
$ awk /"$i"/'{print $2}' d3.txt # returns the second field , space separated
19
Or even awk /"$i"/ file will bring the matched line (all fields)
Although, experts claim that allowing expansion of a bash var in awk is a bad technique.

This source gives some alternatives: http://cfajohnson.com/shell/cus-faq-2.html#Q24
#-----------------------------------------------------------------------------------------------------------------------
AWK:PRINT lines with unique fields (uniq emulation without sorting)
awk -vFS=";" '!unique[$2]++' c.txt
Will print all lines that have a unique field $2 (; separated)
#-----------------------------------------------------------------------------------------------------------------------
AWK:REPLACEMENT Replace fields in a file using replacement keys in a separate file
http://stackoverflow.com/questions/42084340/compare-a-txt-and-csv-file-and-need-to-replace-with-matching-name-in-csv-file/42087905?noredirect=1#comment71347363_42087905
File 1(txt): [fields:WinSpc:defect]
File 2 (csv - holding keys): WinSpc,projects.winspc  # Field , replacement data
Target : [fields:winspc:defect] (replace uses only part of the new value)
awk solution:
awk 'FNR==NR{split($2,list,"."); replacement[$1]=list[2]; next} \
   {for (i in replacement){ if (match($0,i)) {gsub(i,replacement[i],$0); break} }}1 ' \
      FS="," file2.csv file1.txt
      
My Solution with bash:
$ readarray -t a < <(grep -e "\[fields:" a.txt |cut -d: -f2)
$ for ((i=0;i<${#a[@]};i++));do a[i]=s/${a[i]}/$(grep -e "${a[i]}" b.txt |cut -d, -f2 |cut -d. -f2)/g\;;done
$ sed -f <(echo "${a[@]}") a.txt

#-----------------------------------------------------------------------------------------------------------------------
AWK:ARRAY CREATION:
https://www.gnu.org/software/gawk/manual/html_node/Reference-to-Elements.html#Reference-to-Elements

## These next 2 entries are not one-line scripts, but the technique is so handy that it merits inclusion here.

## create an array named "month", indexed by numbers, so that month[1] is 'Jan', month[2] is 'Feb', month[3] is 'Mar' and so on.
split("Jan Feb Mar Apr May Jun Jul Aug Sep Oct Nov Dec", month, " ")

## create an array named "mdigit", indexed by strings, so that mdigit["Jan"] is 1, mdigit["Feb"] is 2, etc. Requires "month" array
for (i=1; i<=12; i++) mdigit[month[i]] = i

##Check if a var belongs in array with IN operator
$ awk -F'|' 'NR==FNR{check[$0];next} $2 in check' file2 file1
Mind that IN operator actually looks in array indexes and not values.

##Array Testing
awk 'BEGIN{a["one"]=1;a["two"]=2;a["two"]++;a["three"]="tria";!a["three"]++;a["four"]++;!a["three"]++;a["three"]++; \
for (i in a) print i,a[i]; if (2 in a) {print "2 found"} else {print "2 not found"}; \
if ("two" in a) print "two found";print a["one"],a["two"],a["three"],a["four"];print !a["one"],!a["two"],!a["three"],!a["four"]}'

three 3
two 3
four 1
one 1
2 not found
two found
1 3 3 1
0 0 0 0

Mind that the declaration a["three"]="tria" has been overwritten by a["three"]++ which assigns a new numeric value

##Array Chalenge : Identify how 'a[$0]++ or '!a[$0]++' works 
'a[$0]++' prints only duplicate lines (up to two-not more) anywhere in file (no sorting required)
'!a[$0]++' prints only unique lines, anywhere in file (no sorting required)
There is a kind of explanation here: https://unix.stackexchange.com/questions/159695/how-does-awk-a0-work/159697

It is actually a condition check together with array creation, both combined in one command.
But the condition check is not identical to '$0 in a' - if it was only 'a[$0]' without ++ should work - but does not.

.B Let's examine the easy case : a[$0]++
This prints duplicate lines - Lines that are found exactly two times. If a line is repeated for more than 2 times will be printed.

Basics:
'{print $0}' is the default action and will be executed if omitted. This is why awk '1' file will print all lines
Simple test conditions (i.e 'NR==FNR{do this}') do not require the full "if (expr) {}" syntax.
a[$0] is a direct array creation of array a with index $0 = line1 , line2, etc from file - value is nothing.
a[$0]++ does array element creation also but also applies a numeric value (1 for the first time) to the just created array element
The second time that awk will read $0 which was found before, the reference a[$0]++ will add +1 to the existed a[$0] , and it's value will change from 1 to 2.

On the same time, the expression a[$0]++ is evaluated as a condition. Actually the value of a[$0] before the ++ is checked. 
If the value of a[$0] is 1 (meaning that one line has been found two times), 1 means true and awk will perform it's default action (print $0)
But if the value of a[$0] is 0 (line found only once) then 0 means false = no printing.

Actually awk evaluates as true any value different than zero or null. This can be checked by applying a[$0]-- which achieve the same results.

What is important is that condition check takes place before increment (post increment method).
If it was ++a[$0] = pre increment, the condition check would happen after ++ , a[$0] would have value 1 even the first time found.


Mind that if a line appears three times , then a[$0] will return two . Two is also evaluated as true and thus even this third line will be printed.

Actually whatever is not 0 or null OR whatever is set will be handled as true by awk and will be printed, and this applies to strings or even to negative numbers.

.B Examining the '!a[$0]++' case - print unique lines without sorting

Using negation (!) in front of a[$0]++ you don't affect the array creation but you affect condition check result and thus the print $0.

The negation will revert the result of values stored in each a[$0] element. 
In the very first reading, a[$0] is null=unset and returns 0=false.  
With negation ! this will be reverted to 1 and will be printed.Then (pody increment) the ++ operator will add 1 to this element

On the second time that a[$0] will be found , condition checking will see a value of 1 (set from the first time).
Any value different than 0 (1,2,3,-1,-2,-3,whatever) will return true and by ! will be reverted to zero thus will not be printed.
As a result , only true unique lines of the file will be printed, excluding all repeating lines (no matter how many times they do repeat)

The success of this method is based on the following facts:
check of a[$0] value happens before assignment due to post increment a[$0]++ 
AWK will print an array element if has any valid value = not zero , not null , not unset
Negation mark will revert above behavior. All nulls/zeros will become 1 (and printed), and all non zero numbers will become 0 (not printed)

.B Important: 
Using just awk 'a[$0]' file4 is not printing duplicate lines , is not printing unique lines or all lines. Prints nothing.
This happens because without ++ the element a[$0] has a null value (and not 0 nor 1,2,etc) and nulls are evaluated as false thus no printing
On the other hand, using awk '!a[$0]' file4 prints all the lines - no filtering since the initiall null value is reverted to 1 by ! =all printed.
Due to the absence of ++, the value of a[$0] remains null=false

Mind these tests:
# awk '{print a[$0]}' file4     ---> #Prints nothing - null for each line (a block of null lines is printed)

# awk 'a[$0]' file4 		---> Prints nothing. a[$0] will have null value at the first time and this new value will be preserved since there is no assignment like ++.

# awk 'a[$0];{if ($0 in a) print}' file4 or 'a[$0];$0 in a'	---> Prints all the lines of file4. 
This is because the if check is after the array assignment and thus always $0 will be found inside array a

# awk '{if ($0 in a) print};a[$0]' file4 or '$0 in a;a[$0]'	---> Prints correctly repeated/duplicate lines = similar to 'a[$0]++'  
Works because condition check ($0 in a)is done before array assignment.

# awk '!($0 in a);a[$0]' file4 ---> Prints correctly only unique lines for the same reason (check before assignment)
Mind that '!$0 in a;a[$0]' does NOT work- requires parenthesis


# awk 'BEGIN{print !1,!2,!3,!4}' ---> 0 0 0 0					# Direct negation of any number gives 0
# awk 'BEGIN{print a,george}'    ---> 							# Prints nothing - null. a and george are handled as newly declared variables with out a value
# awk 'BEGIN{print !a,!george}'  ---> 1 1						# The null value of vars a and george is reverted and prints 1 1

# awk '{print a[$0]++ ,$0 ,a[$0]}' file4
0 line1 1						# 1st Col prints the value of a[$0]=a["line1"]=0 (not assigned yet). 
0 line2 1						# At the end - 3rd column - it prints the value of a[$0] after has been assigned by ++ = 1
0 line3 1						# If the first argument was ++a[$0] then 1st and 3rd column would have been identical (tested)
0 line4 1						# Obviously the trick to work is based on the value of 1st column=check value before initial assignment
0 line5 1						#  
1 line2 2						# When we reach this line , since has been found before, we find a value of 1 before the new ++
1 line3 2						# Same for this line
2 line2 3						# In this line we found already a value of 2 for a["line2"] since has been increased ++ two times so far

# awk 'a[$0]++' file4
line2							# a[$0] = 1 = print 
line3							# a[$0] = 1 = print 
line2							# a[$0] = 2 = print
Remark: Due to this behavior the use of '$0 in a;a[$0]' is better  to exclude repeated lines since the test is based in condition if $0 belongs to a and not in the value of a[$0]

# awk '{print a[$0], !a[$0]++ ,$0 ,a[$0]}' file4
  1 line1 1						# 1st:a[$0]=null , 2nd: !a[$0]++ = null reverted to 1 and THEN add one , 4th =1 final value of a["line1"] after ++
  1 line2 1						# All lines found with null value at 1st col are reverted by ! to 1 (2nd col) and thus get printed
  1 line3 1
  1 line4 1
  1 line5 1
1 0 line2 2						# here the second column reports zero because 1st col found with a value from previous assignments by ++ (col 1 = 1)
1 0 line3 2						# if it was never assigned a value (like awk '!a[$0]') then here would be also 1 @ 2nd col.
2 0 line2 3						# Since the value of these three last lines is reverted to zero by ! at time accessed by awk they don't get printed

# awk '!a[$0]' file4	---> all lines printed 		# Prints ALL the lines - no filtering, because at time of check a[$0] has null=false reverted to 1=true by ! and thus printed
# awk '{print !a[$0]}' file4	---> all 1		# Print 1 for every line - unique and repeated lines. 
							# No value assignment is ever made , thus even the second time that we found line2 , 
							# has value null which is reverted to 1 and this is why we see all 1 and all lines are printed in previous example


As a result awk '!a[$0]' file4 is based on the return value of 2nd col above and prints lines 1-2-3-4-5 . 
last three lines (2-3-2) had already a value when awk reached them (had been found before) the negation turns this value to 0 = no print

You can instead of a[$0] assign a different field like a[$2] if you want to print duplicate lines in terms of dupplicates in field 2.

To understand how awk boolean evaluation works in above codes, you can also apply !a[$0]-- that will have the same behavior.
Only first time a[$0] will be zero = > reverted => true = print. All the next times will be -1,-2 but since it is not zero is evaluated as true / valid by awk = no print due to negation mark


##More tricky usages
.B awk '!a[$0]++' <==> '(a[$0]++ == 0)'  # Bothe emulate uniq
Prints also unique lines. In this alt syntax, it seems that parenthesis is evaluated arithmetically.
For every line that a[$0]++ is evaluated as null=zero then print it because the whole parenthesis will return true.

.B awk '{seen[$0]++}END{for (i in seen) print seen[i],i}' file4  <===> uniq -c (all lines with No of occurences found)

.B awk '(a[$0]++ == 1)'  #Direct equivalent to uniq -d
Uniq -d prints only repeating lines = lines found more than one time (either 2 times or more).
Above AWK with arithmetic Comparison can achieve this without sorting requirement. 
awk will print "line1" ONE Time as soon as this line has been found again (and thus a[$0]==1) . 
First time a[$0]++ will be evaluated as null=0 and will not print. 
Second time a[$0]++ will be evaluated as 1 and will print. (then will be increased by 1 more - post increment).
Third and more times, a[$0]++ will have a value greater than 1 and will not print.

This is different to awk 'a[$0]++' since in this awk, the a[$0] is evaluated with true or false and not arithmetically.
With 'a[$0]++' we have a print for every line that is not null/0 . So if line1 exists three times will be printed two times.

.B awk 'a[$0]++' <==> awk '(a[$0]++ > 0)'
In alt syntax with arithmetic evaluation the whole parenthesis will be true if a[$0} is greater than zero before increment. 
Zero will be only the first time found thus will return arithmetically false.

.B awk '(a[$0]++ > 1)' 
This will print all lines that they do repeat more than two times. First occurence a[$0] would return zero, second would be found 1, more than two will be found greater than 1 = parenthesis true=print

#-----------------------------------------------------------------------------------------------------------------------
##ARRAYS EXAMPLE - GREP SIMULATION
https://stackoverflow.com/questions/42239179/fastest-way-to-find-lines-of-a-text-file-from-another-larger-text-file-in-bash
(grep -F -f file1 file2 but restricted to field2 of file2)

##Inian Method with classic check of field in hash
.B awk 'FNR==NR{hash[$1]; next}$2 in hash' file1.txt FS='|' file2.txt

file1 has patterns only. file2 has three fields in format field1|field2|field3. 
Target is to print lines from file2 in which field 2 match the patterns of file1.

for the first file (FNR==NR) an array hash is created with index the field 1 of file1 (= the pattern).
If file1 has foo in a line , we would create hash["foo"]. This happens for all the lines / patterns of file1
When next finishes the file1, we go to file2. We check if field 2 ($2) bellongs to the array hash.
If this condition returns true, awk default action is performed (print $0 of file2)
This is equivalent to if ($2 in hash) {print $0}, but written in a smart compact way.

##Alternative grep method by GV
awk 'NR==FNR{a[$0]=1;next}a[$0]' patterns bigfile #replace $0 with $2
This will print all the lines of big file that a[$0] of big file has a regular -different than zero - value
If a[$0] of bigfile is null that means that a[$0] was not found in patterns = array element not created

##grep -v method by GV
awk 'NR==FNR{a[$0]=1;next}!a[$0]' patterns bigfile #replace $0 with $2
All lines in bigfile not found in patterns file will return false which will be reverted to true & printed

.B GV Method Pitfalls:
As a matter of speed even in big files has been proved that the a[$2] vs ($2 in a) is the same speed.
Though my method with checking status of a[$2] has a major pitfall. 
If the $2 of bigfile is not valid in array a, then will return null/zero , will not be printed (that is desiarable).
But on the same time this array element WILL BE CREATED and this means that you consume huge amount of memory without reason.

On the other hand, the method ($2 in a) checks for existance of $2 in indeces of array a but is not creating array elements.

One workaround for a[$2] would be to delete the array element afterwards in order to free space. 
But if you delete a[$2] after print, the second time that same pattern is found in bigfile might not be printed (like grep -m1)

Unless is to be done something like this (not tested)
awk 'NR==FNR{a[$0]=1;next}a[$0]?print:delete a[$0]' patterns bigfile #If a[$0] returns true print $0, else delete a[$0]

##Alternative classic grep not restricted to checking field 2:
awk 'FNR==NR{hash[$1]; next}{for (i in hash) if (index($0,i)) {print; break}}' file1.txt FS='|' file2.txt
Same logic as before (file1 loaded in hash array) but the whole line of file2 ($0) is compared to hash array indexes
This will allow matches even if pattern of file1 has been found at any field of file2 (1-2-3), not only in field 2 as before.
This is directly equivallent 'grep -F -f file1 file2'

Can be done also using match : 
awk 'FNR==NR{hash[$1]; next}{for (i in hash) if (match($0,i)) {print; break}}' file1.txt FS='|' file2.txt

GV Way: 
Based on the return status of a[$1] or a[$2] or a[$3] . If null/zero returns false = no print. If has a value = true = print

awk 'NR==FNR{a[$0]++;next}(a[$1] || a[$2] || a[$3])' patterns bigfile #considering that patterns file has only one field


##AWK:MORE ARRAYS EXAMPLE
[: http://unix.stackexchange.com/questions/344856/take-two-columns-in-a-tab-delimited-file-and-merge-into-one/344880?noredirect=1#comment610394_344880 :]
awk 'BEGIN{FS=OFS=" "}{z[NR]=$1FS$3; print $1,$2}END{for (i=1; i<=NR; i++){print z[i]}}' a.txt


##AWK:CHALLENGE : Modification of awk output with 1
http://awk.freeshell.org/AwkTips
The 1 stands for true and forces awk to print. 
This is usefull in idiomatic coding, when you want to quickly built a kind of expression.
Real Example:
Suppose a file with contents line1-line2-line3

awk '{sub(/ine/,"foo")}' or awk 'sub(/ine/,"foo")'    --> Output will be lfoo1-lfoo2-lfoo3
awk '{sub(/www/,"foo")}' or awk 'sub(/www/,"foo")'    --> Output will be nothing! No substitution made.
awk '{sub(/www/,"foo")}1' or awk 'sub(/www/,"foo");1' --> Output will be all the lines, no matter if the subs succeed or failed.

awk '{sub(/1/,"foo")}1' or awk 'sub(/1/,"foo");1' --> Output will be all the lines; replaced (like line1 = linefoo) or not. 
.B Thus this command with the use of ;1 is directly equivalent to sed 's/pattern/replacement' 

#-----------------------------------------------------------------------------------------------------------------------

AWK:PRINT/SPLIT A FILE BASED ON PATTERN
[: http://unix.stackexchange.com/questions/344890/to-split-files-under-linux-shell/344898?noredirect=1#344898 :]
[: http://www.tutorialspoint.com/execute_bash_online.php?PID=0Bw_CjBb95KQMVXZWWUtrMUx1Ums :]
Consider file with multiple sections like 
Start of test case:test1
a
b
c
Date is feb 12

Target : To print/split each section in it's own file.
awk '/Start of test case/{i++}{print >"a"i}' a.txt
Counter will be increased in the next found pattern "Start of test case". Each section will be sent to file a1, a2,a3,etc
With awk you don't need >>. It seems that the file remains open - or awk buffers the data and send them at one push to file.
For testing : awk '/Start of test case/{i++}{print i,$0}' a.txt
#-----------------------------------------------------------------------------------------------------------------------
AWK:COMBINE CSV FILES - TAB SEPARATED (emulates JOIN)
http://unix.stackexchange.com/questions/345051/concatenate-csv-with-some-shared-columns
$ awk -F"\t" '{a[1]="";{for (i=1;i<=NF;i++) if (i==6 ||i==7) continue;else \
if ($i!="") a[1]=a[1]FS$i;else a[1]=a[1]FS"NaN";print a[1]}}' <(paste b.txt c.txt)
==EQUALS TO ==
$ join --nocheck-order -eNaN -13 -22 -t$'\t' -o 1.1 1.2 1.3 1.4 1.5 2.3 2.4 b.txt c.txt

Output:						
A   B   C   D   E   F   G	
1   2   3   4   5   6   7	
NaN 1   2   NaN 1   2   1	
#-----------------------------------------------------------------------------------------------------------------------
AWK:UNDERLINE HEADERS by Gilles
http://unix.stackexchange.com/questions/349790/how-to-insert-a-new-line-after-the-first-line-in-a-file-that-is-similar-to-it/349885?noredirect=1#comment619305_349885

##Add dashes bellow each field of 1st line (not space nor tab) 
awk '{print} NR==1 {gsub(/[^\t ]/, "-"); print}'	
Incredibly smart: 
The initial {print} prints whatever record comes in awk <==> print $0.
If NR==1 = 1st line, global replacement of all chars that is not a tab nor a space with a dash. Then print
You don't need to calculate any lenght etc. You just replace the whole $0 with dashes , excluding tabs and spaces (delimiters)
When the second line comes in, initial {print} will print $0, but the replacement gsub will be skipped since NR >1

##Variations
awk '{print} NR==1 {gsub(/[^ \t]/, "-"); while (sub(/- -/, "---")) {}; print}'  #Tricky:WIll insert a long dashed line bellow 1st line.
awk '{print} NR==1 {gsub(/[^\t]/, "-"); print}'			#Add a dash bellow every non tab char of line1.

#-----------------------------------------------------------------------------------------------------------------------
AWK:ONELINERS
http://www.pement.org/awk/awk1line.txt
http://www.pement.org/awk/
http://awk.freeshell.org/AwkTips

Tip: print is the default action of awk, so even if ommited print will be performed.
Tip: In condition checks it is usuall that /string/ refers to patterns . For fixed checks do not ude /../ but simmply =="..."

sort -u file | awk 'NR==FNR{seen[$1]++;next}seen[$1]==1' - file 	
# Use of awk combined with pipe/stdin and a file *actually the same file. Also possible with awk '...' file file (doube pass)
# http://unix.stackexchange.com/questions/348252/print-if-for-the-same-1st-field-theres-a-single-value-of-2nd-field-on-all-lines

awk '1; NR%3==0 {print "----"}' file			
# Prints some dashes every three lines. Actually is a division modulo NR(lines) / 3 = 0. Modulo will be zero for NR 3,6,9,etc (multipliers of 3) and non-zero in all other numbers.

awk 'FNR==1 && $1==8 && $2>500{print FILENAME}' *.csv 	
# Prints file names of all the files satisfying all three criteria (Field 1 to be 8 and field 2 to be more than 500 at first line of each file)

awk 'BEGIN{FS="";OFS="-"}$1=$1' 			#Input chars will be transformed to dash separated. By setting FS to null "" awk treats each char as a field. 
							#By $1=$1 you force the output to be redisigned acc to new different OFS="-"

awk '{print $1}' file 					#Print first field for each record in file
awk '{print FNR "\t" $0}'				#Print line numbers in each file = cat -n
awk '/regex/' 					        #print only lines which match regular expression (emulates "grep")
awk 'BEGIN {IGNORECASE=1};/regexp/'			#case insensitive grep
awk '!/regex/' 						#print only lines which do NOT match regex (emulates "grep -v")
awk '$2 == "foo"' file					#Print any line where field 2 is equal to "foo" in file
awk '$2 != "foo"' file					#Print lines where field 2 is NOT equal to "foo" in file
awk '$1 ~ /regex/' file					#Print line if field 1 matches regex in file
awk '$1 !~ /regex/' file				#Print line if field 1 does NOT match regex in file
awk 'if ($0 ~ /edu/ && $0 ~ /li/) print'		#Two Patterns - Boolean AND
awk ‘$0 ~ /foo/ && ($2 == bar++)’			# AND Operation. Var bar is not incremented if no substring ‘foo’ is found on record. 
/foo|bar|baz/  { buzzwords++ }END{print buzzwords, "buzzwords seen" }	#regex patter with or + counting + printing of counts
awk '$7  ~ /^[a-f]/'    				#print line if field #7 matches regex
awk '$7 !~ /^[a-f]/'    				#print line if field #7 does NOT match regex
awk 'NR!=1{print $1}' file				#Print first field for each record in file excluding the first record
awk 'END{print NR}' 					#count lines (emulates "wc -l").Actually prints NR after end of file. 
awk '$1 > $2 {print $1,$2,$1-$2}' filename		#prints Field1 , Field2 and difference $1-$2 for all lines that $1>$2

awk '/foo/{n++}; END {print n+0}' file			#Print total number of lines that contain foo
awk '{total=total+NF};END{print total}' file		#Print total number of fields in all lines

awk '/regex/{getline;print}' file			#Print line immediately after regex, but not line containing regex in file
awk 'length > 32' file					#Print lines with more than 32 characters in file

awk 'NR==12' file					#Print line number 12 of file
awk '{s=0; for (i=1; i<=NF; i++) s=s+$i; print s}'  	#print the sums of the fields of every line
awk '{for (i=1; i<=NF; i++) s=s+$i}; END{print s}' 	#print the sums of the fields of ALL lines

awk 'BEGIN{ORS="\n\n"};1'				#double space a file - also awk '1;{print ""}'
awk '1;{print "\n"}'					#triple space a file

awk '{print FNR "\t" $0}' files* 			#precede each line by its line number FOR THAT FILE (left alignment).
awk '{print NR "\t" $0}' files* 			#precede each line by its line number FOR ALL FILES TOGETHER, with tab.
awk '{printf("%5d : %s\n", NR,$0)}'			#number each line of a file (number on left, right-aligned)
x == y ? a[i++] : b[i++]				#The famous conditions expressions - mind the trick to increment array indexes
awk 'NF{$0=++a " :" $0};1' 				#number each line of file, but only print numbers if line is not blank

awk '/Beth/{n++}; END {print n+0}' file           		#print the total number of lines that contain "Beth"
awk '$1 > max {max=$1; maxline=$0}; END{ print max, maxline}'   #print the largest first field and the line that contains it
awk '{ print NF ":" $0 } ' 					#print the number of fields in each line, followed by the line
awk '{ print $NF }' 						#print the last field of each line
awk '{ field = $NF }; END{ print field }'			#print the last field of the last line
awk ’length($0) > 80’ data     					#print lines with length more than 80
awk ’NF > 0’ data      		    			        #print any line with at least one record=no blank line
awk 'NF > 4' 						        #print every line with more than 4 fields
awk '$NF > 4' 							#print every line where the value of the last field is > 4

awk 'BEGIN{while (a++<513) s=s " "; print s}' 			#create a string of a specific length (e.g., generate 513 spaces)
awk 'BEGIN { str=sprintf("%40s", " ");gsub(" ","-",str);print str }' #Will print 40 spaces and then convert to 40dashes 
gawk --re-interval 'BEGIN{while(a++<49)s=s " "};{sub(/^.{6}/,"&" s)};1' #insert 49 spaces after column #6 of each input line.
awk '{print} NR==1 {gsub(/[^\t ]/, "-"); print}'	#Tricky by Gilles: Add dashes bellow each field of 1st line (not space nor tab) 
awk '{print} NR==1 {gsub(/[^ \t]/, "-"); while (sub(/- -/, "---")) {}; print}'  #Tricky:WIll insert a long dashed line bellow 1st line.
awk '{print} NR==1 {gsub(/[^\t]/, "-"); print}'			#Add a dash bellow every non tab char of line1.


awk '{ print $NR }'	    	                #NR is number of records/lines. Will print $1 for 1st line, $2 for second line, etc
awk 'NR < 11' 					#print first 10 lines of file (emulates behavior of "head")
awk 'NR>1{exit};1' 				#print first line of file and then exits(emulates "head -1")
awk '{y=x "\n" $0; x=$0};END{print y}' 		#print the last 2 lines of a file (emulates "tail -2"). Alt: y=x ORS $0
awk 'END {print}' 				#print the last line of a file (emulates "tail -1")
awk 'END {print NR}' 				#print the number of lines of a file (emulates wc -l <file)
awk '$5 == "abc123"' 				#print any line where field #5 is equal to "abc123"
awk '$5 != "abc123"' OR awk '!($5 == "abc123")'	#print only those lines where field #5 is NOT equal to "abc123"-alt:'!($5 == "abc123")'
awk '/regex/{print x};{x=$0}' 			#print the line immediately before a regex, but not the line containing the regex
awk '/regex/{getline;print}' 			#print the line immediately after a regex, but not the line containing the regex
awk '/AAA/ && /BBB/ && /CCC/' 			#grep for AAA and BBB and CCC (in any order on the same line). Works even with ||
awk '/AAA.*BBB.*CCC/' 				#grep for AAA and BBB and CCC (in that order)
awk '/regex/,0' OR awk '/regex/,EOF'		#print section of file from regular expression to end of file - alternative awk '/regex/,EOF'
awk 'NR==8,NR==12' 				#print section of file based on line numbers (lines 8-12, inclusive)
awk 'NR==52 {print;exit}'  			#print line number 52 - efficient on large files. Alternative awk 'NR==52'
awk '/Iowa/,/Montana/' 				#print section of file between two regular expressions (inclusive-case sensitive)
awk '$1 ~ /foo/ { print $0 }' BBS-list  	#print if field1 matches pattern foo

awk NF 						#delete ALL blank lines from a file (same as "grep '.' ") - alternative awk '/./'
awk 'NF--'              			#removes last field and prints the line. Actually reconstructs the record with -1 field
awk 'a !~ $0; {a=$0}' 				#remove duplicate, consecutive lines (emulates classic "uniq")
awk '!a[$0]++' OR awk '!($0 in a){a[$0];print}' #remove duplicate, nonconsecutive lines - even unsorted!! = Advance uniq 

gawk -v BINMODE="w" '1' infile >outfile 	#IN DOS ENVIRONMENT# convert DOS newlines (CR/LF) to Unix format. Alternative: tr -d \r <infile >outfile
awk '{sub(/^[ \t]+/, "")};1' 			#delete leading whitespace (spaces, tabs) from front of each line aligns all text flush left
awk '{sub(/[ \t]+$/, "")};1' 			#delete trailing whitespace (spaces, tabs) from end of each line

awk 'BEGIN{system("a=5 && echo system a=$a")}'	#Will create a shell variable but will only be available in the subshell - not parrent.
						#Another alternative is to export to a file in form a="5" and then source the file.

awk '{print (NF? ++a " :" :"") $0}'		# number each line of file, but only print numbers if line is not blank
awk '{for (i=1; i<=NF; i++) if ($i < 0) $i = -$i; print }'	# print every line after replacing each field with its absolute value
awk '{for (i=1; i<=NF; i++) $i = ($i < 0) ? -$i : $i; print }'	# print every line after replacing each field with its absolute value
awk '{ total = total + NF }; END {print total}' file		# print the total number of fields ("words") in all lines
awk '{sub(/\r$/,"")};1'						#IN UNIX ENVIRONMENT: convert DOS newlines (CR/LF) to Unix format. Assumes EACH line ends with Ctrl-M
awk '{sub(/$/,"\r")};1'						# IN UNIX ENVIRONMENT: convert Unix newlines (LF) to DOS format
awk 1								# IN DOS ENVIRONMENT: convert Unix newlines (LF) to DOS format

awk '{gsub(/^[ \t]+|[ \t]+$/,"")};1' 				#delete BOTH leading and trailing whitespace from each line
awk '{$1=$1};1'           					#also removes extra space between fields

awk '{sub(/^/, "     ")};1' 					#insert 5 blank spaces at beginning of each line (make page offset)
awk '{printf "%79s\n", $0}' file* 				#align all text flush right on a 79-column width
awk '{l=length();s=int((79-l)/2); printf "%"(s+l)"s\n",$0}' file* 	#center all text on a 79-character width

awk '{sub(/foo/,"bar")}; 1'           					#replace "foo" with "bar" only 1st instance
gawk '{$0=gensub(/foo/,"bar",4)}; 1'  					#replace "foo" with "bar" only 4th instance
awk '{gsub(/foo/,"bar")}; 1'          					#replace "foo" with "bar" ALL instances in a line

awk '/baz/{gsub(/foo/, "bar")}; 1' 					#substitute "foo" with "bar" ONLY for lines which contain "baz"
awk '!/baz/{gsub(/foo/, "bar")}; 1' 					#substitute "foo" with "bar" EXCEPT for lines which contain "baz"
awk '{gsub(/scarlet|ruby|puce/, "red")}; 1' 				#change "scarlet" or "ruby" or "puce" to "red"
awk '{a[i++]=$0} END {for (j=i-1; j>=0;) print a[j--] }' file* 		#reverse order of lines (emulates "tac")
awk '/\\$/ {sub(/\\$/,""); getline t; print $0 t; next}; 1' file* 	#if a line ends with a backslash, append the next line to it (fails if there are multiple lines ending with backslash...)
awk -F ":" '{print $1 | "sort" }' /etc/passwd  		#print and sort the login names of all users
awk '{print $2, $1}' file 				#print the first 2 fields, in opposite order, of every line
awk '{temp = $1; $1 = $2; $2 = temp}' file 		#switch the first 2 fields of every line
awk '{ $2 = ""; print }' 				#print every line, deleting the second field of that line
awk '{for (i=NF; i>0; i--) printf("%s ",$i);print ""}'	#print in reverse order the fields of every line
awk 'ORS=NR%5?",":"\n"' file 				#concatenate every 5 lines of input, using a comma separator between fields
awk '$3 != "None" && $4 != "None" {print $2}'		#Print if two fields match the same pattern
awk 'length < 64'					# print only lines of less than 65 characters

echo This++++this+++is+not++done | awk '{gsub(/\++/," ");}1' ==> Removes all +. Actually replaces two ++ with a space
echo This++++this+++is+not++done | awk -F'++' '{$1=$1}1' ==> Same as above (http://stackoverflow.com/questions/14432209/substitute-a-regex-pattern-using-awk). This works because OFS is not assigned = default = space. The $1=$1 is reconstructing the output record with the use of OFS.

awk 'BEGIN{OFS=FS="|"}{$0=$3OFS$2OFS$1}{print $1}' file7 	# This reassigns $0 = swaps original field1 with 3. After swap, awk prints field $1 of the new $0 ==> $3 of original file

awk '!a[$0]++' file1 file2 file3				#This prints unique lines among all files even if UNSORTED!! http://unix.stackexchange.com/questions/159695/how-does-awk-a0-work

awk 'a[$0]++' file1 file2 file3					#This prints all non unique lines among all files even if UNSORTED!!

awk '(a[$0]++ == 1)'  						#Direct equivalent to uniq -d - Arithmetic Evaluation - not boolean

awk 'NR==FNR{a[$2];next}$2 in a' file1 file2     	#grep -f operation - exact match of $2 / file1 with $2/file2. Prints matches. 

awk 'NR==FNR{a[$2]=1;next}a[$2]' file1 file2		#Same as above but even more tricky. This works justs by boolean checking if a[$2] when reading the second file has a value or not (set by file1). Null value=No Value = False=no print.


awk 'NR==FNR{a[$2];next}!($2 in a)' file1 file2    		        #like above but prints unmatched results = grep -v -f. 
awk '{if (NR==FNR){a[$2];next};if (!($2 in a)) print}' file1 file2 	#Long Equivalent of the above short oneliner.

awk 'BEGIN{OFS=FS="|"}{if (FILENAME=="file8") $0=$1OFS$1OFS$1}!a[$2]++' file8 file7           # This reassings $0 of file8 in order $2 to match $2 of file 7 and then removes duplicates based on $2.

awk 'BEGIN{OFS=FS="|"}{if (FILENAME=="file8") {$0=$1OFS$1OFS$1} else {b=$0}}{if (a[$2]++) print b}' file8 file7 # Prints duplicates of file 8 (patterns) and file7 (data). 
Bug is that comparison is made on field2 (in both files) so if data file7 has same $2 with different $1 and $3 will be exluded also.

awk 'BEGIN { cnt = 10; cnt += 10; print "Counter =", cnt }'       #cnt is increased by 10,prints 20. Also available: -= ,*= (multiply),/= (divide),%= (modulo), ^= or**=(raise)

awk '{ $6 = ($5 + $4 + $3 + $2) ; print $6 }' inventory-shipped   # Assign contents to a field, even if the field is out of normal range

echo 1 | awk '{ printf("%02d\n", $1) }' 		# Prints numbers with leading zero. If number is >10 then no leading zero is added.


-------------------------------------------------------------------------------------------------------------------------------------

AWK:READ FROM 2 FILES
##Basics
Consider the following two files:

# cat file1
f1d1 f1d2 f1d3 f1d4
f1d5 f1d6 f1d7 f1d8
f1d9 f1d10 f1d11 f1d12

# cat file2
f2d1 f2d2 f2d3 f2d4
f2d5 f2d6 f2d7 f2d8
f2d9 f2d10 f2d11 f2d12

The main idea is that the files are read in series by awk. So the common workaround is to make a check if NR == FNR.
NR is the line counter continiously increasing along all files. FNR is per file counter = resets on every new file.
The condition NR==FNR is true only for the first file, because on second file the NR keeps increasing and differs from the resetted FNR:

awk '{print "NR:",NR,"FNR:",FNR,"fname:",FILENAME,"Field1:",$1}' file1 file2
NR: 1 FNR: 1 fname: file1 Field1: f1d1
NR: 2 FNR: 2 fname: file1 Field1: f1d5
NR: 3 FNR: 3 fname: file1 Field1: f1d9
NR: 4 FNR: 1 fname: file2 Field1: f2d1
NR: 5 FNR: 2 fname: file2 Field1: f2d5
NR: 6 FNR: 3 fname: file2 Field1: f2d9

This is why this usage is frequent: awk 'NR==FNR {file1[$1]=$0; next} {$1=file[$1]; print}' file1 file2

##Combine files with no common keys:
You just need a way to store the records/lines from first file to memory, in order to be printed later during processing of file2..

awk 'NR==FNR {c++;a[c]=$1;next}{d++;print $1,a[d]}' file2 file1
f1d1 f2d1 #Combined first fields of line 1 of both files
f1d5 f2d5
f1d9 f2d9

You can assign to array a whatever field you need from first file (file 2 in example above)
This is different from the getline var <file method which allways will send in var the whole line = $0
The next forces awk to go on the next record and the second group of commands {print..} is not performed. 

When all the records of the first file are finished by the next, then we enter at the next group of commands{d++;print...} for the second file.
Obviously the next command is valid for the file in process.When it finishes, the script goes on to second group of commands.

Keeping more field from first in the read file:
# awk 'FNR==NR{c++;a[c]=$2 FS $3;next}{d++;print $0, a[d]}' file2 file1
# here we have combined fields in each printed line from both files
f1d1 f1d2 f1d3 f1d4 f2d2 f2d3 
f1d5 f1d6 f1d7 f1d8 f2d6 f2d7
f1d9 f1d10 f1d11 f1d12 f2d10 f2d11

Smart Way: Instead of double counters you can just use
awk 'FNR==NR{a[FNR]=$2 FS $3;next}{print $0, a[FNR]}' file2 file1

a[FNR] will be a[1] for field record 1 of first file2, a[2] for FNR2 , etc

The trick is that when first file2 finishes and second file in the row (file1) is processed , FNR resets back to 1 and thus the calls to a[FNR] are valid.
This will break if file1 and file2 have a different number of rows.
Break means that an invalid call to a[] will print nothing - no error is given (i.e try {print $0,a[500]})

##Combine Files with a common key
This is the same , but in a smarter way you store first file in array with key that equals to the common entry:
Join of bellow files in common field (5 of file 1 - 1 of file 2)
# cat file1
f1d1 f1d2 f1d3 f1d4 f2d1
f1d5 f1d6 f1d7 f1d8 f2d5
f1d9 f1d10 f1d11 f1d12 f2d9
# cat file2
f2d1 f2d2 f2d3 f2d4
f2d5 f2d6 f2d7 f2d8
f2d9 f2d10 f2d11 f2d12

# awk 'FNR==NR{a[$1]=$2 FS $3;next}{ print $0, a[$5]}' file2 file1
f1d1 f1d2 f1d3 f1d4 f2d1 f2d2 f2d3
f1d5 f1d6 f1d7 f1d8 f2d5 f2d6 f2d7
f1d9 f1d10 f1d11 f1d12 f2d9 f2d10 f2d11

Now the calls to array are valid, since the elements are stored in a[$1] of file2 = a[f2d1] (for first line)
So when the second file1 is read, the call to a[$5] = a[f2d1] = the stored values of first file2 are printed.

----------------------------------------------------------------------------------------------------------------------------------
AWK:REGEX MATCH AND REPLACE on XML Files

##XML Replace No1
http://stackoverflow.com/questions/42307534/get-attributes-of-a-parent-tag-in-xml/42322481#42322481
Consider a file with various xml entries like:
<tile x="765" y="491" z="7">
    <item id="2114"/>
</tile>

Below Code gets item id and brings the x-y-z values for all fields. tile x-y-z are not always the previous row.

term="2114";awk -v term=$term '{a[NR]=$0; if ($0 ~ term) {i=NR;while (a[i] !~ "<tile x=.+ y=.+ z=.+") i--; print gensub(/(<tile) (.+)(>)/,"\\2","g",a[i]),":",gensub(/([ ]*)(<item )(.+)(\/>)/,"\\3","g",$0)}} ' file5
>x="765" y="491" z="7" : id="2114"
The logic : If you find the search term we look for, then go back to the stored data and when you find a line with x-y-z data print it together with the item id

Mind the regex here:
gensub(/([ ]*)(<item )(.+)(\/>)/,"\\3","g",$0)
First part=/([ ]*)(<item )(.+)(\/>)/
This is divided in four groups:
([ ]*) #white space before item
(<item ) #literal string match
(.+) #any char following previous <item
(\/>) #closing of the line />

By using parenthesis you can group your regex; You can later select which part of regex you need to call (we call 3rd group = id="2114")

##XML Replace No2 - https://goo.gl/DL2hBl
$ cat file
<word key="ACTIVE" group="Service application" value="testvalue1"/>
<word key="ACTIVE" group="Service application" value="testvalue2"/>
<word key="ACTIVE" group="Service application" value="testvalue3"/>
<word key="ACTIVE" group="Service application" value="testvalue4"/>
<word key="ACTIVE" group="Service application" value=""/> #mind the empty value

$ awk '{print gensub(/(value=")(.+)(".+)/,"\\2","g",$NF)}' file
testvalue1
testvalue2
testvalue3
testvalue4
            #Mind the missing field due to missing value on last line of file

$ awk 'FNR==NR{a[FNR]=$0;next}{print gensub(/(value=")(.*)(".+)/,"\\1"a[FNR]"\\3","g",$NF)}' file2 file ##file2 has 4 lines = newvalue1..4
value="newvalue1"/>
value="newvalue2"/>
value="newvalue3"/>
value="newvalue4"/>
value="newvalue5"/>

$ awk 'FNR==NR{a[FNR]=$0;next}{$NF=gensub(/(value=")(.*)(".+)/,"\\1"a[FNR]"\\3","g",$NF);print}' file2 file #Working with groups
# OR without groups
$ awk 'FNR==NR{a[FNR]=$0;next}{$NF=gensub(/value=".*"\/>/,"value=\""a[FNR]"\"\/>","g",$NF);print}' file2 file #No groups
<word key="ACTIVE" group="Service application" value="newvalue1"/>
<word key="ACTIVE" group="Service application" value="newvalue2"/>
<word key="ACTIVE" group="Service application" value="newvalue3"/>
<word key="ACTIVE" group="Service application" value="newvalue4"/>
<word key="ACTIVE" group="Service application" value="newvalue5"/> #Mind that even the empty value is caught and replaced.

This code assumes (acc to OP) that new values are loaded in a new file2 with so many lines as original xml file.
Since we modify the NF valuer, this code will work only if "value" is the last field of each line = awk $NF


AWK:GREP EMULATION MULTIPLE FILES
http://stackoverflow.com/questions/42415826/how-to-get-two-files-having-max-difference-among-a-series-of-files/42419795#42419795
bash while loop and awk 'NR==FNR{a[$2];next}!($2 in a)' file1 file2 per pair of files = 30 awks

SubRoutines Testing

awk -v file1="20161202.csv" -v file2="20161203.csv" 'BEGIN{while (getline var <file1) {split(var,ff1,OFS);a[ff1[2]]}; \
while (getline var2 <file2) {split(var2,ff2,OFS);if (!(ff2[2] in a)) print var2}}'
123456 50000 some value
123457 70000 some value
#This is indeed the different rows between file1 - file2 given as fixed strings here

awk -v file="20161202.csv" 'BEGIN{FS="";split(file,fn,FS);print fn[1],"-",fn[2],"-",fn[3]}' 		#Splits filename of file by chars -output= 2-0-1). You can restrore FS later if you wish by using FS=" "

awk -v file="20161202.csv" 'BEGIN{FS="";split(file,fn,FS);year=fn[1]fn[2]fn[3]fn[4];month=fn[5]fn[6];day=fn[7]fn[8]; \
print year"--"month"--"day}' 				#prints 2016--12--02

awk -v file="20161202.csv" 'BEGIN{split(file,fn,"");year=fn[1]fn[2]fn[3]fn[4]; \
month=fn[5]fn[6];day=fn[7]fn[8];file1=sprintf("%s%s%02d%s",year,month,day+1,".csv");print file1}'	#Prints 20161203.csv = next day file
Replacing print with ;print file1,(getline < file1 < 0)?"not exists":"exists"}'   prints new day filename and if exists or not (!)

awk -F: '{if(system("[ ! -d " $6 " ]") == 0) {print $1 " " $3 " " $7}}' /etc/passwd 	# Checks with system call if filename ($6) is directory

awk -v f="20121202" 'BEGIN{match(f,/(....)(..)(..)/,arr);print arr[1],arr[2],arr[3],arr[4]}'

awk -v file1="20161201.csv" 'function sepname(file) {match(file,/(....)(..)(..)/,arr);print arr[1];return (arr[2])}BEGIN{print file1,sepname(file1),arr[3]}'
2016
20161201.csv 12 01

awk -v file1="20161201.csv" 'function incfile(file,days) {match(file,/(....)(..)(..)/,fn);newfile=sprintf("%s%s%02d%s",fn[1],fn[2],fn[3]+days,".csv");return (newfile)};BEGIN{print incfile(file1,3)}'
20161204.csv

awk '{max=($2>max)?$2:max};END{print max}' 20161203.csv 		#Prints the max value of $2 inside csv file

##Compare two consecutives files respecting that second file must be no more than +1 day of the first file
http://stackoverflow.com/questions/42415826/how-to-get-two-files-having-max-difference-among-a-series-of-files/42450786#42450786

    awk -v file1="20161201.csv" \
    'function incfile(file,days)                                        #function receives two arguments: file and days
        {
        match(file,/(....)(..)(..)/,fn);                                #splits the string of file to format fn[1]=YYYY,fn[2]=MM and fn[3]=DD
        newfile=sprintf("%s%s%02d%s",fn[1],fn[2],fn[3]+days,".csv");    #this function increase the filename by days variable
        return (newfile)                                                #i.e file 20161201.csv returns 20161201+days
        };
    BEGIN \
    {
        chkdays=1; 
        while (chkdays<=15)
        {
            {
            file2=incfile(file1,1);                                     #Built filename of file2 by increasing file1 +1 day
            if (getline < file2 < 0)                                    #Check if file2 exists
                {
                print file1,"vs",file2,"skipped:",file2 "  not found";  #Print a help message - can be removed
                chkdays=chkdays+2;                                      #increase days counter for the while loop by 2
                file1=incfile(file1,2);                                 #Increase filename of file1 by 2 days (20161201 will be 20161203)
                file2=incfile(file2,2);                                 #The same for filename of file2 (20161202 will be 20161204)
                }
            else                                                        #if file2 exists
                {
                close(file2);                                           
                print "comparing",file1,"vs",file2; 
                while (getline var <file1)                              #read from file1 a line and assign it to var
                    {split(var,ff1,OFS);a[ff1[2]]};                     #split line from file 2 (var) to fields, and keep the field2 in an array as index
                while (getline var2 <file2)
                    {
                    split(var2,ff2,OFS);                                #same for file2.split the line read (var2) 
                    if (!(ff2[2] in a)) {print ">",var2;l=l+1};         #check if ff2[2] (file2-field2) is not found on the array created by file1-field2
                    }
                if (l>maxd) {maxd=l;maxp=file1 " vs " file2};           #hold/save max different lines found and hold also the files that maxd was found
                file1=file2;                                            #Assign file2 to be file1 in order to repeat the loop
                chkdays=chkdays+1;                                      #Increase check days counter by 1
                delete a;l=0;close(file1);close(file2)                  #unset all necessary vars and close files
                }
            }
        };                                                              #End of BEGIN section
        print "max different lines=",maxd,"found at pair:",maxp         #Print the results
    }'                                                                  #Finished


##Alternatives: The last valid file is kept to be compared with the next consecutive file even if it is >1 day long.

awk -v file1="20161201.csv" \
'BEGIN{days=1;nextday=1;while (days<15){{split(file1,fn,"");year=fn[1]fn[2]fn[3]fn[4]; \
month=fn[5]fn[6];day=fn[7]fn[8];file2=sprintf("%s%s%02d%s",year,month,day+nextday,".csv"); \
if (getline < file2 < 0) {print file2 "---not exists--" "compare of",file1,"vs",file2,"skipped";days=days+1;nextday=nextday+1} \
else {close(file2);print file2 "--found--comparing",file1,"vs",file2; \
while (getline var <file1) {split(var,ff1,OFS);a[ff1[2]]};while (getline var2 <file2) {split(var2,ff2,OFS);\
if (!(ff2[2] in a)) print var2};file1=file2;days=days+1;nextday=1;delete a;close(file1);close(file2)}}}}'

Output:
20161202.csv--found--comparing 20161201.csv 20161202.csv
123457 80000 some value
20161203.csv--found--comparing 20161202.csv 20161203.csv
123457 70000 some value
123458 30000 some value
20161204.csv---not exists--compare of 20161203.csv 20161204.csv skipped
20161205.csv---not exists--compare of 20161203.csv 20161205.csv skipped


Filename split alternative (first two lines above):
awk -v file1="20161201.csv" 'BEGIN{days=1;nextday=1;while (days<15){{match(file1,/(....)(..)(..)/,fn);
year=fn[1];month=fn[2];day=fn[3];file2=sprintf("%s%s%02d%s",fn[1],fn[2],fn[3]+nextday,".csv"); \

------------------------------------------------------------------------------------------------------------------------
