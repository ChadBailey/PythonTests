UTILS:CHEAT SHEET by GV
https://www.gnu.org/software/coreutils/manual/html_node/index.html#Top
https://www.gnu.org/software/coreutils/manual/html_node/tac-invocation.html#tac-invocation
------------------------------------------------------------------------------------------------------------------------
UTILS:BASICS - FILE MANIPULATIONS
Depending on the job you can use different commands:

cut file: slices a line of file (or multi lines) based on a delimiter.
paste file1 file2: Prints field of both files side by side , ignoring matching criteria
diff file1 file2 : Prints differences of files 
sort file1 file2 fileN: Sorts the files. Can be adjusted to sort on a specific columnt (i.e 1.2 = file 1 column2)
join file1 file2 : Joins files in a common field
------------------------------------------------------------------------------------------------------------------------
UTILS:MAN 
##CHEAT SHEET TXT 2 MANPAGE FORMATTING
Read this cheat file with man pages:
http://technicalprose.blogspot.gr/2011/06/how-to-write-unix-man-page.html
man formatting: man 7 man & man 7 man-pages
groff programming: http://web.cecs.pdx.edu/~trent/gnu/groff/groff.html#IDX123
man pages making: https://liw.fi/manpages/
https://linux.die.net/man/1/help2man

##WORKING COMMAND:
man --nj <(h=".TH man 1 2017 1.0 cheats page";sed "1i $h" cheatsheets/utils*gv.txt |sed 's/^UTILS:/.SH UTILS:/g; s/^$/\.LP/g; s/^##/\.SS /g; s/\\/\\e/g;G' |sed 's/^$/\.br/g')
You can also combine with --nh 
PS: man options --nj = not auto justified , --nh = not auto break words with hyphen on line changes.

##MAN AND GROFF/troff require special handling.
man ignores normal line feeds at end of lines ($); empty lines (^$) are recognized and displayed
Line feeds in man pages can be done by inserting .br between two lines.
More .br in series of lines are ignored by man and got intepreted as a single line feed - not multiple new lines.
Man pages should start with a .TH line
Man sections / header start with .SH. 
Subsection start with .SS. Alternativelly you can use .B to make this line bold. .B follows text identation - .SS has it's own idents.
The backslash \ works as escape in groff, so you need to escape the backslash with \e (or \\ can also work)
The example tr -d '\n' will become tr -d '\en' with \e escaping, or will become tr -d '\\n' with \\ escaping.

##MAN - SEE MANPAGES OF NON INSTALLED PACKAGES USING CURL (see also manon shell script)
You can either decompress a .deb in the screen and ask man to display it OR
you can download the .deb package and decompress/display the manpages available and finally delete the .deb unless you plan to tinstall it!

.B On line - No deb download
$ apt --print-uris download agrep
'http://ftp.gr.debian.org/debian/pool/non-free/a/agrep/agrep_4.17-9_amd64.deb' agrep_4.17-9_amd64.deb 194788 SHA256:4d3648e483f9a367b821095c9d6e24016b486723920e4abc34324ad1c89f2867
$ curl -sL -o- 'http://ftp.gr.debian.org/debian/pool/non-free/a/agrep/agrep_4.17-9_amd64.deb' |dpkg -c /dev/stdin  --> Provides a list of all the contents inside deb
$ curl -sL -o- http://ftp.gr.debian.org/debian/pool/non-free/a/agrep/agrep_4.17-9_amd64.deb |dpkg-deb --fsys-tarfile /dev/stdin|tar -t  --> Also provide a list of deb contents

$ curl -sL -o- http://ftp.gr.debian.org/debian/pool/non-free/a/agrep/agrep_4.17-9_amd64.deb |dpkg-deb --fsys-tarfile /dev/stdin|tar -xO ./usr/share/man/man1/agrep.1.gz |man /dev/stdin

The use of dpkg-deb --fsys-tarfile is required since deb packages when unzipped are constisted of two major subfiles.

.B With downloading deb package
$ apt download agrep                           ---> Get:1 http://ftp.gr.debian.org/debian jessie/non-free amd64 agrep amd64 4.17-9 [195 kB]
OR 
$ curl -sLO "http://httpredir.debian.org/debian/pool/main/a/avis/avis_1.2.2-4_all.deb" # curl -O saves the file in CWD

$ dpkg -c $(ls *.deb)                          ---> prints contents of data.tar inside deb file on the screen
$ dpkg-deb --fsys-tarfile $(ls *.deb) |tar -t  ---> also prints data.tar content on the screen
$ dpkg-deb --fsys-tarfile $(ls *.deb) |tar -xO ./usr/share/man/man1/agrep.1.gz |man /dev/stdin -->display the man page correctly on screen

OR
ar -t "$(ls *.deb)"  --> Lists the main packages of the deb wrapper (data.tar.gz/xz , control.tar.gz/xz , debian-binary)
ar -p "$(ls *.deb)" data.tar.gz |tar -zt  --> Listing files . data.tar coule be gz (like agrep) or .xz (in this case we need tar -Jt)
ar -p "$(ls *.deb)" data.tar.gz |tar -xzO ./usr/share/man/man1/agrep.1.gz |man /dev/stdin   --> works ok

#-----------------------------------------------------------------------------------------------------------------------
UTILS:DEB - TAR FILES
Extract any compressed text file from downloaded deb:
apt-get download netcat
ar -p `ls *.deb` data.tar.xz |tar -Jt  #list files 
ar -p `ls *.deb` data.tar.xz |tar -xJO ./usr/share/doc/netcat/changelog.Debian.gz |gunzip |cat (or less or whatever)
OR
ar -p `ls *.deb` data.tar.xz |tar -xJO ./usr/share/doc/netcat/changelog.Debian.gz |man /dev/stdin (man does unzip!)

for some reason tar fails to unzip the last gz file (but gunzip works ok):
ar -p `ls *.deb` data.tar.xz |tar -xJO ./usr/share/doc/netcat/changelog.Debian.gz |tar -xzO |cat

if the file is not compressed you can just run tar -O (output to stdout = screen)
ar -p `ls *.deb` data.tar.xz |tar -xJO ./usr/share/doc/netcat/copyright

curl -sL -o- "http://ftp.gnu.org/gnu/tar/tar-latest.tar.gz" |tar -tz --wildcards --no-anchored '*.h'   #archive file listing with wildcards - avoiding grep
curl -sL -o- "http://ftp.gnu.org/gnu/tar/tar-latest.tar.gz" |tar -zt |grep '/src/.*\.h$'               #classic tar archive file listing
curl -sL -o- "http://ftp.gnu.org/gnu/tar/tar-latest.tar.gz" |tar -xzO --wildcards --no-anchored '*.h'    #extract and dump contents on screen
curl -sL -o- "http://ftp.gnu.org/gnu/tar/tar-latest.tar.gz" |tar -xzf - --wildcards --no-anchored '*.h'  #extract and save files locally in a new directory tar-1.29 (goes into CWD)
curl -sL -o- http://ftp.gr.debian.org/debian/pool/non-free/a/agrep/agrep_4.17-9_amd64.deb |dpkg-deb --fsys-tarfile /dev/stdin|tar -t # Displays files in deb archive, identical to dpkg -c


$ curl -sL -o- "https://github.com/fhcrc/seqmagick/archive/0.6.1.tar.gz" |tar -zt  --> listing of tar archive files
$ curl -sLO "https://github.com/fhcrc/seqmagick/archive/0.6.1.tar.gz"; ls -l *.tar.gz
-rw-r--r-- 1 root root 672141 Apr  3 11:37 0.6.1.tar.gz
$ tar -tf 0.6.1.tar.gz ---> Listing of the archive files of the saved tar

Tip: Using always -L ensure that curl will follow moved links to their new location. For example bellow command fails without -L:
$ curl -o- "https://github.com/fhcrc/seqmagick/archive/0.6.1.tar.gz"
<html><body>You are being <a href="https://codeload.github.com/fhcrc/seqmagick/tar.gz/0.6.1">redirected</a>.</body></html>



#-----------------------------------------------------------------------------------------------------------------------
UTILS:GREP
##EXCLUDE FROM ONE FILE ENTRIES THAT EXIST IN ANOTHER FILE: grep -Fxvf file2.txt file1.txt
-F for fixed string in order to avoid regex (with regex an entry "tar" in file2 will match tar,patar, guitar, etc in file1) 
-x = line grep to ensure matching whole lines 
-v = reverse the grep = display not matching lines
-f = load words/lines from file file2.txt
file1.txt = the file to be grepped 

##MULTI GREP WITH REVERSE OPERATION : grep -v -e "pattern" -e "pattern"
grep -nA1 -e "====" c.txt |grep -B1 -e "====" |grep -v -e ":" -e "--"

##GREP ALL FILES FOR TWO PATTERNS (and operation)
http://stackoverflow.com/questions/41896604/linux-listing-files-that-contain-several-words#41897079
grep -ERl 'toto' | xargs -r grep 'tata'
Tip : xargs -r ensures that second grep will not run in null / non matched data.
Alternative : awk '/pattern1/&&/pattern2/'

##GREP A FILE FOR TWO PATTERNS WITH AGREP BY CHAZELAS:
http://unix.stackexchange.com/questions/55359/how-to-run-grep-with-multiple-and-patterns/55391#55391
agrep 'pattern1;pattern2' file (can also accept pattern3;patternN)
grep -X '.*pattern1.*&.*pattern2.*'
grep -P '^(?=.*pattern1)(?=.*pattern2)'
grep -e 'pattern1.*pattern2' -e 'pattern2.*pattern1'
Alternatives:
awk '/pattern1/ && /pattern2/'
awk '/regexp1/ && /regexp2/ && /regexp3/ { print; }'

##GREP COLORIZE 
grep --color 'pattern' or grep --color -E 'pattern|$'
Or in a function/alias : function highlight () { grep --color -E "$1|$" "${@:1}" }

##GREP MULTIPLE PATTERNS AND PRINT
Combine grep with cut and multipatterns: 
instead of grep |cut combination you can use awk
grep -E 'Label 3 \|Label 5' |cut -d' ' -f3 <==> awk '/Label 3/ { print $3 } /Label 5/ { print $3 }'
PS: AWK can directly print to a file using { print $3 >>"F3.csv" } 

##GREP AN HTML LINK FROM A STREAM
stream=...\f4 HYPERLINK "httjps://archive.org/randomURL1?fref=grp_mmbr_list"}...
grep -Eo '\"https?:\/\/[^"]+\"' testsample.txt 

##MGREP ALL CHARACTER BEFORE THE FIRST WHITESPACE USING GREP?
grep -oP "^\S+" filename.txt

##GREP USING A FILE WITH PATTERNS.
Generally you can import patterns by a file using the -f switch.
Also you can feed the -f with process substitution to feed grep with manipulated patters grep -f <(cut -f2 patterns.txt) a.txt
In more tricky usage you can use the same file as file pattern in order to achieve complex grep results.
For example: 
grep -v -f <(grep "ok" a.txt |cut -d' ' -f1) a.txt
This will search the file a.txt for a second column having a text ok and will get the names marked as ok.
Then these names will be the main pattern to grep and exclude them from the whole,same file
Obviously this is usefull if on the master file a.txt not all the names have ok , but indirectly even if one ok is present for a user, you want to exclude this user from all the results.

##GREP COMBINED WITH FIND
find . -type f -exec grep pattern {} +
grep -r pattern /the/dir
find . -type f -print0 | xargs -r0 grep pattern
LC_ALL=C grep -r pattern .

##GREP MULTIPLE FILES IN PARALLEL:
http://unix.stackexchange.com/questions/197352/how-to-start-multi-threaded-grep-in-terminal
find . -type f -print0  | xargs -0 -P number_of_processes grep mypattern > output #Use find to get the files, invocating xargs in parallel mode.
find . -type f | parallel -j+1 grep mypattern #use the parallel utility
find ./appsfiles/ -type f | parallel -k -j200% -n 2000 -m grep -H -n -e "Exec=" -e "Comment" -e "Generic" -e "Name" {}
PS1: by timing above find |parallel vs a single grep yeld same results on 5000 desktop files.
PS2: This gives 50% MORE time than single grep : time find ./appsfiles/* -type f -print0 | xargs -r0 -P2 grep -H -n -e "Exec=" -e "Comment" -e "Generic" -e "Name" {}

##GREP REGEX OR
grep -E '^([a-z]{1,3}|pom.|i,j,k)$' b.txt
match lines with one word, that have less than 4 letters a-z , and also catch word 'pom.' and 'i,j,k'
can be used as -Eow to match words in a line

##GREP LOOK AHEAD METHOD:
\K: This sequence resets the starting point of the reported match. Any previously matched characters are not included in the final matched sequence.

$ cat /var/log/messages.1 |grep  'sda:'
Mar 10 03:37:14 debian kernel: [    7.122304]  sda: sda1 sda2 sda3 sda4 < sda5 sda6 >

$ cat /var/log/messages.1 |grep -Po 'sd[a-z]+: \Ksd[a-z0-9].*'
sda1 sda2 sda3 sda4 < sda5 sda6 >

$ a="Configuration file 'hello2.conf' is in use by process 735." && echo "$a" |grep -Po 'process \K[0-9]*'
735 (https://regex101.com/r/hxwGPA/1)

##GREP - Count occurences of a char within a string
var="text,text,text,text"
GREP: reps=$(grep -o "," <<< "$var" | wc -l)
AWK Alternative: num=`echo $var |  awk -F"," '{print NF-1}'  #or {print NF?NF-1:0}
BASH Alternative: reps="${var//[^,]}" && echo "${#reps}"  #removes everything except comma


##GREP - Find non matching files instead of non  matching lines
grep -v ‘trivial information’  it will not work because the rest of the lines in the file are match to this inverted search so the file.txt will end up in the result.
This job can be done using grep -L 'trivial information' *  (-L, --files-without-match)
With -L scanning will stop on the first match.
#-----------------------------------------------------------------------------------------------------------------------

UTILS:CUT
https://www.gnu.org/software/coreutils/manual/html_node/cut-invocation.html#cut-invocation

cut can accept delimiter with -d'' and can return a range of fields (i.e -f1 , -f1,3 , -f1-3)
To assign a delimiter of \n,\t you have to use -d$'\n' (similar to IFS)

CUT can act directly on each line of file. You don't need while & read line loop.

Instead of -f option you can use cut to isolate part of text based on chars with -c option:
cut -c9-10 file ==> gets characters from 9 to 10. similary you can get c1-5 to get only the 5 first chars of EACH file line.
Also see this:  cut -c1,9-10 a.txt ==> gets 1st char and then gets chars 9-10

##INSERT DASH IN STRING
 String: #  1  2016-05-31-1003-57S._BKSR_003_CM6
 $ cut --output-delimiter='-' -c7-19,20-21 file.txt
 Output : 2016-05-31-10-03
 alternatives:
 $ awk '{print substr($3,0,13)"-"substr($3,14,2)}' file.txt
 $ while IFS= read -r line;do line="${line:6:13}-${line:14:2}";echo $line;done<file.txt
 $ sed 's/..$/-\0/g' <(cut -d- -f1-4 <(cut -d" " -f5- file.txt)) #use >newfile at the end to send the results to a new file

#-----------------------------------------------------------------------------------------------------------------------
UTILS:DIFF
## COMPARING FILES AND VARIABLES:
diff can compare two files line by line.
You can also trick use diff like this to compare two variables line by line : diff <(echo "$a") <(echo "$b") or diff <(cat <<<"$a") <(cat <<<"$b")

##COMPARE GREP RESULT IN A NICE VISUAL WAY:
diff -u  <(cat a.txt) <(grep "tar*" a.txt) #or -y

##REMOVE THE < char from output and keep only the differet items:
diff --changed-group-format='%<' --unchanged-group-format='' c.txt cc.txt
i.e c.txt 
1
2
3
cc.txt
1
2

##USING DIFF WITH PROCESS SUBSTITUTION 
diff -y <(man grep) <(man agrep) #compares man page of grep to manpage of agrep using -y = side by side
PS: normal usage of diff is diff -y file1 file2.

## DIFF TRICKS:
If you are in doubt about the results of a command like an extra grep you can compare results of previous command with new command like this:
diff -w -y <(apt search manpages |grep "/" |cut -d"/" -f1 |grep -E '^[a-zA-Z0-9]') <(apt search manpages |grep "/" |cut -d"/" -f1)
Differences will be noted with > symbol and then you can manually verify that the results of the new command (extra grep) is as expected.
Usefull when you want to verify the performance in commands that produce large output.

------------------------------------------------------------------------------------------------------------------------
UTILS:HEAD-TAIL
head -n1 : Gets the header - 1st line
tail -n+2 : Gets the second line up to EOF of a file - excludes header
Trick to get only one line from file using head and tail
Usage: bash viewline myfile 4
head -n $2 "$1" | tail -n 1

#-----------------------------------------------------------------------------------------------------------------------
UTILS:JOIN
Joins two files based on certain criteria.
join -a1 -o 1.2     - /dev/null # print the second field
join -a1 -o 1.2,1.1 - /dev/null # reorder the first two fields
See man join and also info join for better explanation

##INTERESTING OPTIONS OF JOIN: 
-a1 and/or -a2 : print non matched lines of file 1 and/or 2
-e EMPTY: replace missing lines EMPTY
-t char: Use char as delimiter
-1.2 : Join on field 2 of file 1
-2.4 : Join on field 4 of file 2
--nocheck_order : Don't complain about unsorted input files
--header : Treat 1st line as header
Tip: Combining join with LC_ALL=C in front, speeds up almost 40%.

##JOIN TWO FILES BASED ON COMMON FIELDS:
http://unix.stackexchange.com/questions/342814/combine-two-csv-file-based-on-condition#342839
join -o 0,1.2,1.3,2.3 <(sort A.csv) <(sort B.csv)
-o: Obey Format - specify a particular format. 
-o 0 : Stands for the merged /common field and is different than 1.1 or 2.1 who stand for file 1 field 1 and file 2 field 2.
If file2 has a line that is not present on file 1, then using -o 1.1 will print an empty space in that field.

##ANOTHER JOIN EXAMPLE: 
Join file b.txt and bb.txt. Preserve the contents of file b.txt and insert new contents from file bb.txt
If file bb.txt has similar parameters with different values, preserve the parameters of the original file b.txt - do not update

$ cat b.txt
parameterA=0
parameterB=1
parameterC=0

$ cat bb.txt
parameterA=0
parameterB=0
parameterC=0
parameterD=0

$ join -a1 -a2 -t= b.txt bb.txt |cut -d= -f1-2
parameterA=0
parameterB=1
parameterC=0
parameterD=0

##MORE JOIN EXAMPLES
File1            |File2
-----------------|------------
Category ID      |Purchase ID
C1  A1           |O1  A1

Resulting File3
O1 A1 C1

join -j 2 -o 2.1 2.2 1.1 File1 File2
Join on field 2 of both files (alternatively you can use -1.2 -2.2)
-o controls the output 

##JOIN CSV FILES (combine)
http://unix.stackexchange.com/questions/345051/concatenate-csv-with-some-shared-columns
$ join --nocheck-order -eNaN -13 -22 -t$'\t' -o 1.1 1.2 1.3 1.4 1.5 2.3 2.4 b.txt c.txt

Output:						
A   B   C   D   E   F   G	
1   2   3   4   5   6   7	
NaN 1   2   NaN 1   2   1

#-----------------------------------------------------------------------------------------------------------------------
UTILS:PASTE : Merge lines of files
https://www.gnu.org/software/coreutils/manual/html_node/paste-invocation.html#paste-invocation
num2=file including 1,2 
let3=file including a,b,c
$ paste num2 let3
1       a
2       b
        c
$ paste -s num2 let3
1       2
a       b       c
$ paste num2 let3 num2
1       a      1
2       b      2
        c

Also see this example: http://unix.stackexchange.com/questions/340691/stacked-records-to-columns/340692?noredirect=1#comment602328_340692
$ paste -d"," <(grep -E '[0-9]' c2.txt) <(grep -E '[a-z]' c2.txt)
Mind the process substitution of the same file. Remember that <(...) creates a temp fd descriptor which acts as a file argument in apps requesting files, like paste.
#-----------------------------------------------------------------------------------------------------------------------
UTILS:COLUMN
Print anything nicely with column program
Example : 'mount |column -t'. # -t stands for table . This does Auto cols detection & separation according to whitespace.
If columns are separated by delimiter, define this delimited by using '-s' option.
For files you can directly run '$column -t file.txt'

#-----------------------------------------------------------------------------------------------------------------------
UTILS:BC Calculator
a="$(<a.txt)" #supposing file "a" exists and has a number inside
b="$(<b.txt)" #similary
bc <<<"$a + $b"
c="$( bc <<<"$a + $b" )"

If the <<< syntax feels weird (it's called a "here-string" and is an extension to the POSIX shell syntax supported 
by bash and a few other shells), you may instead use printf to send the addition to bc:

printf '%s + %s\n' "$a" "$b" | bc
c="$( printf '%s + %s\n' "$a" "$b" | bc )"

##TRICKY PRINTF + bc usage to sum up ascii values:
while read -rn1 char;do sumA+=$(printf '%d+' "'$char'");done <<<"1.Nf3 c5 2.e4 Nc6"
echo "$sumA"              ---->> 49+46+78+102+51+39+99+53+39+50+46+101+52+39+78+99+54+39+ #mind the trailing +
$ bc <<<"${sumA:0:-1}"    ---->> 1114
#-----------------------------------------------------------------------------------------------------------------------
UTILS: CAT AND TAC
Source Code of CAT:
http://git.savannah.gnu.org/gitweb/?p=coreutils.git;a=blob;f=src/cat.c;h=001408576c4c17a156c1e8761ed2d9c96aa3d0cf;hb=refs/heads/master
CAT help and man page are quite good. 
TAC is the opposite of CAT; Prints lines of file upside down (start from last).
cat -A or cat -vet displays non printable chars. Can be done better with |od -t x1c

#-----------------------------------------------------------------------------------------------------------------------
UTILS:LN - Create links.
Combine -s to create symbolic links; otherwise will create hardlinks.
Hardlinks remain when the original file is removed. Symlinks fail.
#-----------------------------------------------------------------------------------------------------------------------
UTILS:NL - Number Lines / Line numbering
Can be used / pipe / file to add numbers in file lines (similar to cat -n but MUCH more advanced).
Has a lot of options like starting number, number step, number acc to regex, etc
Simpliest usage : cat file |nl - default usage is equivalent to cat -b = number non empty lines only.
nl -bp"^$" c.txt --> apply numbering only on empty lines (start and end together)

#-----------------------------------------------------------------------------------------------------------------------
UTILS:TR
Replace new lines with null: 
 tr '\n' '\0' <in >out 
tr '\\n' '\\0' <in >out 
.ft B tr '\n' '\0' <in >out 

Mind the [=C=] synthax. Treats variations of the letter given as same letter . For example [=e=] will treat all 'e' (with tones, etc) as plain e
------------------------------------------------------------------------------------------------------------------------

UTILS:SORT 
http://www.skorks.com/2010/05/sort-files-like-a-master-with-the-linux-sort-command-bash/
Mind the options -u and -k for defininig the sort field (i.e -k2) and -t for setting fields delimiter (i.e -t"|" or -t:, etc)
-k lets you defind the sort key. You can have many -k entries.

Moreover with -k you can define except from the column (-k1) also the letters range to make the sort (i.e -k1,5 = 1st Col, chars 1 to 5)

Tip: To be sure that your -k selection works ok , you can apply for testing i.e -k3r (reverse). This means sort by 3rd column in reverse order.

Example:
root@debi64:/home/gv/Desktop# sort -k1,5 a.txt b.txt
crwxrwxr-x+  - user1 hive          0 2017-01-31 16:55 /user/anc/xyz
drwxrwxr-x+  - user1 hive          0 2017-01-31 16:55 /user/abc
drwxrwxr-x+  - user1 hive          0 2017-01-31 16:55 /user/abc
drwxrwxr-x+  - user1 hive          0 2017-01-31 16:55 /user/anc/xyz
drwxrwxr-x+  - user1 hive          0 2017-01-31 16:55 /user/anc/xyzhrt/rth
drwxrwxr-x+  - user1 hive          0 2017-01-31 16:55 /user/anc/xyzhrt/rth
drwxrwxr-x+  - user1 hive          0 2017-01-31 16:55 /user/anc/xyz/rhr/rthrh
-rw-rwxr--+  3 user1 hive       1462 2017-01-31 16:55 /user/anc/xyz/rhrhheh

root@debi64:/home/gv/Desktop# sort -k1,5 -k8r a.txt b.txt
crwxrwxr-x+  - user1 hive          0 2017-01-31 16:55 /user/anc/xyz
drwxrwxr-x+  - user1 hive          0 2017-01-31 16:55 /user/anc/xyz/rhr/rthrh
drwxrwxr-x+  - user1 hive          0 2017-01-31 16:55 /user/anc/xyzhrt/rth
drwxrwxr-x+  - user1 hive          0 2017-01-31 16:55 /user/anc/xyzhrt/rth
drwxrwxr-x+  - user1 hive          0 2017-01-31 16:55 /user/anc/xyz
drwxrwxr-x+  - user1 hive          0 2017-01-31 16:55 /user/abc
drwxrwxr-x+  - user1 hive          0 2017-01-31 16:55 /user/abc
-rw-rwxr--+  3 user1 hive       1462 2017-01-31 16:55 /user/anc/xyz/rhrhheh


In the second example both files are sorted with key 1 to 5 and then are sorted with key 8 reversed.
Offcourse the results of second key are not capable to change the results of first key.
Is a nested /sub sort procedure to be used instead of pipe (sort -k1,5 |sort -k8)

root@debi64:/home/gv/Desktop# sort -k8 a.txt b.txt
drwxrwxr-x+  - user1 aive          0 2017-01-31 16:55 /user/abc
drwxrwxr-x+  - user1 hive          0 2017-01-31 16:55 /user/abc
root@debi64:/home/gv/Desktop# sort -k8 -k1r a.txt b.txt
drwxrwxr-x+  - user1 hive          0 2017-01-31 16:55 /user/abc
drwxrwxr-x+  - user1 aive          0 2017-01-31 16:55 /user/abc

If you need to extract duplicates based on specific columns ignoring data in the middle , uniq -d will not work.
# sort -k8 -k1r a.txt b.txt |uniq -d
drwxrwxr-x+  - user1 hive          0 2017-01-31 16:55 /user/anc/xyzhrt/rth

You can then call awk to print the columns you need ($1 and $8) and then cut -d will work on output of awk.

##SORT -u option (unique)
sort -u a.txt b.txt == cat a.txt b.txt |sort |uniq
sort -u will exclude the duplicate lines, operation similar to classic uniq but with more criteria when used with -k

root@debi64:/home/gv/Desktop# sort -u a.txt b.txt #default sorting = based on k1 - c letter on first field goes on top
crwxrwxr-x+  - user1 hive          0 2017-01-31 16:55 /user/anc/xyz
drwxrwxr-x+  - user1 aive          0 2017-01-31 16:55 /user/abc
drwxrwxr-x+  - user1 hive          0 2017-01-31 16:55 /user/abc

root@debi64:/home/gv/Desktop# sort -u -k8 -k1 a.txt b.txt
drwxrwxr-x+  - user1 aive          0 2017-01-31 16:55 /user/abc
drwxrwxr-x+  - user1 hive          0 2017-01-31 16:55 /user/abc
crwxrwxr-x+  - user1 hive          0 2017-01-31 16:55 /user/anc/xyz
First sorted with k8 then with k1. abc folders go on top

##SORT EXCLUDING HEADER ROW
This is a common problem, since sort does not have a built switch to exlude header row.
Workarounds:
head -n1 file && sort <(tail -n+2 file) or head -n1 && tail -n+2 file |sort
{IFS= read -r header;echo "$header";sort} < file.csv
command | (read -r; printf "%s\n" "$REPLY"; sort)
awk 'NR == 1; NR > 1 {print $0 | "sort -n"}'
command | head -n 2; command | tail -n +3 | sort #command refers to your command (i.e cat , etc) not to the command utility
ps -ef | { head -1 ; sort ; }
function body { IFS= read -r header;printf '%s\n' "$header";"$@";};ps -o pid,comm | body sort -k2 #or |body grep pattern or |body whatever command you need to apply in the body 

------------------------------------------------------------------------------------------------------------------------
UTILS:DATE
##CONVERT DATE TO EPOCH. Date must be in format 14/Feb/2017:11:31:20
date -d "$(echo $dt | sed -e 's,/,-,g' -e 's,:, ,')" +"%s"
I have a smart function in bash profile for this that can work also as a pipe.

##DELETE LINES FROM LOG FILE OLDER THAN 30 days 
For a file with lines like 
2017/04/04 15:53:22 [11487] building file list)

You can exclude old logs using :
grep -v "$(date '+%Y/%m/%d' -d '30 days ago')*" logfile

------------------------------------------------------------------------------------------------------------------------
UTILS:GIT
https://git-scm.com/book/tr/v2/Git-Basics-Viewing-the-Commit-History
##BASIC COMMANDS
git add .
git commit -m "message"
git push

git pull to retrieve new files from git

##GIT HISTORY
git log : all the commits will be shown. Most recent goes on top.
git log -p -2 : gets only the last 2 commits
git log --stat : statistics about commits, like file changed . Also can be combined with --name-only.

##GIT SHOW FILES IN COMMIT
git show : see the last commit in details
git show --name-only : lists only file names changed
git show commitid :  Details about a particular commit id. Can be combined with --name-only. Commit id can be retrieved by git log

##GIT STORE PASSWORD
https://git-scm.com/docs/git-credential-store
git config credential.helper store
On the next push , your given user name and password will be stored.
Can be combined with options for expire, etc

------------------------------------------------------------------------------------------------------------------------
UTILS:TIME & STRACE
You can monitor performance in bash using the time or the strace utils.
time grep pattern file will  #report you details about the time required to execute command (grep)
strace -cf grep pattern file #will also report performance in a much more analytical way, displaying also all the processes to complete the command.

------------------------------------------------------------------------------------------------------------------------
UTILS:APT
apt list pkg
apt list pkg* # Be carefull that bash filename expansion applies to *. You may need to apply set -f for this one or to double quote it like "pkg*"
apt show pkg  # Show details of pkg
apt search keyword  #Search all pkgs in all repos for the keyword/description given
apt install pkg
apt purge pkg
apt-file search showmount   or /bin/showmount #advises which package you need to install in order to get showmount
OR JUST 
apt search showmount --> nfs-common  #or apt-cache search showmount
apt --print-uris install pkg   # Returns the names of the .deb packages that will be installed in case of apt install. Works also with apt-get
apt --print-uris download pkg  # Returns the deb file that apt download pkg would download:
apt --print-uris download nfs-common  ---> 'http://ftp.gr.debian.org/debian/pool/main/n/nfs-utils/nfs-common_1.3.4-2.1_amd64.deb' nfs-common_1%3a1.3.4-2.1_amd64.deb 230730 SHA256:8fedb6f5d28d521ca6b1623610255c7edf26c011b3c45e07e19e4378902bfab4

------------------------------------------------------------------------------------------------------------------------
UTILS:DU (SIZE OF FILES - DISK USAGE)
du reports by default size of directories. Can be combined with -a to report also size of files (by default directories sizes is reported)
Also -d1 restricts max depth to 1. -b returns the real file size and -h returns size in human readable format. -s is for sumarize
Output of du is by default tab separated. If directory is ommited , current working directory is reported.
You can avoid -a and provide to du * or ./* to force report the files.
If you use -d1 you can force subdirs (of first level) with ./*/ (enabling bash options you can go deeper)

$ du -b -h -s /etc
5.0M	/etc

$ du -ab -h -d1 /etc
8.2K	/etc/ufw
42K	/etc/gimp
12K	/etc/sysstat
4.0K	/etc/foomatic
22K	/etc/perl
14K	/etc/dpkg
4.4K	/etc/ldap
40K	/etc/mailcap
6.8K	/etc/UPower
81K	/etc/dbus-1


$ find . -maxdepth 1 -printf %f-%s\\n  #Output in pure bytes - no human readable option available
bsd.txt-173
file4-3
shellcolors.sh-684

Tip: For binary/compiled files check out size utility

------------------------------------------------------------------------------------------------------------------------
UTILS:XARGS
xargs can be used to separate arguments. default action is echo .
also can be used to transform output of previous commands to command line options for another command that does not accept stdin - can not pipe

Options:
-l = max lines. It's use it is depreacated and posix /bsd usage -L1 must be used. 
-t = verbose . Print each command before execution
-p = prompt for user confirmation before running each command. Requires -t.
-r = no run if empty args. Usefull in double greps
-d'\n' = delimiter


Various usage examples:

tr '\n' '\0' < file | xargs -0 mkdir
xargs -d '\n' mkdir -p < file        #GNU Xargs with -d (delimiter) set to \n. Usefull if the file contains dirname with spaces.
grep -r 'someword' |xargs -r grep otherword   #chained greps - xargs -r filters out empty data (second grep does not called if empty result is passed by first grep)
find . -type f -name '*.txt' -print0 |xargs -0 somecommand_here


echo -e "foo bar\ndoo dar" | xargs -n 2 mv  --> -n = max number of args per command. This results to mv foo bar and mv doo dar - tested and working ok
echo "mbar bar\nmbaz baz" | xargs mv  --> This is wrong usage. It is translated by xargs like mv mbar bar mbaz baz
echo "mbar bar\nmbaz baz" | xargs -l -t mv  -->  Not tested by me but by other. Results to mv mbar bar and mv mbaz baz

using xargs to operate over files is not suggested.
For example in above xargs mv, we need to provide the files in a special way (i.e quoted) to handle files with special chars 
like space in filename, single/double quotes, etc
echo "mbar bar\n'my foo' foo" | xargs -l -t mv  --> works ok. Without quoting around file "my foo" the mv breaks

------------------------------------------------------------------------------------------------------------------------
UTILS:SEQ

start=3; seq -s" : " -f "data%g" $start 5    ---> data3 : data4 : data5
-s" : " - item separator

------------------------------------------------------------------------------------------------------------------------
UTILS:TEXT TO ASCII :http://artii.herokuapp.com/
$ curl http://artii.herokuapp.com/make?text=Welcome++++Back++++George+\!+\!;echo
you can assign it in a variable using $( ), you can store in a file with redirection >, etc

------------------------------------------------------------------------------------------------------------------------
UTILS:TIMEOUT
$ timeout command
Start COMMAND, and kill it if still running after DURATION.
For example : timeout 2 yes
timeout --foreground 12s tail -f access.log
