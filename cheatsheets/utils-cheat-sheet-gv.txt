UTILS:CHEAT SHEET by GV
https://www.gnu.org/software/coreutils/manual/html_node/index.html#Top
https://www.gnu.org/software/coreutils/manual/html_node/tac-invocation.html#tac-invocation
#-----------------------------------------------------------------------------------------------------------------------
UTILS:BASICS
FILE MANIPULATIONS
Depending on the job you can use different commands:

cut file: slices a line of file (or multi lines) based on a delimiter.
paste file1 file2: Prints field of both files side by side , ignoring matching criteria
diff file1 file2 : Prints differences of files 
sort file1 file2 fileN: Sorts the files. Can be adjusted to sort on a specific columnt (i.e 1.2 = file 1 column2)
join file1 file2 : Joins files in a common field

UTILS:CHEAT SHEET TXT 2 MANPAGE FORMATTING
##Read this cheat file with man pages:
http://technicalprose.blogspot.gr/2011/06/how-to-write-unix-man-page.html
man formatting: man 7 man & man 7 man-pages
groff programming: http://web.cecs.pdx.edu/~trent/gnu/groff/groff.html#IDX123
man pages making: https://liw.fi/manpages/
https://linux.die.net/man/1/help2man

##Working Command:
man --nj <(h=".TH man 1 2017 1.0 cheats page";sed "1i $h" cheatsheets/utils*gv.txt |sed 's/^UTILS:/.SH UTILS:/g; s/^$/\.LP/g; s/^##/\.SS /g; s/\\/\\e/g;G' |sed 's/^$/\.br/g')
You can also combine with --nh 
PS: man options --nj = not auto justified , --nh = not auto break words with hyphen on line changes.

##man and groff/troff require special handling.
man ignores normal line feeds at end of lines ($); empty lines (^$) are recognized and displayed
Line feeds in man pages can be done by inserting .br between two lines.
More .br in series of lines are ignored by man and got intepreted as a single line feed - not multiple new lines.
Man pages should start with a .TH line
Man sections / header start with .SH. 
Subsection start with .SS. Alternativelly you can use .B to make this line bold. .B follows text identation - .SS has it's own idents.
The backslash \ works as escape in groff, so you need to escape the backslash with \e (or \\ can also work)
The example tr -d '\n' will become tr -d '\en' with \e escaping, or will become tr -d '\\n' with \\ escaping.

#-----------------------------------------------------------------------------------------------------------------------
UTILS:GREP
##Exclude from one file entries that exist in another file: grep -Fxvf file2.txt file1.txt
-F for fixed string in order to avoid regex (with regex an entry "tar" in file2 will match tar,patar, guitar, etc in file1) 
-x = line grep to ensure matching whole lines 
-v = reverse the grep = display not matching lines
-f = load words/lines from file file2.txt
file1.txt = the file to be grepped 

##multi grep with reverse operation : grep -v -e "pattern" -e "pattern"
grep -nA1 -e "====" c.txt |grep -B1 -e "====" |grep -v -e ":" -e "--"

##GREP all files for two patterns (and operation)
http://stackoverflow.com/questions/41896604/linux-listing-files-that-contain-several-words#41897079
grep -ERl 'toto' | xargs -r grep 'tata'
Tip : xargs -r ensures that second grep will not run in null / non matched data.
Alternative : awk '/pattern1/&&/pattern2/'

##GREP a file for two patterns with AGREP by Chazelas:
http://unix.stackexchange.com/questions/55359/how-to-run-grep-with-multiple-and-patterns/55391#55391
agrep 'pattern1;pattern2' file (can also accept pattern3;patternN)
grep -X '.*pattern1.*&.*pattern2.*'
grep -P '^(?=.*pattern1)(?=.*pattern2)'
grep -e 'pattern1.*pattern2' -e 'pattern2.*pattern1'
Alternatives:
awk '/pattern1/ && /pattern2/'
awk '/regexp1/ && /regexp2/ && /regexp3/ { print; }'

##GREP COLORIZE 
grep --color 'pattern' or grep --color -E 'pattern|$'
Or in a function/alias : function highlight () { grep --color -E "$1|$" "${@:1}" }

##GREP MULTIPLE PATTERNS AND PRINT
Combine grep with cut and multipatterns: 
instead of grep |cut combination you can use awk
grep -E 'Label 3 \|Label 5' |cut -d' ' -f3 <==> awk '/Label 3/ { print $3 } /Label 5/ { print $3 }'
PS: AWK can directly print to a file using { print $3 >>"F3.csv" } 

##Grep an html link from a stream
stream=...\f4 HYPERLINK "httjps://archive.org/randomURL1?fref=grp_mmbr_list"}...
grep -Eo '\"https?:\/\/[^"]+\"' testsample.txt 

##mGrep all character before the first whitespace using grep?
grep -oP "^\S+" filename.txt

##Grep using a file with patterns.
Generally you can import patterns by a file using the -f switch.
Also you can feed the -f with process substitution to feed grep with manipulated patters grep -f <(cut -f2 patterns.txt) a.txt
In more tricky usage you can use the same file as file pattern in order to achieve complex grep results.
For example: 
grep -v -f <(grep "ok" a.txt |cut -d' ' -f1) a.txt
This will search the file a.txt for a second column having a text ok and will get the names marked as ok.
Then these names will be the main pattern to grep and exclude them from the whole,same file
Obviously this is usefull if on the master file a.txt not all the names have ok , but indirectly even if one ok is present for a user, you want to exclude this user from all the results.
#-----------------------------------------------------------------------------------------------------------------------

UTILS:CUT
https://www.gnu.org/software/coreutils/manual/html_node/cut-invocation.html#cut-invocation

cut can accept delimiter with -d'' and can return a range of fields (i.e -f1 , -f1,3 , -f1-3)
To assign a delimiter of \n,\t you have to use -d$'\n' (similar to IFS)

CUT can act directly on each line of file. You don't need while & read line loop.

Instead of -f option you can use cut to isolate part of text based on chars: 
cut -c9-10 file ==> gets characters from 9 to 10. similary you can get c1-5 to get only the 5 first chars of EACH file line.
Also see this:  cut -c1,9-10 a.txt ==> gets 1st char and then gets chars 9-10

 Insert dash in string
 String: #  1  2016-05-31-1003-57S._BKSR_003_CM6
 $ cut --output-delimiter='-' -c7-19,20-21 file.txt
 Output : 2016-05-31-10-03
 alternatives:
 $ awk '{print substr($3,0,13)"-"substr($3,14,2)}' file.txt
 $ while IFS= read -r line;do line="${line:6:13}-${line:14:2}";echo $line;done<file.txt
 $ sed 's/..$/-\0/g' <(cut -d- -f1-4 <(cut -d" " -f5- file.txt)) #use >newfile at the end to send the results to a new file

#-----------------------------------------------------------------------------------------------------------------------
UTILS:DIFF
## Comparing files and variables:
diff can compare two files line by line.
You can also trick use diff like this to compare two variables line by line : diff <(echo "$a") <(echo "$b") or diff <(cat <<<"$a") <(cat <<<"$b")

##Compare grep result in a nice visual way:
diff -u  <(cat a.txt) <(grep "tar*" a.txt) #or -y

------------------------------------------------------------------------------------------------------------------------
##Remove the < char from output and keep only the differet items:
diff --changed-group-format='%<' --unchanged-group-format='' c.txt cc.txt
i.e c.txt 
1
2
3
cc.txt
1
2
------------------------------------------------------------------------------------------------------------------------
##Using diff with process substitution 
diff -y <(man grep) <(man agrep) #compares man page of grep to manpage of agrep using -y = side by side
PS: normal usage of diff is diff -y file1 file2.

## DIFF Tricks:
If you are in doubt about the results of a command like an extra grep you can compare results of previous command with new command like this:
diff -w -y <(apt search manpages |grep "/" |cut -d"/" -f1 |grep -E '^[a-zA-Z0-9]') <(apt search manpages |grep "/" |cut -d"/" -f1)
Differences will be noted with > symbol and then you can manually verify that the results of the new command (extra grep) is as expected.
Usefull when you want to verify the performance in commands that produce large output.

------------------------------------------------------------------------------------------------------------------------
UTILS:HEAD-TAIL
Trick to get only one line from file using head and tail
Usage: bash viewline myfile 4
head -n $2 "$1" | tail -n 1

#-----------------------------------------------------------------------------------------------------------------------
UTILS:JOIN
join -a1 -o 1.2     - /dev/null # print the second field
join -a1 -o 1.2,1.1 - /dev/null # reorder the first two fields
See man join and also info join for better explanation


##Join two Files based on common fields:
http://unix.stackexchange.com/questions/342814/combine-two-csv-file-based-on-condition#342839
join -o 0,1.2,1.3,2.3 <(sort A.csv) <(sort B.csv)
-o: Obey Format - specify a particular format. 
-o 0 : Stands for the merged field and is different than 1.1 or 2.1 who stand for file 1 field 1 and file 2 field 2.
If file2 has a line that is not present on file 1, then using -o 1.1 will print an empty space in that field.


##More interesting options of Join: 
-a1 and/or -a2 : print non matched lines of file 1 and/or 2
-e EMPTY: replace missing lines EMPTY
-t char: Use char as delimiter
-1.2 : Join on field 2 of file 1
-2.4 : Join on field 4 of file 2
--nocheck_order : Don't complain about unsorted input files
--header : Treat 1st line as header

##Another join example: 
Join file b.txt and bb.txt. Preserve the contents of file b.txt and insert new contents from file bb.txt
If file bb.txt has similar parameters with different values, preserve the parameters of the original file b.txt - do not update

$ cat b.txt
parameterA=0
parameterB=1
parameterC=0

$ cat bb.txt
parameterA=0
parameterB=0
parameterC=0
parameterD=0

$ join -a1 -a2 -t= b.txt bb.txt |cut -d= -f1-2
parameterA=0
parameterB=1
parameterC=0
parameterD=0

##More Joins
File1
Category ID
C1  A1

File2
Purchase ID
O1  A1

Resulting File3
O1 A1 C1

join -j 2 -o 2.1 2.2 1.1 File1 File2
Join on field 2 of both files (alternatively you can use -1.2 -2.2)
-o controls the output 

#-----------------------------------------------------------------------------------------------------------------------
UTILS:PASTE : Merge lines of files
https://www.gnu.org/software/coreutils/manual/html_node/paste-invocation.html#paste-invocation
num2=file including 1,2 
let3=file including a,b,c
$ paste num2 let3
1       a
2       b
        c
$ paste -s num2 let3
1       2
a       b       c
$ paste num2 let3 num2
1       a      1
2       b      2
        c

Also see this example: http://unix.stackexchange.com/questions/340691/stacked-records-to-columns/340692?noredirect=1#comment602328_340692
$ paste -d"," <(grep -E '[0-9]' c2.txt) <(grep -E '[a-z]' c2.txt)
Mind the process substitution of the same file. Remember that <(...) creates a temp fd descriptor which acts as a file argument in apps requesting files, like paste.
#-----------------------------------------------------------------------------------------------------------------------
UTILS:COLUMN
Print anything nicely with column program
Example : 'mount |column -t'. # -t stands for table . This does Auto cols detection & separation according to whitespace.
If columns are separated by delimiter, define this delimited by using '-s' option.
For files you can directly run '$column -t file.txt'

#-----------------------------------------------------------------------------------------------------------------------
UTILS:BC Calculator
a="$(<a.txt)" #supposing file a has a number
b="$(<b.txt)" #similary
bc <<<"$a + $b"
c="$( bc <<<"$a + $b" )"

If the <<< syntax feels weird (it's called a "here-string" and is an extension to the POSIX shell syntax supported 
by bash and a few other shells), you may instead use printf to send the addition to bc:

printf '%s + %s\n' "$a" "$b" | bc
c="$( printf '%s + %s\n' "$a" "$b" | bc )"
#-----------------------------------------------------------------------------------------------------------------------
UTILS: CAT AND TAC
Source Code of CAT:
http://git.savannah.gnu.org/gitweb/?p=coreutils.git;a=blob;f=src/cat.c;h=001408576c4c17a156c1e8761ed2d9c96aa3d0cf;hb=refs/heads/master
CAT help and man page are quite good. 
TAC is the opposite of CAT; Prints lines of file upside down (start from last).
#-----------------------------------------------------------------------------------------------------------------------
UTILS:LN - Create links.
Combine -s to create symbolic links; otherwise will create hardlinks.
Hardlinks remain when the original file is removed. Symlinks fail.
#-----------------------------------------------------------------------------------------------------------------------
UTILS:NL - Number Lines / Line numbering
Can be used / pipe / file to add numbers in file lines (similar to cat -n but MUCH more advanced).
Has a lot of options like starting number, number step, number acc to regex, etc
Simpliest usage : cat file |nl - default usage is equivalent to cat -b = number non empty lines only.
nl -bp"^$" c.txt --> apply numbering only on empty lines (start and end together)

#-----------------------------------------------------------------------------------------------------------------------
UTILS:TR
Replace new lines with null: 
 tr '\n' '\0' <in >out 
tr '\\n' '\\0' <in >out 
.ft B tr '\n' '\0' <in >out 
#-----------------------------------------------------------------------------------------------------------------------
UTILS:SORT 
http://www.skorks.com/2010/05/sort-files-like-a-master-with-the-linux-sort-command-bash/
Mind the options -u and -k.

-k lets you defind the sort key. You can have many -k entries.
Moreover with -k you can define except from the column (-k1) also the letters range to make the sort (i.e -k1,5 = 1st Col, chars 1 to 5)

Example:
root@debi64:/home/gv/Desktop# sort -k1,5 a.txt b.txt
crwxrwxr-x+  - user1 hive          0 2017-01-31 16:55 /user/anc/xyz
drwxrwxr-x+  - user1 hive          0 2017-01-31 16:55 /user/abc
drwxrwxr-x+  - user1 hive          0 2017-01-31 16:55 /user/abc
drwxrwxr-x+  - user1 hive          0 2017-01-31 16:55 /user/anc/xyz
drwxrwxr-x+  - user1 hive          0 2017-01-31 16:55 /user/anc/xyzhrt/rth
drwxrwxr-x+  - user1 hive          0 2017-01-31 16:55 /user/anc/xyzhrt/rth
drwxrwxr-x+  - user1 hive          0 2017-01-31 16:55 /user/anc/xyz/rhr/rthrh
-rw-rwxr--+  3 user1 hive       1462 2017-01-31 16:55 /user/anc/xyz/rhrhheh

root@debi64:/home/gv/Desktop# sort -k1,5 -k8r a.txt b.txt
crwxrwxr-x+  - user1 hive          0 2017-01-31 16:55 /user/anc/xyz
drwxrwxr-x+  - user1 hive          0 2017-01-31 16:55 /user/anc/xyz/rhr/rthrh
drwxrwxr-x+  - user1 hive          0 2017-01-31 16:55 /user/anc/xyzhrt/rth
drwxrwxr-x+  - user1 hive          0 2017-01-31 16:55 /user/anc/xyzhrt/rth
drwxrwxr-x+  - user1 hive          0 2017-01-31 16:55 /user/anc/xyz
drwxrwxr-x+  - user1 hive          0 2017-01-31 16:55 /user/abc
drwxrwxr-x+  - user1 hive          0 2017-01-31 16:55 /user/abc
-rw-rwxr--+  3 user1 hive       1462 2017-01-31 16:55 /user/anc/xyz/rhrhheh


In the second example both files are sorted with key 1 to 5 and then are sorted with key 8 reversed.
Offcourse the results of second key are not capable to change the results of first key.
Is a nested /sub sort procedure to be used instead of pipe (sort -k1,5 |sort -k8)

root@debi64:/home/gv/Desktop# sort -k8 a.txt b.txt
drwxrwxr-x+  - user1 aive          0 2017-01-31 16:55 /user/abc
drwxrwxr-x+  - user1 hive          0 2017-01-31 16:55 /user/abc
root@debi64:/home/gv/Desktop# sort -k8 -k1r a.txt b.txt
drwxrwxr-x+  - user1 hive          0 2017-01-31 16:55 /user/abc
drwxrwxr-x+  - user1 aive          0 2017-01-31 16:55 /user/abc

If you need to extract duplicates based on specific columns ignoring data in the middle , uniq -d will not work.
# sort -k8 -k1r a.txt b.txt |uniq -d
drwxrwxr-x+  - user1 hive          0 2017-01-31 16:55 /user/anc/xyzhrt/rth

You can then call awk to print the columns you need ($1 and $8) and then cut -d will work on output of awk.

UTILS:SORT -u option (unique)
sort -u a.txt b.txt == cat a.txt b.txt |sort |uniq
sort -u will exclude the duplicate lines, operation similar to classic uniq but with more criteria when used with -k

root@debi64:/home/gv/Desktop# sort -u a.txt b.txt #default sorting = based on k1 - c letter on first field goes on top
crwxrwxr-x+  - user1 hive          0 2017-01-31 16:55 /user/anc/xyz
drwxrwxr-x+  - user1 aive          0 2017-01-31 16:55 /user/abc
drwxrwxr-x+  - user1 hive          0 2017-01-31 16:55 /user/abc

root@debi64:/home/gv/Desktop# sort -u -k8 -k1 a.txt b.txt
drwxrwxr-x+  - user1 aive          0 2017-01-31 16:55 /user/abc
drwxrwxr-x+  - user1 hive          0 2017-01-31 16:55 /user/abc
crwxrwxr-x+  - user1 hive          0 2017-01-31 16:55 /user/anc/xyz
First sorted with k8 then with k1. abc folders go on top

UTILS:DATE
Convert Date to Epoch. Date must be in format 14/Feb/2017:11:31:20
date -d "$(echo $dt | sed -e 's,/,-,g' -e 's,:, ,')" +"%s"
I have a smart function in bash profile for this that can work also as a pipe.
