BASH:CHEATS BY GV

BASH:BASICS
http://mywiki.wooledge.org/BashFAQ
http://mywiki.wooledge.org/BashSheet#Arrays
http://tiswww.case.edu/php/chet/bash/bashref.html#SEC31 - Search for "replace"
BASH CHEAT SHEET : https://github.com/pkrumins/bash-redirections-cheat-sheet/blob/master/bash-redirections-cheat-sheet.pdf
BASH HACKERS EXAMPLES / PARAMETER EXPANSION , ETC: http://wiki.bash-hackers.org/syntax/pe
ADVANCED BASH SCRIPTING : ftp://ftp.monash.edu.au/pub/linux/docs/LDP/abs/html/abs-guide.html#PIPEREF
IO REDIRECTION: http://tldp.org/LDP/abs/html/io-redirection.html
https://www.gnu.org/software/bash/manual/html_node/Shell-Parameter-Expansion.html
http://ss64.com/bash/expr.html
https://debian-administration.org/article/150/Easily_renaming_multiple_files
Also very good info available at local 'man bash' and particularly the section SHELL BUILTIN COMMANDS (find it with / in man page view)

Check man bash for :
mapfile [-d delim] [-n count] [-O origin] [-s count] [-t] [-u fd] [-C callback] [-c quantum] [array]
read [-ers] [-a aname] [-d delim] [-i text] [-n nchars] [-N nchars] [-p prompt] [-t timeout] [-u fd] [name ...]
printf
printf '%s: %s\n' "$user" "$shell"
------------------------------------------------------------------------------------------------------------------------
BASH:CHEAT SHEET TXT 2 MANPAGE FORMATTING
##Read this cheat file with man pages:
http://technicalprose.blogspot.gr/2011/06/how-to-write-unix-man-page.html
groff programming: http://web.cecs.pdx.edu/~trent/gnu/groff/groff.html#IDX123
man pages making: https://liw.fi/manpages/
https://linux.die.net/man/1/help2man
http://man7.org/linux/man-pages/man7/man-pages.7.html
man formatting: man 7 man & man 7 man-pages

##Working Command:
man --nj <(h=".TH man 1 2017 1.0 cheats page";sed "1i $h" cheatsheets/utils*gv.txt |sed 's/^UTILS:/.SH UTILS:/g; s/^$/\.LP/g; s/^##/\.SS /g; s/\\/\\e/g;G' |sed 's/^$/\.br/g')
You can also combine with --nh 
PS: man options --nj = not auto justified , --nh = not auto break words with hyphen on line changes.

Or in a function (i.e sticked at bash_aliases)
.B function mancheat { 
.B man --nj --nh <(h=".TH man 1 2017 1.0 $1-cheats";sed "s/^${1^^}:/.SH ${1^^}:/g; s/^$/\.LP/g; s/^##/\.SS /g;G" cheatsheets/${1,,}*gv.txt |sed 's/^$/\.br/g; s/\\/\\e/g;' |sed "1i $h");
.B }

##man and groff/troff require special handling.
man ignores normal line feeds at end of lines ($); empty lines (^$) are recognized and displayed
Line feeds in man pages can be done by inserting .br between two lines.
More .br in series of lines are ignored by man and got intepreted as a single line feed - not multiple new lines.
Man pages should start with a .TH line
Man sections / header start with .SH. 
Subsection start with .SS. Alternativelly you can use .B to make this line bold. .B follows text identation - .SS has it's own idents.
The backslash \ works as escape in groff, so you need to escape the backslash with \e (or \\ can also work)
The example tr -d '\n' will become tr -d '\en' with \e escaping, or will become tr -d '\\n' with \\ escaping.

To play with man formatting and see immediate results on screen try:
man <(echo -e ".TH man 1 2017 1.0 testing \n\n.SH Test\n\n\n\n.BR\n\.LP\n\.ce\nbefore end\n.BR \n\The end")

For sed explanation see the sed cheatsheet.

------------------------------------------------------------------------------------------------------------------------
BASH:LOCALE
http://unix.stackexchange.com/questions/87745/what-does-lc-all-c-do/87763#87763
For various operations you can temporary apply a C locale which speeds things up in case of grep, etc.
To see the results of various locales use this test:
$ LC_ALL=C sort <<< $'a\nb\nA\nB'
You can try with en_US, with UTF, with POSIX , etc
See the difference:
$ LC_ALL=C man
What manual page do you want?
$ LC_ALL=en_US sort <<< $'a\nb\nA\nB'
a
A
b
B
$ LC_ALL=C sort <<< $'a\nb\nA\nB'
A
B
a
b
------------------------------------------------------------------------------------------------------------------------
BASH:TERMINAL COLORS
https://github.com/aureliojargas/txt2regex/blob/master/txt2regex.sh
Also See the shellcolors.sh file

The main idea is to define some vars like:
#		cN=$(echo -ne "\033[m")      # normal - also 30m works ok.
#		cP=$(echo -ne "\033[31m") #red color
you can then print in color like echo "$cP This is a red text $cN"
or even trickier you can define your vars like $red and $normal and work it like this:
red=$(echo -ne "\033[31m"); normal=$(echo -ne "\033[m");echo $red hi there $normal --> works ok , prints hi there in red color and returns to normal. If you ommit the normal , terminal remains in red!
Or even better do them functions ! function red { echo -ne "\033[32m";echo $@;echo -ne "\033[m";};red hi there

------------------------------------------------------------------------------------------------------------------------
BASH:READ - BASICS
##Reading Files (mapfile, readarray, read -r , etc)
http://unix.stackexchange.com/questions/339992/how-to-read-different-lines-of-a-file-to-different-variables/339996#339996
http://wiki.bash-hackers.org/commands/builtin/read
http://wiki.bash-hackers.org/commands/builtin/mapfile
http://unix.stackexchange.com/questions/209123/understand-ifs-read-r-line

Check man bash for :
readarray /  mapfile [-d delim] [-n count] [-O origin] [-s count] [-t] [-u fd] [-C callback] [-c quantum] [array] #man bash line 2981
read [-ers] [-a aname] [-d delim] [-i text] [-n nchars] [-N nchars] [-p prompt] [-t timeout] [-u fd] [name ...] #man bash line 3056

------------------------------------------------------------------------------------------------------------------------
##Read file to array
http://stackoverflow.com/questions/11393817/bash-read-lines-in-file-into-an-array/11395181#11395181
mapfile -t -O1 var <input.txt --> each line of file goes into array var, withou loop. You just need to refer to line1 as var[1]
mapfile -t -O1 var < <(sed/grep/awk/cat/etc 'expression' file) to feed mapfile with the results of a command
PS1: mapfile has also an alias called readarray
PS2: readarray can also be used like <<<"$var"

This also works: IFS=$'\n' read -d '' -r -a lines < file

------------------------------------------------------------------------------------------------------------------------
##Read and separate a file based on NULL character only and not on new lines
http://stackoverflow.com/questions/8677546/bash-for-in-looping-on-null-delimited-string-variable
http://stackoverflow.com/questions/42381149/bash-function-with-an-array-input-and-output

NULL Separated file : /proc/1/environ to variables script
You can view this file with cat /proc/1/environ |tr '\0' '\n' #all null will become new lines

You can convert any file to null separated (for testing) by cat file |tr '\n' '\0'

To split the files based on null you need something like this:
$ while IFS= read -r -d '' line ; do vars+=("$line");done <file #Mind the gap in -d option of read.
declare -p vars

Also , with readarray - without loop (bash 4.4 and above): 
IFS= readarray -t -d '' vars2 <file3;declare -p vars2
PS: I tried to apply as a delimiter -d$'\0' or -d '\x00' but not working. Has to be -d '' - maybe -d $'\0' could also work (spaced)

This also works for files : readarray -t -d '' var2 < <(find . -print0)

This does not works without loop- it gets only the first element: 
IFS= read -d '' -r -a lines < <(find . -name '*.sh' -print0) #I also tried with IFS=$'\0', IFS='', IFS=$'' , no IFS , but no luck

Need to be combined with a loop to work:
while IFS= read -r -d '' file;do
echo "file found=$file" #this can be whatever , i.e an array assignment filearray+=("$file")
done < <(find . -print0)

For files this also works ok:
find . -print0 |while IFS= read -r -d '' file;do echo "file found=$file";done


.B PRINTING A NULL SEPARATED VARIABLE
For a variable, according to http://stackoverflow.com/questions/8677546/bash-for-in-looping-on-null-delimited-string-variable
while IFS= read -r -d '' line ; do
    echo "line=$line"
done <<<"$var"
In realiry this method works for null separated files (done<file) but never worked with a VAR
Test it with var=$(find . -print0). It did not work even with $(printf '%s\0' "$var")

The only way for this to work with a variable is to transform the variable to a format that will preserve the null (base64) , since bash removes the null byte from vars.

You need to do something like var=$(find . -print0 |base64) and on the "done" part of the while you need to apply 
done < <(echo "$var" |base64 -d) - See the ARGS section for details

------------------------------------------------------------------------------------------------------------------------
## Read while read method:
while read -r var1 var2 ;do ....;done<file.txt :will assign var1 to first field of line 1 and var2 to second field.
If the file has more than two fields, then var1 will get the first text and var2 will get all the rest text (all of them)

Withoud defining IFS, the default IFS is used. Even unsetting the IFS default values will be used.
This is why sometimes is preferable to write IFS= (equals to IFS=null) instead of unset IFS
 
You can define IFS=' ' for space delimiter, or ':' for semicol delimiter or IFS=$'\n' for new line, etc
To read whole lines use just IFS= (empty = whole line is returned) - But in this case the mapfile tool is better.

Skip Lines Trick inside loop : [[ $line = \#* ]] && continue #skips lines starting with # = comments

------------------------------------------------------------------------------------------------------------------------
## Read lines and split to space separated fields
you can combine also multi read (line read / field read) like this:
while IFS= read -r line;do IFS=' ' read -r -a v1 <<<"$line";done<c.txt
OR 
while IFS= read -r line;do readarray -d' ' -t v1 <<<"$line";done<c.txt

It will assign filelines to line and then with different IFS will split line to space separated fields.

Can be also done with ONE loop like this:
while read -r -a vars;do #whatever;done<file #you might also use IFS=' ' , but even with default IFS works fine.
For a line of "arg1_1 arg2_1 arg3_1" , vars array will look like this:
declare -a vars=([0]="arg1_1" [1]="arg2_1" [2]="arg3_1")

Mind that vars array will be redifined/overwritten in the next line of the file. 

To have an array that fiels from all the lines will be appending serially to this array instead of overwritting at every new line:
while IFS= read -r lines;do readarray -t -d' ' -O"${#vars[@]}" vars <<<"$lines";done<c.txt;declare -p vars
Or you can use the read -r -a method and transfer fields to another array that will not be overwritten by the next line.

------------------------------------------------------------------------------------------------------------------------
## Read a single text/var with read -r:
http://mywiki.wooledge.org/BashFAQ/001
read -r first last junk <<< 'Bob Smith 123 Main Street Elk Grove Iowa 123-555-6789'
read -r first last junk <<< "$a" 
# first will contain "Bob", last will contain "Smith", junk holds everything else that follows
------------------------------------------------------------------------------------------------------------------------
## Read lines from file and split to fields separated by double quotes 
Line Example : "aaa bbb" "xxx yyy"  # Next line can be of the same pattern plus a new line separator at end of prev line
Target : separate fields of line like $1="aaa bbb" $2="xxx yyy" in order to be sent to a function or command

This is tricky because delimiter is not new line but neither space.
Using newline as delim will get the whole line : $1="\"aaa bbb\" \"xxx yyy\""
Using space as delim will get $1="\"aaa" $2="bbb\"" etc

This guy here builds an array to take advantage of the existed double quotes:
http://stackoverflow.com/questions/42111441/how-to-pass-command-line-arguments-with-spaces-through-a-variable-in-bash#42111441
while IFS= read -r line;do
  eval args=\("$line"\) 			#or eval args+=\("$line"\)  or declare -a args+=\(....\)
  command_or_function "${args[@]}" 	#or outside the loop if args+=
done<file.txt

By calling function inside while loop it results that function will be called as many times as file lines
Every time function is called will have args starting from [0] up to number of fields i.e [1]
In reality the command eval args=\("$line"\) (or declare -a args=), expands to (as seen using set -x)
++ args=("aaa bbb" "xxx yyy")
Which is two different array entries! Genious. PS: Brackets are quoted to avoid being handled as sub shell by bash
On the next loop the array is redifined (due to absence of +=) and array elements get the fields of the next line.
Using += fields of next line will be added to the existed array

Examples:
$ cat e.txt
"aaa bbb" "xxx yyy"
"some more" "data here"

$ unset args;while IFS= read -r line;do declare -a args=\("$line"\);declare -p args;done<e.txt
declare -a args=([0]="aaa bbb" [1]="xxx yyy")
declare -a args=([0]="some more" [1]="data here")

$ unset args;while IFS= read -r line;do declare -a args+=\("$line"\);done<e.txt;declare -p args
declare -a args=([0]="aaa bbb" [1]="xxx yyy" [2]="some more" [3]="data here")

$ function test { for a in "$@";do echo "$a";done;};test "${args[@]}"
#or function test { while [[ "$#" -gt 0 ]];do echo "$1";shift;done;};test "${args[@]}"

aaa bbb
xxx yyy
some more
data here

More Tips:
To call an external command (not a function) it seems that `xargs command <file` is enough (??strange!!)

In case you want to parse whole lines from file as an argument to a function you can simply use : /some/command "$(<file.txt)"
------------------------------------------------------------------------------------------------------------------------
##Read file in pair of two lines at same time
while read -r prereq && read -r target; do
   printf '%s: %s\n' "$target" "$prereq"
done < file.txt
PS: Mind that two lines are read as a pair. Line 1 and Line 2 , and then Line 3-Line4.
You miss the Line 2- Line3 combination

Can be also used for filenames:
while read -r f1 && read -r f2;do echo "f1=$f1, f2=$f2";done < <(find . -name 'file*')
f1=./file1, f2=./file2
f1=./file3, f2=./file4
------------------------------------------------------------------------------------------------------------------------
##Read Only the first two lines.
{ IFS= read -r line1 && IFS= read -r line2; } < input.txt
{ line1=$(line) && line2=$(line); } < input.txt
------------------------------------------------------------------------------------------------------------------------
Read Using line builtin
line <file will print the first line of file
can also be assigned to a var: a=$(line <file)
------------------------------------------------------------------------------------------------------------------------
##Read loop with file descriptor
exec 9< "$file"
while IFS= read -r line <&9; do
  cat > ignoredfile
  printf '%s\n' "$line"
done
exec 9<&-
------------------------------------------------------------------------------------------------------------------------
BASH:PIPES
Pipes are usefull to send data from one command to the other (i.e cat a.txt |less)
What needs to be said is that pipes create a subshell.
Thus multiple pipes create multiple subshells = performance penalty.
More over variables of parent can not be modified by childs (=subshells)

Example:
s=1000; ps -ly |while read a b c d rest;do s=$(($s+c));echo "c=$c - S=$s";done;echo "final s=$s"
s values will be sent to pipe (child), and value of c will be added. Outside the pipe, final s will be reported 1000, because child can not change parents.
c=PID - S=1000
c=1578 - S=2578
c=1614 - S=4192
c=1998 - S=6190
c=1999 - S=8189
final s=1000

Workaround to avoid pipe / subshell:
s=1000; while read a b c d rest;do s=$(($s+c));echo "c=$c - S=$s";done < <(ps -ly);echo "final s=$s"
In this case (process substitution) the final s will have the correct/final value.
------------------------------------------------------------------------------------------------------------------------
BASH:PIPES Exit Code Checking (http://mywiki.wooledge.org/BashFAQ/002)
Check Exit code of commands inside pipe: ${PIPESTATUS[@]}
"${PIPESTATUS[0]}" refers to first (read from left) command
"${PIPESTATUS[1]}" refers to second (read from left) command, etc
"${PIPESTATUS[@]}" prints them all 

Bash 3.0 added a pipefail option as well, which can be used if you simply want to take action upon failure of the grep: 
set -o pipefail
if ! grep foo somelogfile | head -5; then
    printf "uh oh\n"
fi
------------------------------------------------------------------------------------------------------------------------
BASH:RUN COMMAND AS DIFFERENT USER : 
gksu -u gv command. Usefull if you are in root terminal and want to execute i.e google-chrome-stable
Run Google Chrome as Root : http://unix.stackexchange.com/questions/175967/how-to-run-google-chrome-as-root-in-linux/332128#332128
Easy trick : gksu -u gv google-chrome-stable - works fine either by root terminal or by root login
------------------------------------------------------------------------------------------------------------------------
BASH:CONDITION CHECKS 
See also man bash and man test
One line if check : This is based to the operation of && which executes the next command only if previous command exit with 0 = succesfull exit = pseudocode as TRUE (if it maybe the only time that something with value zero is translated to true!)
For else conditions or for performing actions under false conditions you can use ! operator (not) in front of expression which will reverse exit code.
[ "$USER" = "root" ] && echo "hello root" -> hello root # displays nothing if user is not root
[ "$USER" = "root" ]  || [ "$LOGNAME" = "root" ] && echo "hello root" --> hello root #
[ "$USER" = "root" ]  || [ "$LOGNAME" = "root" ];echo $? -> 0 #zero = all ok = true
[ "$USER" = "root" ]  || [ "$LOGNAME" = "rot" ];echo $? -> 0 #zero = all ok = true due to the OR operator || (for and you should use &&)
[ "$USER" = "rot" ]  || [ "$LOGNAME" = "rot" ];echo $? --> 1 #one = not ok = false since i'm logged in as root and not rot
! [ "$USER" = "rot" ]  || ! [ "$LOGNAME" = "rot" ];echo $? --> 0  #expression ok = true means that it is true that i'm not user rot or logname rot (true since i'm logged in as root)
Tip: When you gksu terminal from normal user account then $USER and $LOGNAME are set to root.
[[ -z $1 ]] && echo "Pass me a directory to cat files" && return #if is not set , if it is blank, empty
[[ ! -z $1 ]]  equals to [[ -n $1 ]] # -n operator makes the opposite job of -z. Returns true / success / exit code 0 if it is set 
[[ ! -d $1 ]] && echo "Argument passed is not a directory - please send a dir" && return

[[ ! $diskstatus =~ ^(ready|online)$ ]] && echo "Not OK: $diskstatus" # Regex match of two values.

$ echo "/home/gv/Desktop" |sed 's![^/]$!&/!' --> /home/gv/Desktop/ #Check if a slash '/' is present in the end- add it if missing
$ d="/home/gv/Desktop"; [[ "${d: -1}" != "/" ]] && d="${d}/" #bash alternative: if last char is not a dash, add a dash

##CONDITION CHECK on command output , based on the return code of command without the need to compare $?.
if command; then
    printf "it succeeded\n" #executed when command returns 0
else
    printf "it failed\n" #executed when command returns <>0 (i.e 1 or other code - carefull with diff)
fi

.I Real Example:
if grep -F "A=" b.txt ;then echo "found";else echo "Not Found";fi
parameterA=0 #Output of grep
found #Output of if

To suppress the grep output use grep -q or use >/dev/null

Mind that IS NOT necessary to enclose the grep in $(...) - it works directly.
Although this will also work if [[  $(grep "pattern" file) ]];then dothis;else dotheother;fi

##CONDITION CHECK -  CASE WITH NUMBERS RANGE
Using case with numbers:

This case makes human logic but not computer logic, since case compares $num to regex pattern
case $num in
  [0-6] )               echo "You're close...but too low" ;;
  [8-14] )              echo "You're close...but too high" ;;
  [15-100] )            echo "You're nowhere near my favorite number...sorry, try again" ;;
  7 )                   echo "YOU GUESSED MY FAVORITE NUMBER!" ;;
  * )                   echo "You didn't pick a number between 1 and 100!" ;;
esac

Can be done with classic if-elif-else combined with -gt , -lt, -eq, etc oprators.
Alternativelly we can use bash with regex that will match the desired combinations like this:

case $num in
  ([0-6])                 echo "You're close...but too low" ;;
  ([8-9]|1[0-4])          echo "You're close...but too high" ;;
  (1[5-9]|[2-9][0-9]|100) echo "You're nowhere near my favorite number...sorry, try again" ;;
  7 )                     echo "YOU GUESSED MY FAVORITE NUMBER!" ;;
  * )                     echo "You didn't pick a number between 1 and 100!" ;;
esac

Pattern []0-6] is a valid regex pattern representing chars from 0 up to 6.
Pattern [8-9]|1[0-4] will epxand to chars 8-9 OR 10-11-12-13-14 (as chars)
Pattern (1[5-9]|[2-9][0-9]|100) will expand to 15-19 OR 20-99 OR 100 = range 15-100
------------------------------------------------------------------------------------------------------------------------
BASH:FIND 
POSIX Find Manual :http://pubs.opengroup.org/onlinepubs/9699919799/utilities/find.html
##Use find as ls alternative
find /home/gv -maxdepth 1 -type d -> list only directories
find /home/gv -maxdepth 1 -type f -> lists only files
find /home/gv -maxdepth 1 -> lists both
output of find can be piped to wc -l , xargs, and other commands.

Some progs (like whatis) can not accept pipes directly ; In this case you have to use xargs (pipe to xargs which will call the prog)
PS: By the way, you don't need find to call whatis. You can call whatis directly with wildmark : whatis /usr/bin/*

##Read File Names as a pair
while read -r f1 && read -r f2;do echo "f1=$f1, f2=$f2";done < <(find . -name 'file*')
f1=./file1, f2=./file2
f1=./file3, f2=./file4

##UNUSUAL FILE NAMES 
Consider files with spaces in their name (i.e a a (01).txx)

In order you want to get the basename of this file the following command fails:
for file in "$(find . -name "*.txx")";do basename "$file";done
But if you just type find . -name "*.txx" files will be listed correctly.
Also this command works ok: for file in "$(find . -name "*.txx")";do echo "$file";done
Mind the double quotes outside $(find...)

To correctly handle file names with spaces you need to combine find with  while read.
This works ok: find . -name "*.txx" |while read -r line;do basename "$line";done

Also this works ok , and it is more simple to use: for file in *.txx;do basename "$file";done

The problem here is that this method doesnt go inside subdirs, while the find method does.
http://stackoverflow.com/questions/4638874/how-to-loop-through-a-directory-recursively-to-find-files-with-certain-extension
http://www.commandlinefu.com/commands/view/14209/repeat-any-string-or-char-n-times-without-spaces-between
http://wiki.bash-hackers.org/syntax/expansion/brace
http://stackoverflow.com/questions/2372719/using-sed-to-mass-rename-files
linux   /boot/vmlinuz-4.0.0-1-amd64 root=UUID=5e285652 ro  quiet text

##FIND OPERATORS (AND , OR,NOT)
exclude files (i.e manifest.txt
find /path/to/ -name '*.txt' -and -not -name 'manifest.txt'

Usage: Multiple file names / types:
find /tmp -name '*.pdf' -or -name '*.doc' | xargs rm #mind the -or operator. 

##FIND CUSTOM PRINTING
find /dir1 -type f -printf "%f\n" #prints only file name, without directory in front.
find . -printf "depth="%d/"sym perm="%M/"perm="%m/"size="%s/"user="%u/"group="%g/"name="%p/"type="%Y\\n

##FIND -EXEC
find can accept an -exec option in two formats:
-exec command {} \; #In this method command is invocated in each result of find
-exec command {} +  #In this method command is invocated after buffering an amount of find results.
find dirname ... -exec somecommand {} \; -exec someothercommand {} \; #by wildcard

.B Example with grep
[: http://unix.stackexchange.com/questions/85789/grep-in-couple-thousands-files :]
find . -type f -exec grep PATTERN {} +   # packs as many files as it can per command invocation, 
find . -type f -exec grep PATTERN {} \;  #rungs grep in every find result = much slower

find / -type f -print0 | xargs -r0 -P2 grep -Hi 'the brown dog'

.B Example: Rename extensionless files
Mind the handling of the find results with {} - becomes $0 if you call bash
find . -type f  ! -name "*.*" -exec mv -v {} {}.txt \;
find . -type f ! -name "*.*" -exec bash -c 'mv "$0" "$0".mp4' {} \;

##Loop in files with find using null as file names separator (the correct way to loop)
By Stephane Chazelas : http://unix.stackexchange.com/questions/321697/why-is-looping-over-finds-output-bad-practice

As a general idea, to loop on files the glob method is better.
for f in dir/* ; do something with $f;done

If you need to do it with find, the only accurate approach is to use find -print0 that will separate file names with \0 = null byte
This will ensure that results will be correct even if filenames contain spaces, strange chars , newlines, whatever because on unix file names are allowed to contain everything except / (assigned to paths) and except null chars.

.B Method 1 - To perform actions on each file - null separated
while IFS= read -r -d '' fn;do var+=("$fn");done < <(find . -name '*.sh' -print0)

.B Method 2 - Quick store of filenames in array without loop
readarray -t -d '' var2 < <(find . -name '*.sh' -print0)
Unfortunatelly the read -r -d '' -a array < <(find . -print0) does not work - does not loop on the results.

.B Pipe find results to other commands:
find . -print0 | xargs -r0 something with
find . -print0 |while IFS= read -r -d '' file;do echo "file found=$file";done #instead of echo you could built an array or whatever

.B The globbing method to create an array with files
shopt -s nullglob dotglob #to have files with names beginning with a dot and avoid glob expanding to * if no files match
files=(*)  ## or fies=(*.sh), etc

------------------------------------------------------------------------------------------------------------------------
BASH:PROCESSES ()TOP - HTOP - PS - KILL)
Search for a process using top . Top seems to catch all processes:
top -p $(echo $(pgrep time) |sed 's/ /,/g')
pgrep search for processes matching pattern even partially. pidof could be used if exact process name is known.
Defaut output of pgrep is to seperate processes found with new lines. By echo \n is removed and a space is used.
If you replaace that space with a comma, then can be fed to top -p which accepts multiple pids (comma seperated)

Processes List and Kill
ps all and ps aux
list all of tty1 : ps -t tty1
Isolate pids: ps -t tty1 |cut -d" " -f1
Remove new line chars: ps -t tty1 |echo $(cut -d" " -f1)
Kill all those processes at once: kill -9 $(ps -t tty1 |echo $(cut -d" " -f1)) # kill requires pids to be seperated by spaces, not new lines.
Best Solution : kill -9 $(echo $(ps -t tty1 --no-headers -o pid))
------------------------------------------------------------------------------------------------------------------------
BASH:PARAMETERS EXPANSION
http://wiki.bash-hackers.org/syntax/pe#use_a_default_value

a="this is some TEXT"; echo ${a: -10} 	-> some TEXT
a="this is some TEXT"; echo ${a: 10} 		-> me TEXT
a="this is some TEXT"; echo ${a: 5:7} 	-> is some
a="this is some TEXT"; echo ${a: 1:-1}    -> his is some TEX  #remove first and last char - OR - get from char 1 up to lst char-1

a="/home/gv/Desktop/PythonTests/a<>rte.zip";echo $(basename ${a/<>/_}) 	->a_rte.zip
a="/home/gv/Desktop/PythonTests/a<>rte.zip";echo ${a#/} 					-> removes only the first / 
a="/home/gv/Desktop/PythonTests/azip<>rte.zip";echo ${a%.zip} 			->removes the last .zip (but not middle zip) 
basename c.jpg .jpg -> c
basename c.jpg pg -> c.j #basename can be used as a tricky tool to remove chars from the end of ANY string but requires exact match
a="logfiletxt";basename $a txt -> logfile
a="logfile.txt";echo ${a/.txt} -> logfile #similar to basename but match starts from 1st char to the last. 1st occurence to be removed.
a="logfile.txt";echo ${a/fil} -> loge.txt #remove fil - exact match
a="logfilefilo.txt";echo ${a/lo} -> gfilefilo.txt #only first occurence of exact pattern removed
a="logfilelofi.txt";echo ${a/lo} -> gfilelofi.txt #only first occurence of exact pattern removed
a="logfilelofi.txt";echo ${a//lo} -> gfilefi.txt #all occurences of exact pattern removed.
a="logfilelofi.txt";echo ${a/lf/_} -> logfilelofi.txt #no replacement made since lf is not present in $a (exact match)
a="logfilelofi.txt";echo ${a//[lf]/_} -> _og_i_e_o_i.txt #all occurences of l and f - not exact match due to [] regex synthax
a="logfilelofi.txt";echo ${a/*l/_} -> _ofi.txt #from start up and including last l
a="logfilelofi.txt";echo ${a/*g/_} -> _filelofi.txt #from start up & including last g
a="logfilelofi.txt";echo ${a/*l/} -> ofi.txt #from start up to last l (if no replace string is specified then delete)
a="logfilelofi.txt";echo ${a/#/_} -> _logfilelofi.txt #replace first char with underscore
a="logfilelofi.txt";echo ${a/%/_} -> logfilelofi.txt_ #replace last char with underscore
a="logfilelofi.txt";echo ${a/.txt/_} -> logfilelofi_ #replace .txt with underscore
a="logfilelofi.txt";echo ${a/.txt} -> logfilelofi #delete .txt
MYSTRING=xxxxxxxxxx;echo ${MYSTRING/#x/y}  # RESULT: yxxxxxxxxx # Here the sign # is like an anchor to beginning
MYSTRING=xxxxxxxxxx;echo ${MYSTRING/%x/y}  # RESULT: xxxxxxxxxy # Here symbol % is an anchor to the end of string
a="logfilelofi.mp3";echo ${a/.[a-z0-9A-Z]*/} -> logfilelofi #delete any extension with a dot and any of a-z,0-9 and A-Z range
CLIP=$'http://abc\".x\'y`.com';cleanclip=$(echo ${CLIP//[\'\`\"]});echo $cleanclip ->http://abc.xy.com #mind the special var declaration of CLIP.
for i in *.JPG; do mv "$i" "${i/.JPG}".jpg; done -> finds files with JPG extension and renames them to .jpg
a="/home/gv/Desktop/PythonTests/a?<>rt*eew?.zip";echo $(basename ${a//[\/<>:\\|*\'\"?]/_}) 	-> _home_gv_Desktop_PythonTests_a___rt_eew_.zip
bash manual: ${parameter/pattern/string} . If pattern begins with ‘/’, all matches of pattern are replaced with string. Normally only the first match is replaced. If pattern begins with ‘#’, it must match at the beginning of the expanded value of parameter. If pattern begins with ‘%’, it must match at the end of the expanded value of parameter.

a="somefile.txt";echo ${a%%.txt} -> somefile #delete from end exact match
a="somefile.txt";echo ${a%.txt} -->somefile
a="sometxtfile.txt";echo ${a%txt} -->sometxtfile. #delete from end only exact match. midle txt is not deleted.
a="sometxtfile.txt";echo ${a##txt} --> sometxtfile.txt #no valid -no effect 
a="sometxtfile.txt";echo ${a##some} --> txtfile.txt #delete pattern (xact match) from the beginning
a="sometxtfile.txt";echo ${a#some} --> txtfile.txt
a="sometxtfile.txt";echo ${a#txt} --> sometxtfile.txt #no effect . there is no "txt" in the beginning.
a="Be conservative in what you send";echo ${a#* } --> conservative in what you send ("Be" is deleted. Single # removes the first word from beginning)
a="Be conservative in what you send";echo ${a##* } --> send #All text deleted except "send" Double ## removes all words from beginning except last
a="this.is.a.file.gz";echo ${a##*.} -->gz #all text deleted except last part (DOT separated) or delete from begining until the last dot found
a="apt";echo ${a:0:1} --> a #prints the first character of a variable (from zero give me 1)
a="Be conservative in what you send";echo ${a% *} --> Be conservative in what you #first word from end deleted. 
a="Be conservative in what you.send";echo ${a% *} --> Be conservative in what #works only for space separated words (IFS makes some effect in the resulted text)
a="Be conservative in what you send";echo ${a%% *} --> Be #all words from the end deleted (space separated)

a="some text here";echo ${a@Q} ->'some text here' #printing with single quotes
a="some text here";echo ${a@A} -> a='some text here' #operators available Q-E-P-A-a
a[0]="some text";a[1]="more text";echo ${a[@]} -> some text more text
a[0]="some text";a[1]="more text";echo ${a[@]@A} ->declare -a a=([0]="some text" [1]="more text")
a[0]="some text";a[1]="more text";echo ${a[@]@Q} ->'some text' 'more text'
a[0]="some text";a[1]="more text";a[2]="much more text";echo ${!a[@]} -> 0 1 2 #index of elements . This can be used in for i in ${a![@]} - i will be 0 , 1, 2 
a[0]="some text";a[1]="more text";a[2]="much more text";echo ${#a[@]} -> 3 #Total number of elements
a[0]="some text";a[1]="more text";a[2]="much more text";echo ${a[-1]} -> much more text. Use of -1 in index prints the last array element.
a="This is some Text";echo "${a^^}" --> THIS IS SOME TEXT #All chars converted to uppercase
array=(This is some Text);echo "${array[@]^^}" --> THIS IS SOME TEXT #All chars converted to uppercase
array=(This is some Text);echo "${array[@],}" --> this is some text
array=(This is some Text);echo "${array[@],,}" --> this is some text #all chars in lower case
array=(This is some Text);echo "${array[@]^}" --> This Is Some Text
array=(This is a text);echo "${array[@]%is}" --> Th a text ("is" is deleted from all elements of array : array=([0]="This" [1]="is" [2]="a" [3]="text"))
http://wiki.bash-hackers.org/syntax/pe : "As for most parameter expansion features, working on arrays will handle each expanded element, for individual expansion and also for mass expansion."
array=(This is a text);echo "${array[@]/t/d}" ⇒ This is a dext #first found t replaced with d. Capital T is intact.
array=(This is a text);echo "${array[@]//t/d}" ⇒ This is a dexd #all t replaced with d
array=(This is a text);echo "${array[@]/[tT]/d}" -> dhis is a dext #First found small and first found capital T replaced using regex

a="logfilelofi.mp3";av="anotherfile";echo ${!a@} -> a av #lists all active/stored parameters starting with letter a
echo ${!BASH*} -> BASH BASH_ARGC BASH_ARGV BASH_COMMAND BASH_LINENO BASH_SOURCE BASH_SUBSHELL BASH_VERSINFO BASH_VERSION
#mv path/you/do/not/want/to/type/twice/oldname !#$:h/newname #!$ returns the argument of last command /history
#Similarry to !$ there is alsos !! which prints last commad (full) and last result
path/you/do/not/want/to/type/twice/oldname !#$:h/newname -> path/you/do/not/want/to/type/twice/oldname path/you/do/not/want/to/type/twice/newname

expr 40 - 3 ->37 #expr is available in GNU Bash. 
expr substr "the is a kind of test" 5 10 -> is a kind  
a="the is a kind of test";echo ${a: 5:10} -> s a kind o
export -p -> gives infor about global vars : declare -x USER="root" , declare -x XDG_CURRENT_DESKTOP="XFCE"
IFS=:;a[0]="some text";a[1]="more text";echo "${a[*]}" -> some text:more text #the use of * instead of @ seperates array elements by IFS 

Print / Refer to array elements in a different way using parameters expansion / string manipulation
array=(0 1 2 3 4 5 6 7 8 9 0 a b c d e f g h)
echo ${array[@]:7} -> 7 8 9 0 a b c d e f g h
echo ${array[@]:7:2} -> 7 8
echo ${array[@]: -7:2} -> b c
echo ${array[@]: -7:-2} ->bash: -2: substring expression < 0
echo ${array[@]:0} -> 0 1 2 3 4 5 6 7 8 9 0 a b c d e f g h  #equivalent to echo ${array[@]}
echo ${array[@]:0:2} -> 0 1 #extract part of array / sub-array
echo ${array[@]:2:1} -> 2   #Start from position 0 and print 1 . eqivalent to echo ${array[2]} 
MYARR=(a b c d e f g);echo ${MYARR[@]:2:3}  -->c d e            # Extract a sub-array
MYARR=(a b c d e f g);echo ${MYARR[@]/d/FOO} --> a b c FOO e f g  # Replace elements that match pattern (d) with word FOO)

##Printing arrays with BASH (declare -p)
MYARR=(a b c d e f g);declare -p MYARR  #Print array in the smart way ;-) Works even with associative arrays.
#Output --> declare -a MYARR=([0]="a" [1]="b" [2]="c" [3]="d" [4]="e" [5]="f" [6]="g")

declare -p array |sed 's/declare -a array=(//g' |tr ' ' '\n' |sed 's/)$//g'
if you just declare -p array then output is like this:
a=( 1 2 3);declare -p a --> declare -a a=([0]="1" [1]="2" [2]="3")
So the first sed gets rid of the 'declare -a a=(' part.
tr replaces spaces (between array elements) with new line
last sed deletes the last ) in the array
result : 
root@debi64:/home/gv/Desktop/PythonTests# a=( 1 2 3);declare -p a |sed 's/declare -a a=(//g' |tr ' ' '\n' |sed 's/)$//g'
[0]="1"
[1]="2"
[2]="3"
You can then further select id of an array directly (see manon script)
You can also have a function for this : function pa { declare -p $1 |sed s/"declare -a $1=("//g |tr ' ' '\n' |sed 's/)$//g';};pa a
or even assign it to an alias:
alias printarray='function _pa (){ if [ -z $1 ];then echo "please provide a var";else declare -p $1 |sed "s/declare -a $1=(//g; s/)$//g; s/\" \[/\n\[/g";fi; };_pa'
for some reason tr ' ' '\n' raises errors in alias.... We switched to last sed replacing  [ with \n[ 

------------------------------------------------------------------------------------------------------------------------
BASH:ARGS - POSITIONAL PARAMETERS 
http://wiki.bash-hackers.org/scripting/posparams#range_of_positional_parameters

##Iterate through args
function test { for a in "$@";do echo "$a";done;};test "${args[@]}"
function test { while [[ "$#" -gt 0 ]];do echo "$1";shift;done;};test "${args[@]}"

START at the last positional parameter: echo "${@: -1}" or -1:1 to get one char from end.

##ARGS to Array
function test { argn=${#@};for ((i=$argn;i>0;i--)); do args[$i]=${@: -$i:1};done;};test a b c;declare -p args
Output --> declare -a args=([1]="c" [2]="b" [3]="a")


##ARGS Seperatation depending on first char (i.e dash -)
${1:0:1} will return the first char. Then you can compare (if) with == "-"

You can assign all args in an array using "-" as delimiter
Function test { local args=$@;declare -a params;readarray -d"-" -t -O1 params <<< "$args";declare -p params;}

Thus you can send an argument like "one two", which will be normally considered as two args ($1=one , $2=two)
But in case of readarray will be considered as $1 since there is not dash - to separate the args.

Another Option is to use case with loop to iterate through args separated by dash -:
for arg in $@;do
case arg in
-*) arg starts with dash , then do this;;
*) not dash , so do the other thing;;
esac

Also see the BASH:READ Section for sending args through a file in various cases.

##USE OF SET FOR CUSTOM USAGE
Build a custom command with args by set - make use of positional parameters
http://unix.stackexchange.com/questions/338852/array-as-value-for-tar-exclude/338854#comment599032_338854

You can use the $@ as a kind of variable to make commands.
By applying set ls , command ls is stored in $@.
Using echo "$@" will see ls.
Using just $@ will EXECUTE ls.

You can also append more commands, switches, etc in $@ using set "$@" -d
Now the command hold in $@ is ls -d. Can be seen with echo, can be executed just like $@
You can add as many parameters as you wish.

Also , you can refer to this $@ var with "${@:2:1}" -> will print the second argument of $@

Example by StackExchange:
set tar -zcvf "$FILE" 
set "$@" --exclude='/home/user/test1'
set "$@" --exclude='/home/user/test2'
set "$@" --exclude='/home/user/test3'
"$@" "$SOURCE"

More Checks:
    $ set ls && echo "${@}" --> ls
    $ set "$@" "*.sh" && echo "${@}" --> ls *.sh
    $ $@ --> prints (ls) all .sh files
    $ set "$@" "*.txt" && echo "${@}" --> ls *.sh *.txt
    $ $@ --> Outputs all .sh and .txt files
    $ echo "${@:2:1}" --> *.sh
    $ echo "${@:2}" --> *.sh *.txt
    $ echo "${@}" --> ls *.sh *.txt
    $ shift && echo "${@}" --> *.sh *.txt
    $ shift && echo "${@}" --> *.txt

.B Use set to strip domain name from http address
dom="http://unix.stackexchange.com/questions/ask"
FS=/; set -- $dom; echo "$3" :--->unix.stackexchange.com
Alternativelly can be done with cut -d/ -f3, or with regex groups (perl) or with tricky sed (i.e sed "y|/|\x00|;s/.*\x00\x00//;s/\x00.*//")

##ARGS null separated
You can manipulate args that may contain any character using null byte as a separator.
arg1="some more";arg2="text here";args=$(echo -e "$arg1\0$arg2\0" |base64) 
#it is important to have null even at the end to separate the last item
#we need to convert to base64 since bash tends to remove the null byte from variables. With base64 we preserve it.
while IFS= read -r -d '' f;do echo "f= $f";done< <(echo "$args" |base64 -d) #base64 variable is decoded

.B Function test with null separated args
function test { while IFS= read -r -d '' f;do echo "f= $f";done< <(echo "$@" |base64 -d);}
test "$args" #works correctly. One arg is sent containing more args null separated.

.B Function Test - Different multi separated args
function test2 { for arg in $@;do echo "test2-arg=" $(echo "$arg" |base64 -d);done;}
arg11=$(echo -e "$arg1\0" |base64); arg22=$(echo -e "$arg2\0" |base64)
test2 $arg11 $arg22
#args are understood and printed correctly in output

.B Tip
For bash 4.4 and above you could also use mapfile/readarray that support -d option (delimiter):
mapfile -t -d '' arr < <(echo "$args" |base64 -d)
mapfile -t -d '\0' b < <(sort_array_function "${a[@]}") 
#see also http://stackoverflow.com/questions/42381149/bash-function-with-an-array-input-and-output

Another way to print null separated args/vars/arrays is to use printf :
  printf '%s\0' "${sorted[@]}"

------------------------------------------------------------------------------------------------------------------------
BASH:PARAMETERS PRACTICAL_USE_OF_BASH_PARAMETERS_EXPANSION
Check these one-liners: http://www.catonmat.net/blog/another-ten-one-liners-from-commandlinefu-explained/
Scroll to the end of page for more one-liners.
http://wiki.bash-hackers.org/syntax/pe

Command substitution : Use contents of file as parameter: $(<file)
Command $(cat file) can be replaced by the equivalent but faster $(< file).
example: echo "$(<file.txt) -- similar to cat file.txt
if [[ " ${array[@]} " =~ " ${value} " ]]; then whatever fi #if array contains value
if [[ ! " ${array[@]} " =~ " ${value} " ]]; then whatever fi
#Get name without extension -> ${FILENAME%.*} ⇒ bash_hackers.txt
#Get extension -> ${FILENAME##*.} ⇒ bash_hackers.txt
#Get extension : find $PWD -type f -exec bash -c 'echo "${0##*.}"' {} \; -> Lists all extensions found.
#Get directory name -> ${PATHNAME%/*} ⇒ /home/bash/bash_hackers.txt
#Get filename -> ${PATHNAME##*/} ⇒ /home/bash/bash_hackers.txt

Remove first and last char with bash expansion: a=$(echo "\"some@some.com\"");echo "Original a=$a - Modified a= ${a:1:-1} - First and Last char removed"
--> Original a="some@some.com" - Modified a= some@some.com - First and Last char removed

FOO="http://unix.stackexchange.com/questions/ask"
tmp="${FOO#*//}" 	#Removes everything from begininng up to first found pattern '//'
echo "${tmp%%/*}" : unix.stackexchange.com		#remove everyting from end up to last/longest (%%) pattern '/' . One % = shortest/first

------------------------------------------------------------------------------------------------------------------------
BASH:TIPS AND TRICKS
##Resources: 
http://www.catonmat.net/blog/another-ten-one-liners-from-commandlinefu-explained/
http://www.catonmat.net/blog/top-ten-one-liners-from-commandlinefu-explained/

##Using subshells: 
$ (cd /tmp && ls) This will call a subshell to perform the commands and will exit. Thus your real shell will not cd to /tmp

##Reverse any word
echo "nixcraft" | rev

##Rename using for and bash parameter expansion
for f in 0[12]/I00[12]0001 ; do mv "$f" "${f}.dcm" ; done # This will go in two folders (01 and 02) and read two files inside each folder (I0010001 and I0020001) and add dcm extension to each of them.

##Remove new line char from strings and replace it with space using trim (tr)
echo -e "hello\nyou asshole" |tr "\n" " " ->hello you asshole #If you remove the tr you will see the text to be printed in two different lines. If you apply -d "\n" new lines will be deleted.
With sed it supposed to be sed -e 's/[\n]//g' but is not working. Texts keeps priting in terminal in two lines.

##Use dnstools to read a wikipedia page in terminal:
dig +short txt hacker.wp.dg.cx # searches wikipedia for term hacker.
I have an alias for that. Alternative : host -t txt hacker.wp.dg.cx

##Quick move and copy 
cp filename{,.bak} #using brace expansion
mv /path/to/file{,_old} #brace expansion

##Trace root with ping together: 
$ mtr google.com

##Find the last command that begins with "whatever," but avoid running it : 
$ !whatever:p

##Change to the previous working directory
$ cd - (insted of cd $OLDPWD)

##Serve the current directory at http://localhost:8000/
$ python -m SimpleHTTPServer 8000

##Run the last command as root : 
$ sudo !! (also simple !! just repeats last command)

##Capture video of a linux desktop : 
$ ffmpeg -f x11grab -s wxga -r 25 -i :0.0 -sameq /tmp/out.mpg

##Read the first line from a file and put it in a variable : 
$ read -r line < file OR IFS= read -r line < file

##Read a random line from a file and put it in a variable : 
$ read -r random_line < <(shuf file)

##Extract filename /dirname from the path : 
filename=${path##*/} 
dirname : dirname=${path%/*}

##declare upper/lower case variables 
uppercase: declare -u foo 
lowercase: declare -l
declare -u b;eval {a,b}="george"; echo "$a --- $b" --> b will print GEORGE due to declare -u in the beginning.

PS: Alternative for lower/upper case is ${a^^} and ${a,,}

##Assign same value to multiple commands using bash parameter expansion: 
eval {a,b,c}="some text" # Without eval is not operating.

##Identify files/ commands : 
type command (try i.e type grep and type eval)
file <file> #will print info about the file , like if it is ASCII

##List all kernel modules that are loaded (i.e lsmod)
cat /lib/modules/$(uname -r)/modules.dep
find /lib/modules/$(uname -r) -type f -name \*.ko

##Print the progress of a recursive command in one / the same line (by Duffy):
touch ff1 ff2 ff3
rm -fv ff1 ff2 ff3 |while read -r line;do 
ws=$(( COLUMNS - ${#line} ));printf '\r%s%*s\r%s' "$line" "$ws" " " "$line";sleep 2;done;echo

##Working with ASCII values of a string / file 
$ echo 'this is \"something\"' |od -t x1c                                                                                               0000000  74  68  69  73  20  69  73  20  5c  22  73  6f  6d  65  74  68
          t   h   i   s       i   s       \   "   s   o   m   e   t   h
0000020  69  6e  67  5c  22  0a
          i   n   g   \   "  \n
od -t x : Prints hex values
od -t d : Prints dec values
od -t o : Prints octal values
Tip1: You can echo a hex ascii value with echo -e "\x5c" => will print a backslash .
Tip2: You can assign a hex ascii in a variable using var=$'\x5c' - 
Tip3: You can also use hex values in sed and other tools to avoid escaping : sed 's/\x22/\x5c\x22/g' --> Will escape all double quotes.


##Looping over hexadecimal values
Range 00-09 and then 0A-0B-0C-0D-0E-0F. Repeats for all numbers.
The most easy way is to use printf since natively supports hex printing

for ((i=0;i<128;i++));do                     #Decimal Numbers in Looping but Hex Output !
h1=$(printf "%#x\n" $i) && echo "$h1"        #prints 0x7a
h2=$(printf "%x\n" $i) && echo "$h2"         #prints just 7a. This can combined in bash /awk/sed/perl with \x in front
h3=$(printf "\\\x%x\n" "20") && echo "$h3"   #prints \x14 - ready to be used in bash/sed/perl/awk as hex code (i.e echo -e "$3")
done

Testing:
$ h=$(printf "\\\x%x" "60") && echo "$h" && echo -e "$h"   #First echo prints \x3c, Second echo prints <

--------------------------------------------------------------------------------------------------------------------------------------
BASH:EVAL - Understanding EVAL:
1) foo=10 x=foo
2) y='$'$x
3) echo $y --> $foo
5) eval y='$'$x
6) echo $y --> 10 # or even eval echo '$'$x will work
Also try to use #{a,b}="some" - bash will complain that a="some" command not recognized. If you use eval then goes ok.

Simpler with indirect expansion:
$ foo=10;x=foo;echo ${!x} 	--> 10
$ foo=10;x=foo;echo ${x} 	--> foo

-----------------------------------------------------------------------------------------------------------------------------------------
BASH:INDIRECT EXPANSION - Similar to eval
http://stackoverflow.com/questions/42047532/bash-for-loop-to-set-a-variable-its-value-and-evaluate-it/42047814?noredirect=1#comment71311357_42047814

$ for i in {1..4};do eval my${i}var="./path${i}_tofile";eval echo "$""my${i}var";done
./path1_tofile
./path2_tofile
./path3_tofile
./path4_tofile
Tip: the first eval (eval my${i}var) can be avoided using declare my${i}var
Remark: OP tried to print the values using echo "$my${i}var" , which never worked (variable within a variable - bash panic!)

For echo part, instead of using eval , the same can be achieved with Indirect Expansion
(see also http://wiki.bash-hackers.org/syntax/pe#indirection)
# for i in {1..4};do declare my${i}var="./path${i}_tofile";tmpvar=my${i}var;echo ${!tmpvar};done

Mind the difference If you don't use the indirect expansion :
# for i in {1..4};do declare my${i}var="./path${i}_tofile";tmpvar=my${i}var;echo ${tmpvar};done
my1var
my2var
my3var
my4var

Difference compared to eval:
No. eval will execute potential command substitutions. Indirect expansion will not. 
Test with this: 
tmpvar='foo`execute nasty command`'; echo "${!tmpvar}"; eval echo "${tmpvar}" -- 
the indirect expansion is essentially "variable not found, substitute the empty string", while eval executes the nasty command.

-----------------------------------------------------------------------------------------------------------------------------------------
BASH:FILE DESCRIPITORS AND REDIRECTIONS
##Sources
Check this FAQ http://mywiki.wooledge.org/BashFAQ/002 for a nice table explaining redirections with file descriptos
Redirections explained with graphics: http://www.catonmat.net/blog/bash-one-liners-explained-part-three/
http://www.tldp.org/LDP/abs/html/io-redirection.html
http://stackoverflow.com/questions/4102475/bash-redirection-with-file-descriptor-or-filename-in-variable
http://unix.stackexchange.com/questions/13724/file-descriptors-shell-scripting
http://www.tldp.org/LDP/abs/html/ioredirintro.html

##BASICS: 
fd0=stdin , fd1=stdout , fd2=stderr - Equals to 0>/dev/tty, 1>/dev/tty , 2>/dev/tty
This redirection format >/dev/null ()or >wherever) EQUALS TO 1>/dev/null (or 1>wherever) == redirection of stdout

IF you try : test=$(java -version);echo $test then you will receive output of java -version in your terminal but var test will be empty.
But if you try test=$(java -version 2>&1);echo $test works ok.
java app prints its version to stderr and not to stdout.
By default you can not assign in vars output of commands that send their output other than stdout &1 (i.e &2 =stderr) 
With the 2>&1 you redirect stderr to stdout and thus you can store that output in a variable.

<GV Remark>
Although we said that at initial stage all fds point to screen this means all fds go to /dev/tty and not to >1
seems that 1&,2& are a kind of "internal variable"
In a pseudocode format : uservar=$(command) EQUALS to command 1>uservar (instead of 1>/dev/tty=screen)
</GV Remark>

Redirect stderr to file and stdout + stderr to screen :
exec 3>&1 && foo 2>&1 >&3 | tee stderr.txt (equals to 2>&1 1>&3)

##Tricky redirection from bins missing man pages:
man -w binaryfile 2>&1 >/dev/null (-w prints man page location)
In case of a normal bin file (i.e grep) then nothing is printed. In case of a bin that do not have a man page (i.e getweb) then
the error message is printed.
mind also that a=$(man -w getweb >/dev/null) will also print the error message, even if $a is NOT echoed and also $a will be blank.
the redirection >/dev/null is in reality equal to 1>/dev/null, meaning redirecting stdout (&1) to dev/null
mind also that examples:
man -w grepp 2>/dev/null -> although the package / bin is wrong = no man page , nothing is printed on screen since stderr is forwarded to /dev/null
man -w grep 1>/dev/null -> equivalent to man -w grep >/dev/null
man -w grep 2>/dev/null -> since there is a man page for grep, the location is printed on screen since this redirection affects only &2 = stderr
man -w grepp &>/dev/null -> this syntax forwards both stdout and stderr and as result nothing is printed either for grepp (no man page) or grep (valid man page)
#
With annotate-output shell script of devscripts you can run any command and it's output will be marked by O or E depending on where it's printed (0 for stdout, E for stderr). It is also provide Info (I) about exit code
Main usage of file descriptors is when you need to split your code like this;
exec >data-file #equivalent to 1>data-file = redirect stdout to a data-file
exec 3>log-file
echo "first line of data"  #though you don't specify fd , it is redirected to data_file due to the very first exec
echo "this is a log line" >&3 #this goes to fd3 = log file
if something_bad_happens; then echo error message >&2; fi #this goes to fd2 (not specially defined in this example)
exec &>-  # close the data output file
echo "output file closed" >&3
But again you dont gain anything with fds. You can send output directly to >anyfile in case of echo
On the other hand , by assigning stdout to ata-file (just >data-file) you can capture messages from scripts/programms etc that would
normally go to stdout.
correspondingly you can exec 2>error-file and any programm that prints anything of &2 (stderr) will be sent to error-file.

-----------------------------------------------------------------------------------------------------------------------------------------
BASH:FILE DESCRIPTORS tricky usage from http://mywiki.wooledge.org/BashFAQ/002
Let's say you want only the stderr, but not stdout. 
Well, then first you have to decide where you do want stdout to go: 
(a)output=$(command 2>&1 >/dev/null)  # Save stderr, discard stdout.
(b)output=$(command 2>&1 >/dev/tty)   # Save stderr, send stdout to the terminal.
(c)output=$(command 3>&2 2>&1 1>&3-)  # Save stderr, send stdout to script's stderr.

Analyzing the last (c) case:
Initial :If this run in a terminal without any redirections stdin, stdout and stderr are all initially connected to the terminal (tty).
state	: equals to 0>/dev/tty , 1>/dev/tty, 2>/dev/tty and 3 does not exist by default.

3>&2 	: 	FD 3 should point to what FD 2 points to at this very moment, meaning FD 3 will point to the script's stderr 
			(also called "save stderr in FD 3").

2>&1 	:	Next, FD 2 should point to what FD 1 currently points to, meaning FD 2 will point to stdout. 
			Right now, both FD 2 and FD 1 would be captured.

1>&3 	:	Next, FD 1 should point to what FD 3 currently points to, meaning FD 1 will point to the script's stderr. 
			FD 1 is no longer captured. We have "swapped" FD 1 and FD 2.

3>&- 	:	Finally, we close FD 3 as it is no longer necessary.

gv: This swaps stdout and stderr. In real life the succesfull results of the command go to &1 which is printed in screen instead of the command buffed (pipe). The error of the command goes to &2 which has been redirected to buffer/pipe.
Verificable example:
sh-4.2$ echo "echo printing: $(cat test 3>&2 2>&1 1>&3)" 
this is a test #this is printed directly on screen (tty0) by cat
echo printing: #this is the output of the command $(..) which is nothing since with above redirections the pipe will hold only stderr.

sh-4.2$ echo "echo printing: $(cat notexistingfile 3>&2 2>&1 1>&3)" 
echo printing: cat: notexistingfile: No such file or directory 
Nothing is printed on the screen. Error message is stored in the pipe / command buffer $(...)

Another redirection example . It is useless but can give some tips on how redirection works:
sh-4.2$ echo "echo printing: $(cat notexistingfile 1>&2 2>&1)"
cat: notexistingfile: No such file or directory #printed directly on the screen/tty0
echo printing: #command buffer holds nothing
sh-4.2$ echo "echo printing: $(cat test 1>&2 2>&1)"
this is a test #printed directly on tty0 / screen
echo printing: #command buffer / pipe holds nothing again

Explanation:
First Redirection :1>&2 detach fd 1 from pipe/buffer (normal operation for a command $()) and send it to fd2 = tty0 (default for fd2)
Second Redirection: 2>&1 detach fd2 from tty0 (default) and send it to wherever fd1 is attached = tty0 (from previous redir)
Result : both fd1 and fd2 are sent to tty0

Another useless example:
sh-4.2$ echo "echo printing: $(cat notexistingfile 2>&1 1>&2)"
echo printing: cat: notexistingfile: No such file or directory
sh-4.2$ echo "echo printing: $(cat test 2>&1 1>&2)"
echo printing: this is a test

Explanation:
First redirection 2>&1 : detach fd2 from default (tty0) and send it to wherever fd1 goes (command buffer/pipe when used with $())
Second redirection 1>&2 :detach fd1 from pipe/buffer (default in case of $()) and send it to wherever fd2 goes = command buffer = useless
Result: Nothing is printed on tty0 but both normaloutput (stdout) and error output (stderr) go to pipe/command buffer.

Another example of swapping stderr with stdout
sh-4.2$ echo "echo printing: $(cat notexistingfile 3>&1 1>&2 2>&3)"
echo printing: cat: notexistingfile: No such file or directory #stderr goes to command pipe/buffer

sh-4.2$ echo "echo printing: $(cat test 3>&1 1>&2 2>&3)"
this is a test #normal out (stdout) goes directly to screen 
echo printing: #command pipe/buffer is empty since no error raised - no data send to stderr.

The key point to understand redirections is that:
commands behavior is always the same : normal output goes to stdout , errors go to stderr.
Works like :
command output -> stdout ->fd1 -> tty0 (in CLI)
command output -> stdout ->fd1 -> buffer/pipe (in script format $())
command errors -> stderr ->fd2 -> tty0 (in both CLI and script format $())
With redirections you only alter the last part of the chain.

So in case of  $(cat 'any file' 3>&1 1>&2 2>&3) we have 
Initial state in script usage:
cat command succesfull -> normal output --> stdout -->fd1 --> command pipe
cat command error (file not found) -------> stderr -->fd2 -->tty0

Redirections:
---------------------------STEP 1 (3>&1)----------------------------------------------------------------------------------
fd3-----|          #fd3 created and goes to wherever stdout-->fd1 goes = pipe for scripts format 
        |
fd1---> pipe 
fd2---> tty0
---------------------------STEP 2 (1>&2)----------------------------------------------------------------------------------
fd3 ------------|
                |
fd1 ----|      pipe
        |          #stdout --> fd1 detached from pipe and goes to wherever fd2 (stderr) goes = tty0 
fd2--->tty0

---------------------------STEP 3 (2>&3)----------------------------------------------------------------------------------

fd3 ------------|
                |
fd1 ----|      pipe
        |       |
       tty0     |
                |
fd2-------------|  #stderr-fd2 detached from tty0 and goes to wherever fd3 goes = pipe


Redirection Result Chain (swap fd1 with fd2 for scrip usage):
cat command normal output --> stdout -->fd1 --> tty0 	#step 2 above
cat command error --> stderr -->fd2 -->pipe		#step 3 (2>&3) and 1 (3>&1) above

And a final redirection that disregards normal output and get's only error to do something with them:
sh-4.2$ echo "echo printing: $(cat test 2>&1 1>/dev/null;echo $?)"
echo printing: 0 #normal output --> stdout-->fd1-->/dev/null
sh-4.2$ echo "echo printing: $(cat tesat 2>&1 1>/dev/null;echo $?)"
echo printing: cat: tesat: No such file or directory 1 #error output ->stderr ->fd2 -> pipe/buffer
In both cases nothing printed on screen directly.

Mind the exit code of cat. Can be used to built if statements like if error without printing anything.
remember that the usage if $(command);then EQUALS TO if $? of last command = 0 then

AS IT IS OBVIOUS, REDIRECTION SEQUENCE IS IMPORTANT TO ACHIEVE YOUR RESULT. REDIRECTIONS ARE READ FROM LEFT TO RIGHT.

In all above redirections, the existance of fd3 is like a temp save of the location that fd1 goes.
This could be avoided if we were sure where exactly to redirect final destination.
If for example in all systems world wide the screen was tty0, then we could avoid the use of temp fd3 like this:
$(cat 'any file' 2>&1 1>/dev/tty0)
Meaning: detach fd2 from tty0(default) and send it to wherever fd1 goes (=pipe) and then detach fd1 from pipe and send it to /dev/tty0.

But this is not portable and this is why is not used at all. 
You create a tmp fd3 to go at fd1 in the very beginning , saving the pipe internal location/name , then you ask fd1 to go to wherever fd2 goes (screen , maybe tty0, maybe tty1, maybe something else) and finally you detach fd2 from screen and send it to fd1 previous location = fd3 current location , since you can not redirect fd to a place that no longer exists (like fd1 old place) 

.B ANOTHER TRICKY REDIRECTION FOR SWAPPING STDERR WITH STDOUT (http://mywiki.wooledge.org/BashFAQ/002)
This also achieves the swap in a different way:
exec 3>&1			# Save the place that stdout (1) points to . This is the screen because we are in CLI mode.
output=$(command 2>&1 1>&3)	# Run command.  stderr is captured, and 
exec 3>&- 			#close fd3	

This does the swap job (stderr is captured at output variable pipe and script's output is send to screen) because:
exec 3>&1 OUTSIDE script is sent to wherever fd1 goes OUTSIDE script = SCREEN(tty0)
2>&1 inside script goes detach fd2 from tty0 and send it to wherever fd1 goes INSIDE script = pipe  
1>&3 inside script detaches fd1 from script's pipe and send it to fd3 which has been assigned OUTSIDE the script to go at screen.
Brilliant! 

.B Redirecting output to multiple processes:
$ tr '\n' '\\' <file3 > >(sed 's#a#A#g') > >(sed "s#[\]#\0n#g")
Apples\norAnges\nbAnAnAs\ncArrots\n

.B  GLOBAL SCRIPT REDIRECTION
http://mywiki.wooledge.org/BashFAQ/106
#!/usr/bin/env bash
# or whichever shebang you prefer
exec >log 2>&1 #equals to 1>log 2>&1 = 2>log
Now, all of the commands in the rest of your script will inherit log as their stdout and stderr. 

------------------------------------------------------------------------------------------------------------------------
BASH:FIFO NAMED PIPES
FIFOs actually work as named buffers. With all subshells/subprocesses can share info.
Maybe some commands do not accept input by refular files but from fifos and/or file descriptors. 

To create a FIFO pipe use "mkfifo mypipe1"
This actually creates a kind of FIFO file with name mypipe1 (can be seen with ls). 
Command "file mypipe1" will advise that this is a fifo.
Delete a fifo by rm mypipe1, as with any regular file.
You can echo something to this FIFO using echo "something" >mypipe1 . Mind that terminal prompt is trapped.
And you can then retrieve the buffer data (i.e from another shell) using cat mypipe1 or cat <mypipe1 and terminal1 and terminal2 prompt are released
After cat , the fifo is empty - can be verified by trying to cat again.
Once you cat fifo in terminal2 and info has no data , terminal 2 remains trapped awaiting for data to come.
But once data comes in , will be printed and prompt will be freed.
Yad designed uses fifos in this example: https://sourceforge.net/p/yad-dialog/wiki/Frontend%20for%20find+grep%20commands/
Another example is the wikipedia netcat small proxy
mkfifo backpipe; nc -l 12345  0<backpipe | nc www.google.com 80 1>backpipe
This makes the fifo, and redirects local connections to port 12345 to google (default netcat operation)
but also redirects response back to browser!! (this is not netcat default operation)
You can verify if a fifo is present with if [[ ! -p "$pipe" ]];then mkfifo XXX;fi

There are various techniques to cheat the fifo "one-shot" behavior.
In terminal 1 if you run cat >fifo1 & , this will release prompt1 and you can then echo many times to fifo1 witout terminal1 to be trapped again.
terminal 2 will print immediately whatever comes in fifo.
see also: http://stackoverflow.com/questions/8410439/how-to-avoid-echo-closing-fifo-named-pipes-funny-behavior-of-unix-fifos
------------------------------------------------------------------------------------------------------------------------
BASH:ASSOCIATIVE ARRAYS (declare -A array)
http://www.artificialworlds.net/blog/2012/10/17/bash-associative-array-examples/
http://www.artificialworlds.net/blog/2013/09/18/bash-arrays/  #Tips / Examples of Normal Arrays.

##Using Associative Arrays
Works like dictionaries of advanced programming languages.
You can assign whatever index you want (i.e array[file])
declare -A MYMAP=( [foo]=bar [baz]=quux [corge]=grault ); echo ${MYMAP[foo]};echo ${MYMAP[baz]} -> bar \n quux
K=baz; MYMAP[$K]=quux;echo ${MYMAP[$K]} -->quux   #also echo ${MYMAP[baz]} works 
declare -A MYMAP=( [foo a]=bar [baz b]=quux );echo "${!MYMAP[@]}" --> foo a baz b #prints only the keys
declare -A MYMAP=( [foo a]=bar [baz b]=quux );for K in "${!MYMAP[@]}"; do echo $K; done  #loop on keys only - mind double quotes.
--> foo a 
--> baz b
declare -A MYMAP=( [foo a]=bar [baz b]=quux );for V in "${MYMAP[@]}"; do echo $V; done #loop on values only
--> bar
--> quux
declare -A MYMAP=( [foo a]=bar [baz b]=quux );for K in "${!MYMAP[@]}"; do echo $K --- ${MYMAP[$K]}; done #loop on keys and values
--> foo a --- bar
--> baz b --- quux
declare -A MYMAP=( [foo a]=bar [baz b]=quux );echo ${#MYMAP[@]}  --> 2 # Number of keys in an associative array

##Number Indexing of Associative Array :
declare -A MYMAP=( [foo a]=bar [baz b]=quux );KEYS=("${!MYMAP[@]}");echo "${KEYS[0]} --- ${MYMAP[${KEYS[0]}]}" -> foo a --- bar   # KEYS=(${!MYMAP[@]}) = a normal array containing all the keys of the associative array. You can then refer to associative array with numerical index (0,1,2,etc)
declare -A MYMAP=( [foo a]=bar [baz b]=quux );for (( I=0; $I < ${#MYMAP[@]}; I+=1 )); do KEY=${KEYS[$I]};  echo $KEY --- ${MYMAP[$KEY]}; done
--> foo a --- bar
--> baz b --- quux

##Associative Arrays over files / globbing
declare -A file_hash;for file in *; do file_hash+=([$file]=1);done
file_array=(*);declare -A file_hash=( $(echo ${file_array[@]} | sed 's/[^ ]*/[&]=&/g') )  #Not working on bash 4.4
declare -A filehash; eval $(printf 'filehash+=(["%s"]=1);' *.txt) #Works ok

------------------------------------------------------------------------------------------------------------------------
BASH:TIPS WHEREIS & WHATIS
whereis finds where is the executable of a programm (whereis sed). 
whatis shows one-line info of the program.

Trick : whatis /bin/* 2>&1 |grep -v "nothing appropriate" |grep "file" -> Scans the whole bin directory for all executables/commands 
excluding "nothing appropriate" that appears in execs without a single line description and matching file in description

Display a small message about installed bin files in usr/bin (and other folders)
find /usr/bin -type f -executable |xargs whatis -v 2>&1 |sed 's/ - /:/g' >whatis.log
mind xargs. Without xargs is not operating.

------------------------------------------------------------------------------------------------------------------------
BASH:HEREDOCS
Best explained : http://tldp.org/LDP/abs/html/here-docs.html
Basic format : cat <<EOF >file or >/dev/stdout or nothing=stdout

##Inside a script: 
when using here-doc format within a script, the input to cat comes from the script.
Example:
#! /bin/bash
l="line 3"
cat <<End-of-message
-------------------------------------
	This is line 1 of the message.
This is line 2 of the message.
This is $l of the message.
This is line 4 of the message.
This is the last line of the message.
-------------------------------------
End-of-message

when the script finishes above lines are printed in stdout.
If you apply cat <<-ENDOFMESSAGE (mind the dash) then white space is trimmed (except space)

##Tricky script usage:
You can use the here-doc format to comment big blocks of text or code for debugging.
format is :<<whatever ...... whatever
if instead of :<< you use cat << , everything bellow tags will be printed on screen or to >file if defined.

##Another trick usage inside script:
GetPersonalData ()
{
  read firstname
  read lastname
} # This certainly appears to be an interactive function, but . . .


Supply input to the VARIABLES of above function.
GetPersonalData <<RECORD001
Bozo
Bozeman
RECORD001
exit 0

Use a cat here-doc to insert a new line to the end of an existed file
cat <<EOF >>file.txt
This line will be appended to the end of file
EOF

Use cat to insert a line in the BEGINNING of the file:
cat <<EOF >file.txt
This line will go at the beginning
$(cat file.txt)
EOF

You can offcourse use tac instead of cat. But in tac lines of here-doc will be inserted from the last to the first.
This is what tac does = reverse of cat.

##Create Script from Script
Also see http://linuxcommand.org/wss0030.php

#!/bin/bash 
This is master script
Various code of master script
cat > /home/$USER/bin/SECOND_SCRIPT <<- EOT
#!/bin/bash
This is a secondary script generated by master script.
    # - This shall be the second script which automaticall gets placed elsewhere
    # - This shall not be executed when executing the main script
    # - Code within this script shall not appear within the terminal of the main script
	# Comment are also send to secondary script.	
    # Settings

    LOCALMUSIC="$HOME/Music"
    ALERT="/usr/share/sounds/pop.wav"
    PLAYER="mpv --vo null"
(more lines of code here)
EOT # Secondary script finished
Rest Code of Master Script Continues

##Source external code inside your script (instead of sourcing the whole script)
http://unix.stackexchange.com/questions/160256/can-you-source-a-here-document

source <(sed -n '/function justatest/,/\}/p' .bash_aliases) && justatest
The function justatest is sourced correctly.

More source examples:
source <(cat << EOF
A=42
EOF
)
echo $A --> prints 42

Alternative - Directly eval the code 
eval "$(sed -n '/function justatest/,/\}/p' .bash_aliases)" && justatest #worked fine

##Here Doc trick to assign strange chars to variable without escaping
$ IFS='' read -r -d '' var <<'EOF'
j!'^+%&/()=1!'^+%&/()c
EOF
$ echo $var --> Output = j!'^+%&/()=1!'^+%&/()c #This is hard to be assigned even with escaping.

------------------------------------------------------------------------------------------------------------------------
BASH:IFS Tricks
bash hackers word split:http://wiki.bash-hackers.org/syntax/expansion/wordsplit
The IFS variable holds the characters that Bash sees as word boundaries in this step. The default contains the characters
<space>
<tab>
<newline>
These characters are also assumed when IFS is unset. 
When IFS is empty (nullstring), no word splitting is performed at all.

#### Special IFS settings used for string parsing. ####
Whitespace == :Space:Tab:Line Feed:Carriage Return:
WSP_IFS=$'\x20'$'\x09'$'\x0A'$'\x0D'
No Whitespace == Line Feed:Carriage Return
NO_WSP=$'\x0A'$'\x0D'

later, you can just set IFS=${WSP_IFS}

You can temporary set IFS inside a command like while IFS=: read -r lines
Set IFS= to read whole lines , separated by \n
Or set IFS=$'\n'
Set IFS to any char to perform special field split
For file with lines in format field1:field2:field3 using IFS= you will get the whole line, using IFS=: you can split each field.
------------------------------------------------------------------------------------------------------------------------
BASH:OPTIONS  
Globbing ,bash filename expansion, bash options ans shopt options
Bash Debugging: http://tldp.org/LDP/Bash-Beginners-Guide/html/sect_02_03.html#sect_02_03_02
The Set Builtin: https://www.gnu.org/software/bash/manual/html_node/The-Set-Builtin.html
The Shopt Builtin: https://www.gnu.org/software/bash/manual/html_node/The-Shopt-Builtin.html#The-Shopt-Builtin
Shell Expansion: http://tldp.org/LDP/Bash-Beginners-Guide/html/sect_03_04.html
Globbing: http://www.tldp.org/LDP/abs/html/globbingref.html

Bash has by default enabled filename expansion.
This means that a simple echo a* will print all the files starting with a (if any).

This is why sometimes the command apt list a* prints results and sometimes not. 
If there is a file starting with a in the directory you run apt list a*, then a* is expanded due to bash filename expansion.
This was revealed using set -x on bash before command execution.
With -x bash informs you - prints out - the command that is going to be executed.

And this filename expansion confuses people since apt list a* is actually interpreted as apt list allfilesbeginningwitha, but apt list xfce* works without quotes if there are not files beginning with xfce.

You can disable this behavior using "set -f" , but this command will also disable the globbing in general, meaning that ls a* will result to literal a* and not global *

Or you can just run apt list "a*" and this will work fine.

Most used debuging commands: set -fvx (f for filename expansion disable, v for verbose, x for xtrace

To print all bash set parameters run #echo $SHELLOPTS && echo $-
Typical Output: 
braceexpand:emacs:hashall:histexpand:history:interactive-comments:monitor
himBHs

By command 'set' you get a variables list (including predefined startup functions of .bash_aliases file)

Print environmental variables : export -p , printenv or just env, ( set -o posix ; set ) , declare -p (or -xp)

To print all bash shopt parameters run #shopt. Combine with -s to see options set to ON or -u to see options set to OFF
Typical Output:
extquote       	on
force_fignore  	on
hostcomplete   	on
...................
dirspell       	off
dotglob        	off
execfail       	off
extdebug       	off
extglob        	off
failglob       	off
------------------------------------------------------------------------------------------------------------------------
BASH:FIND Iterate through various folders without find:
for i in a b c d e
do
    (cd $i/; for i in Test_*_hit.txt; do cut -f1,2 $i > ${i%.txt}2.txt; done)
done

BASH:FUNCTIONS AND ALIASES
##Alias
You can use alias some='your alias or your function here'
You can store all your custom alias in .bashrc file or even better in the .bash_aliases file.
Usually .bashrc file has a check for existance of .bash_aliases file, and if found , then it sources this file.
Check the .bash_aliases file for cool aliases.
To see current alias use "alias". To unset an alias use unalias <name>

##Functions
Can be declared either with function myf { } or directly as myf () {...}
Function can get arguments ($@, $*, $#,$1,etc) that can be used by function in the same way that a script receives arguments.

##functions in shell - like aliase
In the same file (.bashrc or .bash_aliases) you can have functions.
Those functions work directly from command line - can be exposed using set
Tip: You can not use the bash functions in a script, but you can source them by sourcing the .bash_aliases.

##Functions in scripts
See [: http://mywiki.wooledge.org/BashFAQ/084 :]
Functions you can have also in your scripts.
Functions can return as exit status an integer directly from number 1..255 . 
PS: Function return code an be captured by $? i suppose.
Indirectly , functions can return strings with simple echo if they are called in script mode : 
$(myfunction)  

The reason is that echoing within a function will send the data to stdout but in script mode $( ) echos will be caught by script buffer/pipe.
foo() {
   echo "running foo()..."  >&2        # send user prompts and error messages to stderr
   echo "this is my data"              # variable will be assigned this value below
}
x=$(foo)                               # prints:  running foo()...
echo "foo returned '$x'"               # prints:  foo returned 'this is my data'

Functions when called like $(foo) runs in subshell,  which means that any variable assignments, etc. performed in the function will not take effect in the caller's environment. 
Redirecting unwanted messages to >&2 will cause them to print on screen or wherever &2 might have been redirected, but will not captured by $( ) buffer/pipe.

Instead of capturing the function in a script format, you can use function to assign data to a global variable, and then in the main script you can just read the value of this variable.
foo() { return="this is my data";}; foo && echo "foo returned '$return'" 
This will print this is my data because function is not executed in a subshell but var $return is just read.

Tip:
It is not true that functions can not change global variables. Mind this test:
$ function foo { echo "i am inside function. a recevied = $a";a=$(($a*3));};a=1000;foo;echo "function changed a to :$a"
Will change the value of a from 1000 to 3000.
The "childs can not change parrents" rule applies only to subshells (i.e pipes)

##Function Local Variables. 
You can have local variables inside a function by assigning local var=value
If you don't specify local , then the var is considered as global, even if it is not previously defined in the main script.

##Use a function as a pipe
To do this all you have to do is to read the /dev/stdin , like for example var=$(</dev/stdin)
Then var can be used inside the function.
See this date to epoch function converter that works as a pipe:
function dtoe {
[[ -z $1 ]] && local dt=$(</dev/stdin) || local dt="$1" 
#if $1 is empty, use dev/stdin = work like a pipe. Otherwise use $1 as value for $dt
date -d "$(echo $dt | sed -e 's,/,-,g' -e 's,:, ,')" +"%s"
}

##How to ignore alias : http://mywiki.wooledge.org/BashFAQ/086
You might need to run a command and not it's alias that you might have set.
You can do that by unalias, or by $ command ls, or by \ls , etc

##Force Different behavior of functions/aliases if called by command line or script
With function you can have something like:
ls() { if test -t 1; then command ls -FC "$@"; else command ls "$@";fi;}
In this way you use the -FC args only if ls runs from a terminal and not from a script ; More generally when the &1 is not redirected to /dev/stdout
